<!-- ---
hide:
  - footer
--- -->


# **References**

## **Decision Support**

&emsp;&emsp;**1) Implicit World Models for Action Prediction and Planning**

&emsp;&emsp;Do as i can, not as i say: Grounding language in robotic affordances [[paper](https://arxiv.org/abs/2204.01691)] [[code](https://github.com/google-research/google-research/tree/master/saycan)]

&emsp;&emsp;Pandora: Towards general world model with natural language actions and video states [[paper](https://arxiv.org/abs/2406.09455)] [[code](https://github.com/maitrix-org/Pandora)]

&emsp;&emsp;PaLM-e: An embodied multimodal language model [[paper](https://arxiv.org/abs/2303.03378)]

&emsp;&emsp;DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge [[paper](https://arxiv.org/abs/2507.04447)] [[code](https://github.com/Zhangwenyao1/DreamVLA)]

&emsp;&emsp;An embodied generalist agent in 3D world [[paper](https://arxiv.org/abs/2311.12871)] [[code](https://github.com/embodied-generalist/embodied-generalist)]

&emsp;&emsp;Multiply: A multisensory object-centric embodied large language model in 3d world [[paper](https://arxiv.org/abs/2401.08577)] [[code](https://github.com/UMass-Embodied-AGI/MultiPLY)]

&emsp;&emsp;Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning [[paper](https://arxiv.org/abs/2311.17842)]

&emsp;&emsp;Vlmpc: Vision-language model predictive control for robotic manipulation [[paper](https://arxiv.org/abs/2407.09829)] [[code](https://github.com/PPjmchen/VLMPC)]

&emsp;&emsp;Rt-2: Vision-language-action models transfer web knowledge to robotic control [[paper](https://arxiv.org/abs/2307.15818)] [[code](https://github.com/kyegomez/RT-2)]

&emsp;&emsp;3D-VLA: A 3D vision-language-action generative world model [[paper](https://arxiv.org/abs/2403.09631)] [[code](https://github.com/UMass-Embodied-AGI/3D-VLA)]

&emsp;&emsp;3d-llm: Injecting the 3d world into large language models [[paper](https://arxiv.org/abs/2307.12981)] [[code](https://github.com/UMass-Embodied-AGI/3D-LLM)]

&emsp;&emsp;Thinking, fast and slow [[paper](https://link.springer.com/article/10.1007/s00362-013-0533-y)]

&emsp;&emsp;Gr00t n1: An open foundation model for generalist humanoid robots [[paper](https://arxiv.org/abs/2503.14734)] [[code](https://github.com/NVIDIA/Isaac-GR00T)]

&emsp;&emsp;Vision-language-action model with open-world embodied reasoning from pretrained knowledge [[paper](https://arxiv.org/pdf/2505.21906)]

&emsp;&emsp;OpenVLA: An open-source vision-language-action model [[paper](https://arxiv.org/abs/2406.09246)] [[code](https://github.com/openvla/openvla)]

&emsp;&emsp;EgoAgent: A joint predictive agent model in egocentric worlds [[paper](https://arxiv.org/abs/2502.05857)] [[code](https://github.com/zju3dv/EgoAgent)]

&emsp;&emsp;Internlm: A multilingual language model with progressively enhanced capabilities [[paper](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf)]

&emsp;&emsp;GenRL: Multimodal-foundation world models for generalization in embodied agents [[paper](https://arxiv.org/abs/2406.18043)] [[code](https://github.com/mazpie/genrl)]

&emsp;&emsp;Founder: Grounding foundation models in world models for open-ended embodied decision making [[paper](https://arxiv.org/abs/2507.12496)]

&emsp;&emsp;UP-VLA: A unified understanding and prediction model for embodied agent [[paper](https://arxiv.org/abs/2501.18867)] [[code](https://github.com/CladernyJorn/UP-VLA)]

&emsp;&emsp;Cot-vla: Visual chain-of-thought reasoning for vision-language-action models [[paper](https://arxiv.org/abs/2503.22020)]

&emsp;&emsp;**2) Latent Dynamics Modeling for Action Prediction and Planning**

&emsp;&emsp;Learning latent dynamics for planning from pixels [[paper](https://arxiv.org/abs/1811.04551)] [[code](https://github.com/google-research/planet)]

&emsp;&emsp;Dream to control: Learning behaviors by latent imagination [[paper](https://arxiv.org/pdf/1912.01603.pdf)] [[code](https://github.com/danijar/dreamer)]

&emsp;&emsp;Mastering atari with discrete world models [[paper](https://arxiv.org/abs/2010.02193)] [[code](https://github.com/danijar/dreamerv2)]

&emsp;&emsp;Daydreamer: World models for physical robot learning [[paper](https://arxiv.org/abs/2206.14176)] [[code](https://github.com/danijar/daydreamer)]

&emsp;&emsp;Mastering diverse domains through world models [[paper](https://arxiv.org/abs/2301.04104)] [[code](https://github.com/danijar/dreamerv3)]

&emsp;&emsp;Mastering diverse control tasks through world models [[paper](https://www.nature.com/articles/s41586-025-08744-2)] [[code](https://github.com/danijar/dreamerv3)]

&emsp;&emsp;Planning to explore via self-supervised world models [[paper](https://arxiv.org/abs/2005.05960)] [[code](https://github.com/ramanans1/plan2explore)]

&emsp;&emsp;Learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics [[paper](https://openreview.net/pdf?id=TjCDNssXKU)] [[code](https://github.com/CognitiveModeling/THICK)]

&emsp;&emsp;FOCUS: Object-centric world models for robotic manipulation [[paper](https://arxiv.org/abs/2307.02427)] [[code](https://github.com/StefanoFerraro/FOCUS)]

&emsp;&emsp;EgoAgent: A joint predictive agent model in egocentric worlds [[paper](https://arxiv.org/abs/2502.05857)] [[code](https://github.com/zju3dv/EgoAgent)]

&emsp;&emsp;V-jepa 2: Self-supervised video models enable understanding, prediction and planning [[paper](https://arxiv.org/abs/2506.09985)] [[code](https://github.com/facebookresearch/vjepa2)]

&emsp;&emsp;Temporal difference learning for model predictive control [[paper](https://arxiv.org/abs/2203.04955)]

&emsp;&emsp;TD-MPC2: Scalable, robust world models for continuous control [[paper](https://arxiv.org/abs/2310.16828)] [[code](https://github.com/wangjs96/tdmpc2-check)]

&emsp;&emsp;**3) Implicit World Models for Action Prediction and Planning**

&emsp;&emsp;Unsupervised learning for physical interaction through video prediction [[paper](https://arxiv.org/abs/1605.07157)]

&emsp;&emsp;Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning [[paper](https://arxiv.org/abs/1810.03043)] [[code](https://github.com/febert/robustness_via_retrying)]

&emsp;&emsp;Visual foresight: Model-based deep reinforcement learning for vision-based robotic control [[paper](https://arxiv.org/abs/1812.00568)] [[code](https://github.com/SudeepDasari/visual_foresight)]

&emsp;&emsp;Closed-loop visuomotor control with generative expectation for robotic manipulation [[paper](https://arxiv.org/abs/2409.09016)] [[code](https://github.com/OpenDriveLab/CLOVER)]

&emsp;&emsp;Deep visual foresight for planning robotic motion [[paper](https://arxiv.org/abs/1610.00696)]

&emsp;&emsp;Learning language-conditioned robot behavior from offline data and crowd-sourced annotation [[paper](https://arxiv.org/abs/2109.01115)] [[code](https://github.com/suraj-nair-1/lorel)]

&emsp;&emsp;UP-VLA: A unified understanding and prediction model for embodied agent [[paper](https://arxiv.org/abs/2501.18867)] [[code](https://github.com/CladernyJorn/UP-VLA)]

&emsp;&emsp;RoboDreamer: Learning compositional world models for robot imagination [[paper](https://arxiv.org/abs/2404.12377)] [[code](https://github.com/UMass-Embodied-AGI/robodreamer)]

&emsp;&emsp;DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge [[paper](https://arxiv.org/abs/2507.04447)] [[code](https://github.com/Zhangwenyao1/DreamVLA)]

&emsp;&emsp;DINOv2: Learning robust visual features without supervision [[paper](https://arxiv.org/abs/2304.07193)] [[code](https://github.com/facebookresearch/dinov2)]

&emsp;&emsp;Segment anything [[paper](https://arxiv.org/abs/2304.02643)] [[code](https://github.com/facebookresearch/segment-anything)]

&emsp;&emsp;This&that: Language-gesture controlled video generation for robot planning [[paper](https://arxiv.org/abs/2407.05530)] [[code](https://github.com/Kiteretsu77/This_and_That_VDM)]

&emsp;&emsp;Learning universal policies via text-guided video generation [[paper](https://arxiv.org/abs/2302.00111)]

&emsp;&emsp;Cot-vla: Visual chain-of-thought reasoning for vision-language-action models [[paper](https://arxiv.org/abs/2503.22020)]

&emsp;&emsp;Video generators are robot policies [[paper](https://arxiv.org/abs/2508.00795)] [[code](https://github.com/cvlab-columbia/videopolicy)]

&emsp;&emsp;COMBO: Compositional world models for embodied multi-agent cooperation [[paper](https://arxiv.org/abs/2404.10775)] [[code](https://github.com/UMass-Embodied-AGI/COMBO)]

&emsp;&emsp;3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model [[paper](https://arxiv.org/abs/2506.06199)] [[code](https://github.com/Hoyyyaard/3DFlowAction)]

&emsp;&emsp;Cotracker: It is better to track together [[paper](https://arxiv.org/abs/2307.07635)] [[code](https://github.com/facebookresearch/co-tracker)]

&emsp;&emsp;Cotracker3: Simpler and better point tracking by pseudo-labelling real videos [[paper](https://arxiv.org/abs/2410.11831)] [[code](https://github.com/facebookresearch/co-tracker)]

&emsp;&emsp;Cosmos world foundation model platform for physical ai [[paper](https://arxiv.org/abs/2501.03575)] [[code](https://github.com/nvidia-cosmos/cosmos-predict1)]

&emsp;&emsp;Predictive inverse dynamics models are scalable learners for robotic manipulation [[paper](https://arxiv.org/abs/2412.15109)] [[code](https://github.com/InternRobotics/Seer)]

&emsp;&emsp;Prediction with action: Visual policy learning via joint denoising process [[paper](https://arxiv.org/abs/2411.18179)] [[code](https://github.com/Robert-gyj/Prediction_with_Action)]

&emsp;&emsp;FlowDreamer: A RGB-d world model with flow-based motion representations for robot manipulation [[paper](https://arxiv.org/abs/2505.10075)]


## **Training Facilitation**

&emsp;&emsp;**1) Data Engine**

&emsp;&emsp;Rt-2: Vision-language-action models transfer web knowledge to robotic control [[paper](https://arxiv.org/abs/2307.15818)] [[code](https://github.com/kyegomez/RT-2)]

&emsp;&emsp;Ï€_0: A vision-language-action flow model for general robot control [[paper](https://arxiv.org/abs/2410.24164)] [[code](https://github.com/Physical-Intelligence/openpi)]

&emsp;&emsp;Gemini robotics: Bringing ai into the physical world [[paper](https://arxiv.org/abs/2503.20020)]

&emsp;&emsp;Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems [[paper](https://arxiv.org/abs/2503.06669)] [[code](https://github.com/OpenDriveLab/AgiBot-World)]

&emsp;&emsp;Gr00t n1: An open foundation model for generalist humanoid robots [[paper](https://arxiv.org/abs/2503.14734)] [[code](https://github.com/NVIDIA/Isaac-GR00T)]

&emsp;&emsp;RDT-1B: A diffusion foundation model for bimanual manipulation [[paper](https://arxiv.org/abs/2410.07864)] [[code](https://github.com/thu-ml/RoboticsDiffusionTransformer)]

&emsp;&emsp;DreamGen: Unlocking generalization in robot learning through video world models [[paper](https://arxiv.org/abs/2505.12705)] [[code](https://github.com/nvidia/GR00T-dreams)]

&emsp;&emsp;Learning universal policies via text-guided video generation [[paper](https://arxiv.org/abs/2302.00111)]

&emsp;&emsp;Ivideogpt: Interactive videogpts are scalable world models [[paper](https://arxiv.org/abs/2405.15223)] [[code](https://github.com/thuml/iVideoGPT)]

&emsp;&emsp;Wan: Open and advanced large-scale video generative models [[paper](https://arxiv.org/abs/2503.20314)] [[code](https://github.com/Wan-Video/Wan2.1)]

&emsp;&emsp;Latent action pretraining from videos [[paper](https://arxiv.org/abs/2410.11758)] [[code](https://github.com/LatentActionPretraining/LAPA)]

&emsp;&emsp;Video pretraining (vpt): Learning to act by watching unlabeled online videos [[paper](https://arxiv.org/abs/2206.11795)] [[code](https://github.com/openai/Video-Pre-Training)]

&emsp;&emsp;GWM: Towards scalable gaussian world models for robotic manipulation [[paper](https://arxiv.org/abs/2508.17600)] [[code](https://github.com/Gaussian-World-Model/gaussianwm)]

&emsp;&emsp;GigaBrain-0: A world model-powered vision-language-action model [[paper](https://arxiv.org/abs/2510.19430)]

&emsp;&emsp;RoboTransfer: Geometry-consistent video diffusion for robotic visual policy transfer [[paper](https://arxiv.org/abs/2505.23171)] [[code](https://github.com/HorizonRobotics/RoboTransfer)]

&emsp;&emsp;Cosmos-reason1: From physical common sense to embodied reasoning [[paper](https://arxiv.org/abs/2503.15558)] [[code](https://github.com/nvidia-cosmos/cosmos-reason1)]

&emsp;&emsp;Dream to manipulate: Compositional world models empowering robot imitation learning with imagination [[paper](https://arxiv.org/abs/2412.14957)] [[code](https://github.com/leobarcellona/drema_code)]

&emsp;&emsp;3D gaussian splatting for real-time radiance field rendering [[paper](https://arxiv.org/abs/2308.04079)] [[code](https://github.com/graphdeco-inria/gaussian-splatting)]

&emsp;&emsp;World-env: Leveraging world model as a virtual environment for VLA post-training [[paper](https://arxiv.org/abs/2509.24948)] [[code](https://github.com/amap-cvlab/world-env)]

&emsp;&emsp;**2) Evaluation**

&emsp;&emsp;Mujoco: A physics engine for model-based control [[paper](https://ieeexplore.ieee.org/document/6386109)] [[code](https://github.com/google-deepmind/mujoco)]

&emsp;&emsp;Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx [[paper](https://ieeexplore.ieee.org/document/7139807)]

&emsp;&emsp;Drake: Model-based design and verification for robotics [[paper](https://drake.mit.edu/)] [[code](https://github.com/RobotLocomotion/drake)]

&emsp;&emsp;The limits and potentials of deep learning for robotics [[paper](https://arxiv.org/abs/1804.06557)]

&emsp;&emsp;A study on the challenges of using robotics simulators for testing [[paper](https://arxiv.org/abs/2004.07368)]

&emsp;&emsp;On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward [[paper](https://www.pnas.org/doi/abs/10.1073/pnas.1907856118)]

&emsp;&emsp;Challenges of real-world reinforcement learning [[paper](https://arxiv.org/abs/1904.12901)]

&emsp;&emsp;Sim-to-real transfer in deep reinforcement learning for robotics: A survey [[paper](https://arxiv.org/abs/2009.13303)]

&emsp;&emsp;CARLA: An open urban driving simulator [[paper](https://arxiv.org/abs/1711.03938)] [[code](https://github.com/carla-simulator/carla)]

&emsp;&emsp;Robothor: An open simulation-to-real embodied ai platform [[paper](https://arxiv.org/abs/2004.06799)] [[code](https://github.com/allenai/ai2thor)]

&emsp;&emsp;WorldEval: World model as real-world robot policies evaluator [[paper](https://arxiv.org/abs/2505.19017)] [[code](https://github.com/liyaxuanliyaxuan/Worldeval)]

&emsp;&emsp;Wan: Open and advanced large-scale video generative models [[paper](https://arxiv.org/abs/2503.20314)] [[code](https://github.com/Wan-Video/Wan2.1)]

&emsp;&emsp;Gemini: A family of highly capable multimodal models [[paper](https://arxiv.org/abs/2312.11805)]

&emsp;&emsp;Evaluating robot policies in a world model [[paper](https://arxiv.org/abs/2506.00613v1)] [[code](https://github.com/world-model-eval/world-model-eval)]

&emsp;&emsp;Gpt-4o system card [[paper](https://arxiv.org/abs/2410.21276)]

&emsp;&emsp;Pre-trained video generative models as world simulators [[paper](https://arxiv.org/abs/2502.07825)]

&emsp;&emsp;Irasim: Learning interactive real-robot action simulators [[paper](https://arxiv.org/abs/2406.14540v1)] [[code](https://github.com/bytedance/IRASim)]

&emsp;&emsp;Genie envisioner: A unified world foundation platform for robotic manipulation [[paper](https://arxiv.org/abs/2508.05635)] [[code](https://github.com/AgibotTech/Genie-Envisioner)]

&emsp;&emsp;Learning real-world action-video dynamics with heterogeneous masked autoregression [[paper](https://arxiv.org/abs/2502.04296)] [[code](https://github.com/liruiw/HMA)]

&emsp;&emsp;Video prediction models as rewards for reinforcement learning [[paper](https://arxiv.org/abs/2305.14343)] [[code](https://github.com/escontra/viper_rl)]
