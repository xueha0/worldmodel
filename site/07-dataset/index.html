<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://your-domain.example/07-dataset/">
      
      
        <link rel="prev" href="../06-core-components/">
      
      
        <link rel="next" href="../08-conclusion/">
      
      
      <link rel="icon" href="../assets/img/badge.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>VII Dataset - A Step Toward World Models: A Survey on Robotic Manipulation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#table-ii" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="A Step Toward World Models: A Survey on Robotic Manipulation" class="md-header__button md-logo" aria-label="A Step Toward World Models: A Survey on Robotic Manipulation" data-md-component="logo">
      
  <img src="../assets/img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            A Step Toward World Models: A Survey on Robotic Manipulation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              VII Dataset
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../abstract/" class="md-tabs__link">
          
  
  
  About

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../99-references/" class="md-tabs__link">
        
  
  
    
  
  References

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="A Step Toward World Models: A Survey on Robotic Manipulation" class="md-nav__button md-logo" aria-label="A Step Toward World Models: A Survey on Robotic Manipulation" data-md-component="logo">
      
  <img src="../assets/img/logo.png" alt="logo">

    </a>
    A Step Toward World Models: A Survey on Robotic Manipulation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            About
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../abstract/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Abstract
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-preliminaries/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    II Preliminaries
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    III Overview of the World Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IV Functions of the World Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-key-tech-challenges/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    V Key Techniques and Notable Challenges
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-core-components/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VI Core Components &amp; Capabilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    VII Dataset
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    VII Dataset
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-ii" class="md-nav__link">
    <span class="md-ellipsis">
      Table Ⅱ
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-conclusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VIII Conclusion &amp; Future directions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../99-references/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="vii-dataset"><strong>VII Dataset</strong><a class="headerlink" href="#vii-dataset" title="Permanent link">¶</a></h1>
<p>  There are abundant datasets that facilitate robot learning, including general robotic manipulation datasets <sup id="fnref:khazatsky2024droid"><a class="footnote-ref" href="#fn:khazatsky2024droid">1</a></sup> <sup id="fnref:walke2023bridgedata"><a class="footnote-ref" href="#fn:walke2023bridgedata">2</a></sup> <sup id="fnref:ebert2021bridge"><a class="footnote-ref" href="#fn:ebert2021bridge">3</a></sup> <sup id="fnref:mandlekar2019scaling"><a class="footnote-ref" href="#fn:mandlekar2019scaling">4</a></sup> <sup id="fnref:deng2025graspvla"><a class="footnote-ref" href="#fn:deng2025graspvla">5</a></sup> <sup id="fnref:chen2024rh20t"><a class="footnote-ref" href="#fn:chen2024rh20t">6</a></sup>, Dual-arm robotic manipulation datasets <sup id="fnref:bu2025agibot"><a class="footnote-ref" href="#fn:bu2025agibot">7</a></sup>,  human manipulation datasets <sup id="fnref:miech2019howto100m"><a class="footnote-ref" href="#fn:miech2019howto100m">8</a></sup> <sup id="fnref:goyal2017something"><a class="footnote-ref" href="#fn:goyal2017something">9</a></sup> <sup id="fnref:souvcek2024genhowto"><a class="footnote-ref" href="#fn:souvcek2024genhowto">10</a></sup> <sup id="fnref:carreira2019short"><a class="footnote-ref" href="#fn:carreira2019short">11</a></sup> <sup id="fnref:cheang2024gr"><a class="footnote-ref" href="#fn:cheang2024gr">12</a></sup> <sup id="fnref:zhen20243d"><a class="footnote-ref" href="#fn:zhen20243d">13</a></sup>, combinations of robotic &amp; human manipulation <sup id="fnref2:zhen20243d"><a class="footnote-ref" href="#fn:zhen20243d">13</a></sup>, egocentric datasets <sup id="fnref:grauman2022ego4d"><a class="footnote-ref" href="#fn:grauman2022ego4d">14</a></sup> <sup id="fnref:grauman2024ego"><a class="footnote-ref" href="#fn:grauman2024ego">15</a></sup> <sup id="fnref:damen2018scaling"><a class="footnote-ref" href="#fn:damen2018scaling">16</a></sup>, 3D &amp; 4D datasets <sup id="fnref:zhen2025tesseract"><a class="footnote-ref" href="#fn:zhen2025tesseract">17</a></sup> <sup id="fnref3:zhen20243d"><a class="footnote-ref" href="#fn:zhen20243d">13</a></sup> <sup id="fnref2:mandlekar2019scaling"><a class="footnote-ref" href="#fn:mandlekar2019scaling">4</a></sup> <sup id="fnref:zhi20253dflowaction"><a class="footnote-ref" href="#fn:zhi20253dflowaction">18</a></sup> <sup id="fnref2:grauman2022ego4d"><a class="footnote-ref" href="#fn:grauman2022ego4d">14</a></sup> <sup id="fnref2:grauman2024ego"><a class="footnote-ref" href="#fn:grauman2024ego">15</a></sup> <sup id="fnref:ramakrishnan22021habitat"><a class="footnote-ref" href="#fn:ramakrishnan22021habitat">19</a></sup> <sup id="fnref:chang2017matterport3d"><a class="footnote-ref" href="#fn:chang2017matterport3d">20</a></sup> <sup id="fnref:hong2024multiply"><a class="footnote-ref" href="#fn:hong2024multiply">21</a></sup> <sup id="fnref:o2024open"><a class="footnote-ref" href="#fn:o2024open">22</a></sup> <sup id="fnref:gu2023maniskill2"><a class="footnote-ref" href="#fn:gu2023maniskill2">23</a></sup>,  multi-view datasets <sup id="fnref2:hong2024multiply"><a class="footnote-ref" href="#fn:hong2024multiply">21</a></sup> <sup id="fnref3:grauman2024ego"><a class="footnote-ref" href="#fn:grauman2024ego">15</a></sup> <sup id="fnref2:chen2024rh20t"><a class="footnote-ref" href="#fn:chen2024rh20t">6</a></sup> and panoramic-view datasets <sup id="fnref2:chang2017matterport3d"><a class="footnote-ref" href="#fn:chang2017matterport3d">20</a></sup>. A detailed information of them can be found in Fig.Ⅱ.</p>
<h3 id="table-ii" style="text-align:center">Table Ⅱ<a class="headerlink" href="#table-ii" title="Permanent link">¶</a></h3>
<p>A SUMMARY OF REPRESENTATIVE DATASETS. <code>H</code>: HOUR, <code>Manip</code>: MANIPULATION, <code>Env.</code>: ENVIRONMENTS, <code>Traj.</code>: TRAJECTORIES. </p>
<figure>
<!-- ![Perspectives on world models](assets/img/table1title.png){ width="100%" } -->
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../assets/img/table2.png" data-desc-position="bottom"><img alt="Perspectives on world models" src="../assets/img/table2.png" width="100%"></a></p>
</figure>
<p>  Recent efforts in world models for robotic manipulation often have leveraged a large and diverse datasets, e.g., a combinations of different dataset, to be capable of generalizing across tasks and environments. For example, Yang <em>et al.</em> <sup id="fnref:yang2023learning"><a class="footnote-ref" href="#fn:yang2023learning">24</a></sup> constructed a large-scale natural dataset combining simulated executions and renderings <sup id="fnref2:ramakrishnan22021habitat"><a class="footnote-ref" href="#fn:ramakrishnan22021habitat">19</a></sup>, real robot data <sup id="fnref2:ebert2021bridge"><a class="footnote-ref" href="#fn:ebert2021bridge">3</a></sup>, human activity videos <sup id="fnref3:grauman2022ego4d"><a class="footnote-ref" href="#fn:grauman2022ego4d">14</a></sup>, 3D panorama scans <sup id="fnref3:chang2017matterport3d"><a class="footnote-ref" href="#fn:chang2017matterport3d">20</a></sup>, and internet text-image data LAION-400M <sup id="fnref:schuhmann2021laion"><a class="footnote-ref" href="#fn:schuhmann2021laion">25</a></sup>. Bruce <em>et al.</em> <sup id="fnref:bruce2024genie"><a class="footnote-ref" href="#fn:bruce2024genie">26</a></sup> combine the RT-1 dataset <sup id="fnref:brohan2023rt"><a class="footnote-ref" href="#fn:brohan2023rt">27</a></sup> with real robot grasping data <sup id="fnref:kalashnikov2018scalable"><a class="footnote-ref" href="#fn:kalashnikov2018scalable">28</a></sup>. Wu <em>et al.</em> <sup id="fnref:wu2024ivideogpt"><a class="footnote-ref" href="#fn:wu2024ivideogpt">29</a></sup> train the world model based on the combination of the Open X-Embodiment (OXE) dataset <sup id="fnref2:o2024open"><a class="footnote-ref" href="#fn:o2024open">22</a></sup> and the Something-Something v2 (SSv2) trajectory dataset <sup id="fnref2:goyal2017something"><a class="footnote-ref" href="#fn:goyal2017something">9</a></sup>. Bruce <em>et al.</em> <sup id="fnref2:cheang2024gr"><a class="footnote-ref" href="#fn:cheang2024gr">12</a></sup> employ a pretraining and fine-tuning strategy. In the pretraining stage, a combination of human demonstration datasets such as Howto100M <sup id="fnref2:miech2019howto100m"><a class="footnote-ref" href="#fn:miech2019howto100m">8</a></sup>, Ego4D <sup id="fnref4:grauman2022ego4d"><a class="footnote-ref" href="#fn:grauman2022ego4d">14</a></sup>, Something-Something V2 <sup id="fnref3:goyal2017something"><a class="footnote-ref" href="#fn:goyal2017something">9</a></sup>, EPIC-KITCHENS <sup id="fnref2:damen2018scaling"><a class="footnote-ref" href="#fn:damen2018scaling">16</a></sup>, Kinetics-700 <sup id="fnref2:carreira2019short"><a class="footnote-ref" href="#fn:carreira2019short">11</a></sup>, and robot datasets <sup id="fnref2:brohan2023rt"><a class="footnote-ref" href="#fn:brohan2023rt">27</a></sup>. Fine-tuning data includes 105 table-top tasks via teleoperation covering eight skills (e.g., pick, place). Data augmentation are performed to add new objects or change backgrounds by means of a diffusion model <sup id="fnref:ho2020denoising"><a class="footnote-ref" href="#fn:ho2020denoising">30</a></sup> and the Segment Anything Model (SAM) <sup id="fnref:kirillov2023segment"><a class="footnote-ref" href="#fn:kirillov2023segment">31</a></sup>, as well as a video generation model <sup id="fnref:ma2025latte"><a class="footnote-ref" href="#fn:ma2025latte">32</a></sup> to sytheize new videos. Du <em>et al.</em> <sup id="fnref:du2023learning"><a class="footnote-ref" href="#fn:du2023learning">33</a></sup> curated an internet-scale pretraining dataset consisting of 14 million video-text pairs, 60 million image-text pairs <sup id="fnref:ho2022imagen"><a class="footnote-ref" href="#fn:ho2022imagen">34</a></sup>, LAION-400M <sup id="fnref2:schuhmann2021laion"><a class="footnote-ref" href="#fn:schuhmann2021laion">25</a></sup>, and a smaller real-world robotic dataset <sup id="fnref3:ebert2021bridge"><a class="footnote-ref" href="#fn:ebert2021bridge">3</a></sup>. Huang <em>et al.</em> <sup id="fnref:huang2025enerverse"><a class="footnote-ref" href="#fn:huang2025enerverse">35</a></sup> constructed multi-anchor view video datasets using public sources including RT-1 <sup id="fnref3:brohan2023rt"><a class="footnote-ref" href="#fn:brohan2023rt">27</a></sup>, Taco-Play <sup id="fnref:rosete2023latent"><a class="footnote-ref" href="#fn:rosete2023latent">36</a></sup>, ManiSkill <sup id="fnref2:gu2023maniskill2"><a class="footnote-ref" href="#fn:gu2023maniskill2">23</a></sup>, BridgeData V2 <sup id="fnref2:walke2023bridgedata"><a class="footnote-ref" href="#fn:walke2023bridgedata">2</a></sup>, LanguageTable <sup id="fnref:lynch2023interactive"><a class="footnote-ref" href="#fn:lynch2023interactive">37</a></sup>, and RoboTurk <sup id="fnref3:mandlekar2019scaling"><a class="footnote-ref" href="#fn:mandlekar2019scaling">4</a></sup>, augmented with Isaac Sim simulations <sup id="fnref:mittal2023orbit"><a class="footnote-ref" href="#fn:mittal2023orbit">38</a></sup>.  <sup id="fnref2:zhen2025tesseract"><a class="footnote-ref" href="#fn:zhen2025tesseract">17</a></sup> construct a 4D embodied video dataset based on previous datasets <sup id="fnref:james2020rlbench"><a class="footnote-ref" href="#fn:james2020rlbench">39</a></sup> <sup id="fnref4:brohan2023rt"><a class="footnote-ref" href="#fn:brohan2023rt">27</a></sup> <sup id="fnref3:walke2023bridgedata"><a class="footnote-ref" href="#fn:walke2023bridgedata">2</a></sup> <sup id="fnref4:goyal2017something"><a class="footnote-ref" href="#fn:goyal2017something">9</a></sup> by measuring depth and normal information.</p>
<h2 id="references"><strong>References</strong><a class="headerlink" href="#references" title="Permanent link">¶</a></h2>
<div class="footnote">
<hr>
<ol>
<li id="fn:khazatsky2024droid">
<p>A. Khazatsky <em>et al.</em>, "Droid: A large-scale in-the-wild robot manipulation dataset," <em>arXiv preprint arXiv:2403.12945</em>, 2024.&nbsp;<a class="footnote-backref" href="#fnref:khazatsky2024droid" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:walke2023bridgedata">
<p>H. R. Walke <em>et al.</em>, "Bridgedata v2: A dataset for robot learning at scale," in <em>Conference on robot learning</em>, 2023, pp. 1723--1736.&nbsp;<a class="footnote-backref" href="#fnref:walke2023bridgedata" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="#fnref2:walke2023bridgedata" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="#fnref3:walke2023bridgedata" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:ebert2021bridge">
<p>F. Ebert <em>et al.</em>, "Bridge data: Boosting generalization of robotic skills with cross-domain datasets," <em>arXiv preprint arXiv:2109.13396</em>, 2021.&nbsp;<a class="footnote-backref" href="#fnref:ebert2021bridge" title="Jump back to footnote 3 in the text">↩</a><a class="footnote-backref" href="#fnref2:ebert2021bridge" title="Jump back to footnote 3 in the text">↩</a><a class="footnote-backref" href="#fnref3:ebert2021bridge" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:mandlekar2019scaling">
<p>A. Mandlekar <em>et al.</em>, "Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity," in <em>2019 IEEE/RSJ international conference on intelligent robots and systems (IROS)</em>, IEEE, 2019, pp. 1048--1055.&nbsp;<a class="footnote-backref" href="#fnref:mandlekar2019scaling" title="Jump back to footnote 4 in the text">↩</a><a class="footnote-backref" href="#fnref2:mandlekar2019scaling" title="Jump back to footnote 4 in the text">↩</a><a class="footnote-backref" href="#fnref3:mandlekar2019scaling" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:deng2025graspvla">
<p>S. Deng <em>et al.</em>, "Graspvla: A grasping foundation model pre-trained on billion-scale synthetic action data," <em>arXiv preprint arXiv:2505.03233</em>, 2025.&nbsp;<a class="footnote-backref" href="#fnref:deng2025graspvla" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
<li id="fn:chen2024rh20t">
<p>Z. Chen <em>et al.</em>, "RH20T-p: A primitive-level robotic manipulation dataset towards composable generalization agents in real-world scenarios," in <em>NeurIPS 2024 workshop on open-world agents</em>, 2024.&nbsp;<a class="footnote-backref" href="#fnref:chen2024rh20t" title="Jump back to footnote 6 in the text">↩</a><a class="footnote-backref" href="#fnref2:chen2024rh20t" title="Jump back to footnote 6 in the text">↩</a></p>
</li>
<li id="fn:bu2025agibot">
<p>Q. Bu <em>et al.</em>, "Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems," <em>arXiv preprint arXiv:2503.06669</em>, 2025.&nbsp;<a class="footnote-backref" href="#fnref:bu2025agibot" title="Jump back to footnote 7 in the text">↩</a></p>
</li>
<li id="fn:miech2019howto100m">
<p>A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips," in <em>Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 2630--2640.&nbsp;<a class="footnote-backref" href="#fnref:miech2019howto100m" title="Jump back to footnote 8 in the text">↩</a><a class="footnote-backref" href="#fnref2:miech2019howto100m" title="Jump back to footnote 8 in the text">↩</a></p>
</li>
<li id="fn:goyal2017something">
<p>R. Goyal <em>et al.</em>, "The\" something something\" video database for learning and evaluating visual common sense," in <em>Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 5842--5850.&nbsp;<a class="footnote-backref" href="#fnref:goyal2017something" title="Jump back to footnote 9 in the text">↩</a><a class="footnote-backref" href="#fnref2:goyal2017something" title="Jump back to footnote 9 in the text">↩</a><a class="footnote-backref" href="#fnref3:goyal2017something" title="Jump back to footnote 9 in the text">↩</a><a class="footnote-backref" href="#fnref4:goyal2017something" title="Jump back to footnote 9 in the text">↩</a></p>
</li>
<li id="fn:souvcek2024genhowto">
<p>T. Souček, D. Damen, M. Wray, I. Laptev, and J. Sivic, "Genhowto: Learning to generate actions and state transformations from instructional videos," in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2024, pp. 6561--6571.&nbsp;<a class="footnote-backref" href="#fnref:souvcek2024genhowto" title="Jump back to footnote 10 in the text">↩</a></p>
</li>
<li id="fn:carreira2019short">
<p>J. Carreira, E. Noland, C. Hillier, and A. Zisserman, "A short note on the kinetics-700 human action dataset," <em>arXiv preprint arXiv:1907.06987</em>, 2019.&nbsp;<a class="footnote-backref" href="#fnref:carreira2019short" title="Jump back to footnote 11 in the text">↩</a><a class="footnote-backref" href="#fnref2:carreira2019short" title="Jump back to footnote 11 in the text">↩</a></p>
</li>
<li id="fn:cheang2024gr">
<p>C.-L. Cheang <em>et al.</em>, "Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation," <em>arXiv preprint arXiv:2410.06158</em>, 2024.&nbsp;<a class="footnote-backref" href="#fnref:cheang2024gr" title="Jump back to footnote 12 in the text">↩</a><a class="footnote-backref" href="#fnref2:cheang2024gr" title="Jump back to footnote 12 in the text">↩</a></p>
</li>
<li id="fn:zhen20243d">
<p>H. Zhen <em>et al.</em>, "3D-VLA: A 3D vision-language-action generative world model," in <em>Proceedings of the 41st international conference on machine learning</em>, 2024, pp. 61229--61245.&nbsp;<a class="footnote-backref" href="#fnref:zhen20243d" title="Jump back to footnote 13 in the text">↩</a><a class="footnote-backref" href="#fnref2:zhen20243d" title="Jump back to footnote 13 in the text">↩</a><a class="footnote-backref" href="#fnref3:zhen20243d" title="Jump back to footnote 13 in the text">↩</a></p>
</li>
<li id="fn:grauman2022ego4d">
<p>K. Grauman <em>et al.</em>, "Ego4d: Around the world in 3,000 hours of egocentric video," in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 18995--19012.&nbsp;<a class="footnote-backref" href="#fnref:grauman2022ego4d" title="Jump back to footnote 14 in the text">↩</a><a class="footnote-backref" href="#fnref2:grauman2022ego4d" title="Jump back to footnote 14 in the text">↩</a><a class="footnote-backref" href="#fnref3:grauman2022ego4d" title="Jump back to footnote 14 in the text">↩</a><a class="footnote-backref" href="#fnref4:grauman2022ego4d" title="Jump back to footnote 14 in the text">↩</a></p>
</li>
<li id="fn:grauman2024ego">
<p>K. Grauman <em>et al.</em>, "Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives," in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2024, pp. 19383--19400.&nbsp;<a class="footnote-backref" href="#fnref:grauman2024ego" title="Jump back to footnote 15 in the text">↩</a><a class="footnote-backref" href="#fnref2:grauman2024ego" title="Jump back to footnote 15 in the text">↩</a><a class="footnote-backref" href="#fnref3:grauman2024ego" title="Jump back to footnote 15 in the text">↩</a></p>
</li>
<li id="fn:damen2018scaling">
<p>D. Damen <em>et al.</em>, "Scaling egocentric vision: The epic-kitchens dataset," in <em>Proceedings of the european conference on computer vision (ECCV)</em>, 2018, pp. 720--736.&nbsp;<a class="footnote-backref" href="#fnref:damen2018scaling" title="Jump back to footnote 16 in the text">↩</a><a class="footnote-backref" href="#fnref2:damen2018scaling" title="Jump back to footnote 16 in the text">↩</a></p>
</li>
<li id="fn:zhen2025tesseract">
<p>H. Zhen <em>et al.</em>, "TesserAct: Learning 4D embodied world models," <em>arXiv preprint arXiv:2504.20995</em>, 2025.&nbsp;<a class="footnote-backref" href="#fnref:zhen2025tesseract" title="Jump back to footnote 17 in the text">↩</a><a class="footnote-backref" href="#fnref2:zhen2025tesseract" title="Jump back to footnote 17 in the text">↩</a></p>
</li>
<li id="fn:zhi20253dflowaction">
<p>H. Zhi <em>et al.</em>, "3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model," <em>arXiv preprint arXiv:2506.06199</em>, 2025.&nbsp;<a class="footnote-backref" href="#fnref:zhi20253dflowaction" title="Jump back to footnote 18 in the text">↩</a></p>
</li>
<li id="fn:ramakrishnan22021habitat">
<p>S. K. Ramakrishnan <em>et al.</em>, "Habitat-matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI," in <em>Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2)</em>, 2021.&nbsp;<a class="footnote-backref" href="#fnref:ramakrishnan22021habitat" title="Jump back to footnote 19 in the text">↩</a><a class="footnote-backref" href="#fnref2:ramakrishnan22021habitat" title="Jump back to footnote 19 in the text">↩</a></p>
</li>
<li id="fn:chang2017matterport3d">
<p>A. Chang <em>et al.</em>, "Matterport3D: Learning from RGB-d data in indoor environments," in <em>2017 international conference on 3D vision (3DV)</em>, IEEE Computer Society, 2017, pp. 667--676.&nbsp;<a class="footnote-backref" href="#fnref:chang2017matterport3d" title="Jump back to footnote 20 in the text">↩</a><a class="footnote-backref" href="#fnref2:chang2017matterport3d" title="Jump back to footnote 20 in the text">↩</a><a class="footnote-backref" href="#fnref3:chang2017matterport3d" title="Jump back to footnote 20 in the text">↩</a></p>
</li>
<li id="fn:hong2024multiply">
<p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, "Multiply: A multisensory object-centric embodied large language model in 3d world," in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2024, pp. 26406--26416.&nbsp;<a class="footnote-backref" href="#fnref:hong2024multiply" title="Jump back to footnote 21 in the text">↩</a><a class="footnote-backref" href="#fnref2:hong2024multiply" title="Jump back to footnote 21 in the text">↩</a></p>
</li>
<li id="fn:o2024open">
<p>A. O'Neill <em>et al.</em>, "Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0," in <em>2024 IEEE international conference on robotics and automation (ICRA)</em>, IEEE, 2024, pp. 6892--6903.&nbsp;<a class="footnote-backref" href="#fnref:o2024open" title="Jump back to footnote 22 in the text">↩</a><a class="footnote-backref" href="#fnref2:o2024open" title="Jump back to footnote 22 in the text">↩</a></p>
</li>
<li id="fn:gu2023maniskill2">
<p>J. Gu <em>et al.</em>, "ManiSkill2: A unified benchmark for generalizable manipulation skills," in <em>The eleventh international conference on learning representations</em>, 2023.&nbsp;<a class="footnote-backref" href="#fnref:gu2023maniskill2" title="Jump back to footnote 23 in the text">↩</a><a class="footnote-backref" href="#fnref2:gu2023maniskill2" title="Jump back to footnote 23 in the text">↩</a></p>
</li>
<li id="fn:yang2023learning">
<p>M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, "Learning interactive real-world simulators," <em>arXiv preprint arXiv:2310.06114</em>, vol. 1, no. 2, p. 6, 2023.&nbsp;<a class="footnote-backref" href="#fnref:yang2023learning" title="Jump back to footnote 24 in the text">↩</a></p>
</li>
<li id="fn:schuhmann2021laion">
<p>C. Schuhmann <em>et al.</em>, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," <em>arXiv preprint arXiv:2111.02114</em>, 2021.&nbsp;<a class="footnote-backref" href="#fnref:schuhmann2021laion" title="Jump back to footnote 25 in the text">↩</a><a class="footnote-backref" href="#fnref2:schuhmann2021laion" title="Jump back to footnote 25 in the text">↩</a></p>
</li>
<li id="fn:bruce2024genie">
<p>J. Bruce <em>et al.</em>, "Genie: Generative interactive environments," in <em>Proceedings of the 41st international conference on machine learning</em>, 2024, pp. 4603--4623.&nbsp;<a class="footnote-backref" href="#fnref:bruce2024genie" title="Jump back to footnote 26 in the text">↩</a></p>
</li>
<li id="fn:brohan2023rt">
<p>A. Brohan <em>et al.</em>, "RT-1: Robotics transformer for real-world control at scale," <em>Robotics: Science and Systems</em>, 2023.&nbsp;<a class="footnote-backref" href="#fnref:brohan2023rt" title="Jump back to footnote 27 in the text">↩</a><a class="footnote-backref" href="#fnref2:brohan2023rt" title="Jump back to footnote 27 in the text">↩</a><a class="footnote-backref" href="#fnref3:brohan2023rt" title="Jump back to footnote 27 in the text">↩</a><a class="footnote-backref" href="#fnref4:brohan2023rt" title="Jump back to footnote 27 in the text">↩</a></p>
</li>
<li id="fn:kalashnikov2018scalable">
<p>D. Kalashnikov <em>et al.</em>, "Scalable deep reinforcement learning for vision-based robotic manipulation," in <em>Conference on robot learning</em>, 2018, pp. 651--673.&nbsp;<a class="footnote-backref" href="#fnref:kalashnikov2018scalable" title="Jump back to footnote 28 in the text">↩</a></p>
</li>
<li id="fn:wu2024ivideogpt">
<p>J. Wu <em>et al.</em>, "Ivideogpt: Interactive videogpts are scalable world models," <em>Advances in Neural Information Processing Systems</em>, vol. 37, pp. 68082--68119, 2024.&nbsp;<a class="footnote-backref" href="#fnref:wu2024ivideogpt" title="Jump back to footnote 29 in the text">↩</a></p>
</li>
<li id="fn:ho2020denoising">
<p>J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," <em>Advances in neural information processing systems</em>, vol. 33, pp. 6840--6851, 2020.&nbsp;<a class="footnote-backref" href="#fnref:ho2020denoising" title="Jump back to footnote 30 in the text">↩</a></p>
</li>
<li id="fn:kirillov2023segment">
<p>A. Kirillov <em>et al.</em>, "Segment anything," in <em>Proceedings of the IEEE/CVF international conference on computer vision</em>, 2023, pp. 4015--4026.&nbsp;<a class="footnote-backref" href="#fnref:kirillov2023segment" title="Jump back to footnote 31 in the text">↩</a></p>
</li>
<li id="fn:ma2025latte">
<p>X. Ma <em>et al.</em>, "Latte: Latent diffusion transformer for video generation," <em>Transactions on Machine Learning Research</em>, 2025.&nbsp;<a class="footnote-backref" href="#fnref:ma2025latte" title="Jump back to footnote 32 in the text">↩</a></p>
</li>
<li id="fn:du2023learning">
<p>Y. Du <em>et al.</em>, "Learning universal policies via text-guided video generation," <em>Advances in neural information processing systems</em>, vol. 36, pp. 9156--9172, 2023.&nbsp;<a class="footnote-backref" href="#fnref:du2023learning" title="Jump back to footnote 33 in the text">↩</a></p>
</li>
<li id="fn:ho2022imagen">
<p>J. Ho <em>et al.</em>, "Imagen video: High definition video generation with diffusion models," <em>arXiv preprint arXiv:2210.02303</em>, 2022.&nbsp;<a class="footnote-backref" href="#fnref:ho2022imagen" title="Jump back to footnote 34 in the text">↩</a></p>
</li>
<li id="fn:huang2025enerverse">
<p>S. Huang <em>et al.</em>, "Enerverse: Envisioning embodied future space for robotics manipulation," <em>arXiv preprint arXiv:2501.01895</em>, 2025.&nbsp;<a class="footnote-backref" href="#fnref:huang2025enerverse" title="Jump back to footnote 35 in the text">↩</a></p>
</li>
<li id="fn:rosete2023latent">
<p>E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Burgard, "Latent plans for task-agnostic offline reinforcement learning," in <em>Conference on robot learning</em>, 2023, pp. 1838--1849.&nbsp;<a class="footnote-backref" href="#fnref:rosete2023latent" title="Jump back to footnote 36 in the text">↩</a></p>
</li>
<li id="fn:lynch2023interactive">
<p>C. Lynch <em>et al.</em>, "Interactive language: Talking to robots in real time," <em>IEEE Robotics and Automation Letters</em>, 2023.&nbsp;<a class="footnote-backref" href="#fnref:lynch2023interactive" title="Jump back to footnote 37 in the text">↩</a></p>
</li>
<li id="fn:mittal2023orbit">
<p>M. Mittal <em>et al.</em>, "Orbit: A unified simulation framework for interactive robot learning environments," <em>IEEE Robotics and Automation Letters</em>, vol. 8, no. 6, pp. 3740--3747, 2023.&nbsp;<a class="footnote-backref" href="#fnref:mittal2023orbit" title="Jump back to footnote 38 in the text">↩</a></p>
</li>
<li id="fn:james2020rlbench">
<p>S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, "Rlbench: The robot learning benchmark\ &amp; learning environment," <em>IEEE Robotics and Automation Letters</em>, vol. 5, no. 2, pp. 3019--3026, 2020.&nbsp;<a class="footnote-backref" href="#fnref:james2020rlbench" title="Jump back to footnote 39 in the text">↩</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../06-core-components/" class="md-footer__link md-footer__link--prev" aria-label="Previous: VI Core Components &amp; Capabilities">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                VI Core Components &amp; Capabilities
              </div>
            </div>
          </a>
        
        
          
          <a href="../08-conclusion/" class="md-footer__link md-footer__link--next" aria-label="Next: VIII Conclusion &amp; Future directions">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                VIII Conclusion &amp; Future directions
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.path", "navigation.top", "toc.integrate", "toc.follow", "search.suggest", "search.highlight", "content.code.copy", "header.autohide", "navigation.footer"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"></script>
      
        <script src="../js/katex-init.js"></script>
      
        <script src="../js/strip-nocase.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>