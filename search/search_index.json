{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"A Step Toward World Models: A Survey on Robotic Manipulation <p>Peng-Fei Zhang, Ying Cheng, Xiaofan Sun, Shijie Wang, Fengling Li, Lei Zhu, Heng Tao Shen</p> <p>School of Computer Science and Technology, Tongji University</p> Read the Survey References Contact <p>Quick links</p> <ul> <li> Start reading Overview &amp; roadmap  Go to \u201cI Introduction\u201d</li> <li> Datasets Benchmarks &amp; curation  Go to \u201cVII Dataset\u201d</li> <li> Key challenges Open problems &amp; insights  Read \u201cV Key Techniques\u201d</li> <li> Core components Perception \u2192 World model \u2192 Action  Explore \u201cVI Components\u201d</li> </ul>"},{"location":"01-introduction/","title":"I Introduction","text":""},{"location":"01-introduction/#i-introduction","title":"I Introduction","text":"<p> \u201cIf I have seen further, it is by standing on the shoulders of giants.\u201d \u2003\u2003\u2003\u2014\u2014 Isaac Newton</p> <p>\u2003\u2003Autonomous agents are designed to extend human capabilities, assisting in tasks that are dangerous, repetitive, or demand high precision, thereby enhancing productivity across diverse applications. Achieving such capabilities requires agents to move beyond reactive control and the mere replication of observed states, instead developing the ability to model, reason about, and predict environmental dynamics. In this context, world models have emerged as powerful internal representations that enable robots to anticipate future outcomes, support effective decision-making, and ultimately act intelligently in the real world. Richens et al.<sup>1</sup> argue that any agent capable of generalizing to solve multi-step tasks must implicitly learn a predictive model of its environment, e.g., a world model. </p> <p>\u2003\u2003The concept of world models in computer science dates back to the 1960s <sup>2</sup>, and numerous methods have since been proposed as steps toward more capable models  <sup>3</sup> <sup>4</sup> <sup>5</sup> <sup>6</sup> <sup>7</sup> <sup>8</sup> <sup>9</sup> <sup>10</sup>, although not all of these works explicitly identify themselves as world models. For example, Wang et al. <sup>9</sup> <sup>10</sup> <sup>5</sup> <sup>6</sup> <sup>7</sup> leverage video generation models as a form of world models, which encode extensive world knowledge from large-scale training data and can predict future states based current observations and/or actions. LeCun et al. <sup>11</sup> <sup>12</sup> <sup>13</sup> <sup>14</sup> <sup>15</sup> emphasizes modeling abstract world state representations, while Zitkovich et al. <sup>16</sup> <sup>17</sup> <sup>18</sup> utilize vision-language-action models (VLA) models that do not explicitly generate future states. The scope of existing methods varies from 2D scene prediction to 4D world modeling <sup>19</sup> <sup>20</sup> <sup>21</sup> <sup>22</sup> <sup>23</sup>, reflecting different understandings of what it means to model the world. The observation viewpoint of the world includes both third-person (exocentric) <sup>21</sup> <sup>24</sup> <sup>25</sup> and first-person (egocentric) <sup>26</sup> <sup>27</sup> perspectives.</p> <p> </p> Fig. 1. Conceptual flow of the survey. The survey aims to clarify the motivation behind world modeling, explore its essential scope and development, and illuminate pathways toward more general and capable world models. <p>\u2003\u2003World models play a critical role in robotic learning in two ways. They allow robots to improve autonomous policies by simulating multiple action proposals and selecting the optimal one for execution <sup>28</sup> <sup>12</sup> <sup>13</sup> <sup>14</sup> <sup>29</sup> <sup>21</sup>. They also support scalable policy training and evaluation by generating realistic rollouts and physical interactions, providing an efficient alternative to collecting data in the real world. <sup>16</sup> <sup>30</sup> <sup>31</sup> <sup>32</sup> <sup>33</sup> <sup>34</sup>. From a functional standpoint, current approaches range from single-purpose models, such as those designed for visual planning <sup>29</sup> <sup>35</sup>, future-scene generation <sup>36</sup> <sup>32</sup> <sup>37</sup>, or action prediction <sup>38</sup>, to more integrated systems that couple multiple abilities within a unified framework <sup>39</sup> <sup>26</sup> <sup>40</sup> <sup>41</sup> <sup>42</sup>.</p> <p>\u2003\u2003These variations indicate that the notion of a world model remains unsettled, with its conceptual, architectural, and functional boundaries not yet clearly defined.  </p> <p>\u2003\u2003Addressing these questions requires standing on the shoulders of prior contributions, carefully analyzing existing methodologies to gain inspiration for elucidating the boundaries of world models. In this survey, rather than hastily defining what constitutes a world model, we provide a comprehensive review of the literature, highlighting their core principles, architectures, and functional roles in enabling intelligent robotic systems. We extend the scope beyond works explicitly labeled as world models, examining their core principles and outlining pathways for constructing practical models that can drive the development of general and adaptive robotic agents. </p> <p>\u2003\u2003This survey is organized around a set of guiding questions designed to provoke thought and provide inspiration. Readers can explore the survey with these questions in mind, using them to provoke thought, gain inspiration, and reflect on the challenges and opportunities in developing world models for robotic manipulation.</p> <ul> <li>What is the world model and its conceptual, architectural, and functional boundaries? \u2022 How should the world be sensed and presented? </li> <li>What level of model fidelity and coverage is required to reliably support robotic tasks? </li> <li>Is it necessary to learn a world model, given the complexity, resource demands, and potential challenges involved? </li> <li>How far are current world models from fully realized world models? </li> <li>Is human cognition <sup>43</sup> <sup>44</sup> the ultimate goal for world models?</li> </ul> <p>\u2003\u2003The main contributions of this survey are as follows:  </p> <ul> <li>Comprehensive taxonomy of world model architectures. We provide a systematic categorization of existing designs, including latent space modeling methods, video generation-based models, direct projection based methods and other emerging structures. </li> <li>Functional analysis. We discuss the diverse roles of world models in robotics, including robotic learning, evaluation, and planning, highlighting their contribution to autonomous control.  </li> <li>Capability framework. We analyze the essential abilities that a world model should possess, such as perception, prediction, imagination, and interaction, aiming to clarify what constitutes a generalizable and capable world model. </li> <li>Challenges and future directions. We summarize key challenges, including scalability, physical awareness, and generalization, and discuss potential research directions and solutions toward building practical, real-world models.</li> </ul> <p> Related Surveys. Our survey differs substantially from existing reviews. Several surveys have examined world models in robotics, but most focus on specific aspects and provide limited conceptual analysis. For example, Yu et al. <sup>45</sup> emphasize video generation, Kong et al. <sup>46</sup> cover 3D/4D world modeling, Ai et al. <sup>47</sup> study dynamics learning, and Lin et al. <sup>48</sup> address physics cognition. Long et al. <sup>49</sup> review architectures and functional roles of world models, whereas Zhu et al. <sup>50</sup> <sup>51</sup> <sup>52</sup> primarily compile representative works. While these surveys provide valuable overviews, they offer limited discussion of the essential characteristics and functional requirements of comprehensive world models for embodied agents. In contrast, our survey presents a holistic, problem-centered perspective, highlighting key challenges, solution strategies, and future directions for world modeling in robotics.</p> <p> Paper Organization. The remainder of this paper is organized as follows. Section II introduces the conceptual foundations of world models. Section III provides an overview of current world models, including their learning paradigms, structural designs, representations of the world, and task scopes. Sections IV and V describe the key functions of existing world models and summarize the principal techniques and challenges, respectively. Section VII reviews the major training resources used in world-model research. Section VI then summarizes the fundamental components and capabilities of world models based on this review, followed by Section VIII, which presents conclusions and outlines future research directions. Although this may occasionally lead to some repetition, certain key ideas are revisited throughout the paper to aid understanding and reinforce their conceptual connections.</p>"},{"location":"01-introduction/#references","title":"References","text":"<ol> <li> <p>J. Richens, T. Everitt, and D. Abel, \"General agents need world models,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>M. MINSKY, \"A framework for representing knowledge,\" The psychology of computer vision, pp. 211--277, 1975.\u00a0\u21a9</p> </li> <li> <p>J. Schmidhuber, \"On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models,\" arXiv preprint arXiv:1511.09249, 2015.\u00a0\u21a9</p> </li> <li> <p>D. Ha and J. Schmidhuber, \"World models,\" arXiv preprint arXiv:1803.10122, 2018.\u00a0\u21a9</p> </li> <li> <p>Y. Du et al., \"Video language planning,\" arXiv preprint arXiv:2310.10625, 2023.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Wu et al., \"Ivideogpt: Interactive videogpts are scalable world models,\" Advances in Neural Information Processing Systems, vol. 37, pp. 68082--68119, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Zhang et al., \"COMBO: Compositional world models for embodied multi-agent cooperation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Huang et al., \"Enerverse: Envisioning embodied future space for robotics manipulation,\" arXiv preprint arXiv:2501.01895, 2025.\u00a0\u21a9</p> </li> <li> <p>B. Wang et al., \"This\\ &amp;that: Language-gesture controlled video generation for robot planning,\" in 2025 IEEE international conference on robotics and automation (ICRA), IEEE, 2025, pp. 12842--12849.\u00a0\u21a9\u21a9</p> </li> <li> <p>L. Yang et al., \"RoboEnvision: A long-horizon video generation model for multi-task robot manipulation,\" arXiv preprint arXiv:2506.22007, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. LeCun, \"A path towards autonomous machine intelligence,\" Open Review, vol. 62, no. 1, pp. 1--62, 2022.\u00a0\u21a9</p> </li> <li> <p>D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, \"Dream to control: Learning behaviors by latent imagination,\" in International conference on learning representations, 2019.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, \"Mastering atari with discrete world models,\" in International conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, \"Daydreamer: World models for physical robot learning,\" in Conference on robot learning, PMLR, 2023, pp. 2226--2240.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse domains through world models,\" arXiv preprint arXiv:2301.04104, 2023.\u00a0\u21a9</p> </li> <li> <p>B. Zitkovich et al., \"Rt-2: Vision-language-action models transfer web knowledge to robotic control,\" in Conference on robot learning, PMLR, 2023, pp. 2165--2183.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Huang et al., \"An embodied generalist agent in 3D world,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 20413--20451.\u00a0\u21a9</p> </li> <li> <p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, \"Multiply: A multisensory object-centric embodied large language model in 3d world,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 26406--26416.\u00a0\u21a9</p> </li> <li> <p>M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, \"Learning interactive real-world simulators,\" arXiv preprint arXiv:2310.06114, vol. 1, no. 2, p. 6, 2023.\u00a0\u21a9</p> </li> <li> <p>J. Bruce et al., \"Genie: Generative interactive environments,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 4603--4623.\u00a0\u21a9</p> </li> <li> <p>J. Guo, X. Ma, Y. Wang, M. Yang, H. Liu, and Q. Li, \"FlowDreamer: A RGB-d world model with flow-based motion representations for robot manipulation,\" arXiv preprint arXiv:2505.10075, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Q. Bu et al., \"Closed-loop visuomotor control with generative expectation for robotic manipulation,\" Advances in Neural Information Processing Systems, vol. 37, pp. 139002--139029, 2024.\u00a0\u21a9</p> </li> <li> <p>H. Zhu et al., \"Aether: Geometric-aware unified world modeling,\" in ICCV, 2025.\u00a0\u21a9</p> </li> <li> <p>S. Ferraro, P. Mazzaglia, T. Verbelen, and B. Dhoedt, \"FOCUS: Object-centric world models for robotic manipulation,\" Frontiers in Neurorobotics, vol. 19, p. 1585386, 2025.\u00a0\u21a9</p> </li> <li> <p>A. Villar-Corrales and S. Behnke, \"PlaySlot: Learning inverse latent dynamics for controllable object-centric video prediction and planning,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Chen et al., \"EgoAgent: A joint predictive agent model in egocentric worlds,\" arXiv preprint arXiv:2502.05857, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>K. Grauman et al., \"Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 19383--19400.\u00a0\u21a9</p> </li> <li> <p>D. Hafner et al., \"Learning latent dynamics for planning from pixels,\" in International conference on machine learning, 2019, pp. 2555--2565.\u00a0\u21a9</p> </li> <li> <p>F. Ebert, S. Dasari, A. X. Lee, S. Levine, and C. Finn, \"Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning,\" in Conference on robot learning, PMLR, 2018, pp. 983--993.\u00a0\u21a9\u21a9</p> </li> <li> <p>K. Black et al., \"\\(\\pi\\_0\\): A vision-language-action flow model for general robot control,\" arXiv preprint arXiv:2410.24164, 2024.\u00a0\u21a9</p> </li> <li> <p>G. R. Team et al., \"Gemini robotics: Bringing ai into the physical world,\" arXiv preprint arXiv:2503.20020, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, \"Dream to manipulate: Compositional world models empowering robot imitation learning with imagination,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>G. Lu et al., \"GWM: Towards scalable gaussian world models for robotic manipulation,\" arXiv preprint arXiv:2508.17600, 2025.\u00a0\u21a9</p> </li> <li> <p>S. Liu et al., \"RDT-1B: A diffusion foundation model for bimanual manipulation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>C. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" in Proceedings of the 30th international conference on neural information processing systems, 2016, pp. 64--72.\u00a0\u21a9</p> </li> <li> <p>S. Sudhakar, R. Liu, B. V. Hoorick, C. Vondrick, and R. Zemel, \"Controlling the world by sleight of hand,\" in European conference on computer vision, Springer, 2024, pp. 414--430.\u00a0\u21a9</p> </li> <li> <p>J. Jang et al., \"DreamGen: Unlocking generalization in robot learning through video world models,\" arXiv preprint arXiv:2505.12705, 2025.\u00a0\u21a9</p> </li> <li> <p>R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, \"Planning to explore via self-supervised world models,\" in International conference on machine learning, 2020, pp. 8583--8592.\u00a0\u21a9</p> </li> <li> <p>J. Cen et al., \"WorldVLA: Towards autoregressive action world model,\" arXiv preprint arXiv:2506.21539, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Zhen et al., \"3D-VLA: A 3D vision-language-action generative world model,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 61229--61245.\u00a0\u21a9</p> </li> <li> <p>Z. Song, S. Qin, T. Chen, L. Lin, and G. Wang, \"Physical autoregressive model for robotic manipulation without action pretraining,\" arXiv preprint arXiv:2508.09822, 2025.\u00a0\u21a9</p> </li> <li> <p>F. Zhu, H. Wu, S. Guo, Y. Liu, C. Cheang, and T. Kong, \"Irasim: Learning interactive real-robot action simulators,\" in ICCV, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Bjorck et al., \"Gr00t n1: An open foundation model for generalist humanoid robots,\" arXiv preprint arXiv:2503.14734, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Wang, R. Shelim, W. Saad, and N. Ramakrishnan, \"DMWM: Dual-mind world model with long-term imagination,\" arXiv preprint arXiv:2502.07591, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Yu et al., \"A survey of interactive generative video,\" arXiv preprint arXiv:2504.21853, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Kong et al., \"3D and 4D world modeling: A survey,\" arXiv preprint arXiv:2509.07996, 2025.\u00a0\u21a9</p> </li> <li> <p>B. Ai et al., \"A review of learning-based dynamics models for robotic manipulation,\" Science Robotics, vol. 10, no. 106, p. eadt1497, 2025.\u00a0\u21a9</p> </li> <li> <p>M. Lin et al., \"Exploring the evolution of physics cognition in video generation: A survey,\" arXiv preprint arXiv:2503.21765, 2025.\u00a0\u21a9</p> </li> <li> <p>X. Long et al., \"A survey: Learning embodied intelligence from physical simulators and world models,\" arXiv preprint arXiv:2507.00917, 2025.\u00a0\u21a9</p> </li> <li> <p>Z. Zhu et al., \"Is sora a world simulator? A comprehensive survey on general world models and beyond,\" arXiv preprint arXiv:2405.03520, 2024.\u00a0\u21a9</p> </li> <li> <p>W. Liang et al., \"Large model empowered embodied AI: A survey on decision-making and embodied learning,\" arXiv preprint arXiv:2508.10399, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Ding et al., \"Understanding world or predicting future? A comprehensive survey of world models,\" ACM Computing Surveys, vol. 58, no. 3, pp. 1--38, 2025.\u00a0\u21a9</p> </li> </ol>"},{"location":"02-preliminaries/","title":"II Preliminaries","text":""},{"location":"02-preliminaries/#ii-preliminaries","title":"II Preliminaries","text":""},{"location":"02-preliminaries/#a-what-is-the-world-to-be-modeled","title":"A. What Is the \u201cWorld\u201d to Be Modeled?","text":"<p>\u2003\u2003Despite the debate among philosophers about the ultimate nature of the world, the world can be roughly described as a set of entities, each with its own attributes or properties, along with the relationships and interactions that connect them. These attributes, such as shape, size, material, or state, and the connections, which can be spatial, causal, functional, or temporal, mean that objects, agents, and features are not only statically arranged but also evolve and influence one another over time. In order to interact effect-ively with such a world, an intelligent agent must capture critical information about entities, their properties, and their interactions. Collectively, these entities and interactions form a rich and dynamic environment in which an agent must actively explore, interact, and learn to achieve its goals. This naturally raises the question of what fundamental capabilities underpin an agent\u2019s ability to capture and reason about such complex dynamics, as well as what forms of representation, learning, and interaction are required to model and act within an uncertain and evolving world.</p>"},{"location":"02-preliminaries/#table-i","title":"Table \u2160","text":""},{"location":"02-preliminaries/#a-summary-of-representative-world-models","title":"A Summary of Representative World Models","text":"<ul> <li>Prediction tasks: <code>AP</code> \u2014 Action Prediction; <code>PL</code> \u2014 Policy Learning; <code>VP</code> \u2014 Visual Planning; <code>Static</code> \u2014 Static Visual Prediction; <code>Action-cond.</code> \u2014 Action-conditioned Visual Prediction.</li> <li>Input &amp; Output: <code>L</code> \u2014 Language; <code>V</code> \u2014 Video; <code>A</code> \u2014 Action; <code>S</code> \u2014 State; <code>I</code> \u2014 Image; <code>P</code> \u2014 Point Cloud; <code>Tr</code> \u2014 Trajectory; <code>Ar</code> \u2014 Autoregressive.</li> <li>Core components: <code>CLIP</code> \u2014 Contrastive Language-Image Pre-training; <code>DiT</code> \u2014 Diffusion Transformer; <code>IDM</code> \u2014 Inverse Dynamics Model; <code>GPT</code> \u2014 Generative Pre-trained Transformer; <code>LLM</code> \u2014 Large Language Model; <code>LSTM</code> \u2014 Long Short-Term Memory; <code>RSSM</code> \u2014 Recurrent State-Space Model; <code>U-Net</code> \u2014 U-shaped Convolutional Neural Network; <code>VAE</code> \u2014 Variational AutoEncoder; <code>VDM</code> \u2014 Video Diffusion Model; <code>ViT</code> \u2014 Vision Transformer; <code>VLM</code> \u2014 Vision-Language Model; <code>VQ</code> \u2014 Vector Quantization.</li> </ul>"},{"location":"02-preliminaries/#b-world-models-empowering-robot-intelligence","title":"B. World Models Empowering Robot Intelligence","text":"<p>\u2003\u2003Embodied intelligence refers to a system\u2019s ability to perceive, reason, and learn through direct interaction with its environment. Unlike traditional AI confined to abstract or symbolic domains, embodied intelligence integrates a physical body, sensors, actuators, and computational processes that together enable situated perception, reasoning, and action. Intelligent robotic agents serve as the primary physical instantiation of embodied intelligence. They inherently combine perception (via sensors), cognition (for learning and reasoning), and motor control (via actuators) to operate autonomously and acquire knowledge from real-world experience, much like biological organisms.  </p> <p>\u2003\u2003However, because intelligent agents perceive only a partial and noisy projection of reality through their sensors, many underlying relationships and causal dependencies remain latent. This limitation makes structured internal representations essential for prediction, planning, and multi-step reasoning. To achieve robust and efficient embodied intelligence, recent research introduces the notion of world models, which serve as internal representations that capture environmental dynamics and common-sense regularities of how the world operates. By internally simulating potential outcomes, world models empower embodied agents to understand their context, anticipate the consequences of actions, and plan complex behaviors before execution, thereby reducing reliance on costly or unsafe real-world trial and error.</p>"},{"location":"02-preliminaries/#c-competing-perspectives-on-world-models","title":"C. Competing Perspectives on World Models","text":"<p>\u2003\u2003Although the concept of a \u201cworld model\u201d is prevalent in computer science, its definition remains unsettled, with ongoing debate in the research community regarding its fundamental nature and role in intelligence <sup>1</sup>. A central point of contention concerns its generative capability, as illustrated by NVIDIA <sup>2</sup>, who define world models as systems that learn environmental dynamics from multimodal data and generate videos capturing spatial and physical properties. Emphasizing action dependence, Sudhakar et al. <sup>3</sup> <sup>4</sup> characterize world models specifically as action-conditioned video generation models, distinguishing them from conventional video prediction. Similarly, Hafner et al. <sup>5</sup> <sup>6</sup> <sup>7</sup> <sup>8</sup> <sup>9</sup> identify action-conditioned prediction as a core feature of world models, emphasizing the prediction of latent representations rather than raw observations. Despite these differing perspectives, a common consensus emerges: world models aim to construct internal representations that capture environmental dynamics and action consequences, thereby enabling the prediction of future states.</p> <p> </p> Fig. 1. A visualization of an agent <sup>10</sup>, where the world model predicts possible future world states as a function of imagined actions sequences proposed by the actor."},{"location":"02-preliminaries/#d-revisiting-modern-ai-models-through-the-lens-of-world-modeling","title":"D. Revisiting Modern AI Models Through the Lens of World Modeling","text":"<p>\u2003\u2003The rapid progress of large-scale artificial intelligence models has blurred the boundaries between traditional task-specific learning and general world modeling. Although many of these models are not explicitly designed as world models, they exhibit key characteristics of world modeling, such as learning structured representations of reality, reasoning about causality, and predicting or generating plausible future states. Revisiting these modern models through the lens of world modeling provides valuable insights into how intelligence emerges from data, embodiment, and multimodal integration. This perspective helps clarify which components of contemporary architectures, such as large language models (LLMs), vision-language models (VLMs), vision-language-action models (VLAs) and video generation models, implicitly capture aspects of the world and how they contribute to the broader goal of generalpurpose world understanding. </p> <p> 1) LLMs, VLMs &amp; VLA </p> <p>\u2003\u2003 The strong reasoning capabilities and next-token prediction mechanism of Large Language Models (LLMs) make them natural foundations for constructing world models, as they capture sequential dependencies, causal relationships, and abstract dynamics. When equipped with auxiliary modules such as value functions <sup>11</sup> or modality-specific encoders <sup>12</sup> <sup>13</sup> <sup>14</sup>, LLMs can achieve a more comprehensive understanding of the environment. Moving beyond the language-centric paradigm, Vision\u2013Language Models (VLMs) focus on the joint modeling of multiple modalities, providing a perceptually grounded understanding of the world <sup>15</sup> <sup>16</sup>. Furthermore, an increasing number of studies have explored augmenting VLMs with low-level action-generation capabilities, thereby transforming them into Vision\u2013Language\u2013Action (VLA) models <sup>17</sup> <sup>18</sup> <sup>19</sup> that bridge perception, reasoning, and control. In addition to action generation, there are also many works that enable additional visual prediction <sup>20</sup> <sup>21</sup>.</p> <p>\u2003\u2003 From the above discussion, the designs and functions of LLMs, VLMs, and VLAs align with the spirit of world models, as they aim to represent and reason about world dynamics. Therefore, these models should not be excluded from the broader conceptual scope of world modeling. However, solely relying on LLMs, VLMs, or VLAs often constrains a system\u2019s capacity for long-horizon prediction, reasoning, and imagination, all of which are essential for modeling dynamic and interactive environments. Recent studies have thus begun to integrate these models into architectures that explicitly function as world models, such as the JEPA framework <sup>22</sup>, Dreamer-style frameworks <sup>23</sup>, positioning them as core mechanisms for capturing temporal and causal dynamics.</p> <p> 2) Video Generation Models. </p> <p>\u2003\u2003Video generation models primarily aim to produce visually realistic and temporally coherent sequences, which implicitly rely on learning the underlying dynamics of the environment. They can operate on diverse modalities, including language, visual data, and action inputs, allowing them to access environmental context and imagine future scenes. These characteristics position video generation models as a form of world modeling. Indeed, many recent world models adopt video generation as their core mechanism <sup>24</sup> <sup>25</sup> <sup>26</sup>, enabling the prediction of future states encompassing observations, actions, and environmental changes. However, most video generation models focus on observation-level prediction and may lack interpretable internal representations of the world.</p> <p></p>"},{"location":"02-preliminaries/#references","title":"References","text":"<ol> <li> <p>E. Xing, M. Deng, J. Hou, and Z. Hu, \"Critiques of world models,\" arXiv preprint arXiv:2507.05169, 2025.\u00a0\u21a9</p> </li> <li> <p>NVIDIA, \"World models.\" [Online]. Available: https://www.nvidia.com/en-au/glossary/world-models \u21a9</p> </li> <li> <p>S. Sudhakar, R. Liu, B. V. Hoorick, C. Vondrick, and R. Zemel, \"Controlling the world by sleight of hand,\" in European conference on computer vision, Springer, 2024, pp. 414--430.\u00a0\u21a9</p> </li> <li> <p>J. Cen et al., \"WorldVLA: Towards autoregressive action world model,\" arXiv preprint arXiv:2506.21539, 2025.\u00a0\u21a9</p> </li> <li> <p>P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, \"Daydreamer: World models for physical robot learning,\" in Conference on robot learning, PMLR, 2023, pp. 2226--2240.\u00a0\u21a9</p> </li> <li> <p>D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, \"Dream to control: Learning behaviors by latent imagination,\" in International conference on learning representations, 2019.\u00a0\u21a9</p> </li> <li> <p>D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, \"Mastering atari with discrete world models,\" in International conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse domains through world models,\" arXiv preprint arXiv:2301.04104, 2023.\u00a0\u21a9</p> </li> <li> <p>D. Chen et al., \"Planning with reasoning using vision language world model,\" arXiv preprint arXiv:2509.02722, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. LeCun, \"A path towards autonomous machine intelligence,\" Open Review, vol. 62, no. 1, pp. 1--62, 2022.\u00a0\u21a9</p> </li> <li> <p>M. Ahn et al., \"Do as i can, not as i say: Grounding language in robotic affordances,\" arXiv preprint arXiv:2204.01691, 2022.\u00a0\u21a9</p> </li> <li> <p>J. Xiang et al., \"Pandora: Towards general world model with natural language actions and video states,\" arXiv preprint arXiv:2406.09455, 2024.\u00a0\u21a9</p> </li> <li> <p>D. Driess et al., \"PaLM-e: An embodied multimodal language model,\" in Proceedings of the 40th international conference on machine learning, 2023, pp. 8469--8488.\u00a0\u21a9</p> </li> <li> <p>W. Zhang et al., \"DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge,\" arXiv preprint arXiv:2507.04447, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Hu, F. Lin, T. Zhang, L. Yi, and Y. Gao, \"Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning,\" arXiv preprint arXiv:2311.17842, 2023.\u00a0\u21a9</p> </li> <li> <p>W. Zhao, J. Chen, Z. Meng, D. Mao, R. Song, and W. Zhang, \"Vlmpc: Vision-language model predictive control for robotic manipulation,\" arXiv preprint arXiv:2407.09829, 2024.\u00a0\u21a9</p> </li> <li> <p>B. Zitkovich et al., \"Rt-2: Vision-language-action models transfer web knowledge to robotic control,\" in Conference on robot learning, PMLR, 2023, pp. 2165--2183.\u00a0\u21a9</p> </li> <li> <p>Y. Hong et al., \"3d-llm: Injecting the 3d world into large language models,\" Advances in Neural Information Processing Systems, vol. 36, pp. 20482--20494, 2023.\u00a0\u21a9</p> </li> <li> <p>J. Bjorck et al., \"Gr00t n1: An open foundation model for generalist humanoid robots,\" arXiv preprint arXiv:2503.14734, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Zhang, Y. Guo, Y. Hu, X. Chen, X. Zhu, and J. Chen, \"UP-VLA: A unified understanding and prediction model for embodied agent,\" ICML, 2025.\u00a0\u21a9</p> </li> <li> <p>Q. Zhao et al., \"Cot-vla: Visual chain-of-thought reasoning for vision-language-action models,\" in Proceedings of the computer vision and pattern recognition conference, 2025, pp. 1702--1713.\u00a0\u21a9</p> </li> <li> <p>L. Chen et al., \"EgoAgent: A joint predictive agent model in egocentric worlds,\" arXiv preprint arXiv:2502.05857, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Wang, R. Yu, S. Wan, L. Gan, and D.-C. Zhan, \"Founder: Grounding foundation models in world models for open-ended embodied decision making,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Du et al., \"Video language planning,\" arXiv preprint arXiv:2310.10625, 2023.\u00a0\u21a9</p> </li> <li> <p>J. Wu et al., \"Ivideogpt: Interactive videogpts are scalable world models,\" Advances in Neural Information Processing Systems, vol. 37, pp. 68082--68119, 2024.\u00a0\u21a9</p> </li> <li> <p>H. Zhang et al., \"COMBO: Compositional world models for embodied multi-agent cooperation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> </ol>"},{"location":"03-overview/","title":"III Overview of the World Model","text":""},{"location":"03-overview/#iii-overview-of-the-world-model","title":"III Overview of the World Model","text":"<p>\u201cWhat we observe is not nature itself, but nature exposed to our method of questioning.\u201d </p> <p>\u2014 Werner Heisenberg</p>"},{"location":"03-overview/#a-paradigms","title":"A. Paradigms","text":"<p>\u2003\u2003Building on the previous review of current models, contemporary architectures for capturing world dynamics can be broadly stratified along a methodological spectrum: implicit world modeling (e.g., LLMs, VLMs, and VLAs) <sup>1</sup> <sup>2</sup> <sup>3</sup> <sup>4</sup>, latent dynamics modeling <sup>5</sup> <sup>6</sup> <sup>7</sup> <sup>8</sup>, and video generation paradigms <sup>9</sup> <sup>10</sup> <sup>11</sup> <sup>12</sup>, each targeting distinct representational granularities and predictive mechanisms.</p> <p> </p> Fig. 2. A visualization of LLM-based world models <sup>13</sup>. <p> 1) Implicit World Modeling </p> <p>\u2003\u2003Representative models include LLMs, VLMs, and VLAs, which offer distinct advantages in semantic grounding, generalization, and interpretability <sup>14</sup> <sup>15</sup> <sup>13</sup> <sup>16</sup> <sup>17</sup>. An illustration of these models is shown in Figure\u00a01 At the same time, these models can be integrated into broader world-modeling architectures to capture temporal dependencies and enable long-horizon prediction <sup>18</sup> <sup>4</sup> <sup>19</sup>. Detailed discussions of these models are provided in Sections II-D1 and IV-A1.</p> <p> 2) Latent Dynamics Modeling </p> <p>\u2003\u2003Latent dynamics models typically encode high-dimensional observations into compact latent states through a variational autoencoder (VAE) or encoder network, and employ recurrent or transformer modules (e.g., RNNs or Transformers) to predict the temporal evolution of these latent representations <sup>6</sup> <sup>7</sup> <sup>5</sup> <sup>8</sup>. This architecture is characterized by latent-space imagination and task-oriented optimization over visual granularity, facilitating long-horizon learning by forecasting future states without the need for pixel-level reconstruction.</p> <p>\u2003\u2003Recurrent State-Space Model (RSSM) <sup>20</sup> resembles the structure of a partially observable Markov decision process.  Its learning framework consists of three main components: an encoder, a decoder, and a dynamics model. The encoder network fuses sensory inputs (observations) o together into the stochastic representations z. The dynamics model learns to predict the sequence of stochastic representations by using its recurrent state s. The decoder reconstructs sensory inputs to provide a rich signal for learning representations and enables human inspection of model predictions, but is not needed while learning behaviors from latent rollouts. Specifically, at time step t, let the image observation be o\u209c, the action vectors at and the reward r\u209c. RSSM can be formulated as the generative process of the images and rewards conditioned a hidden state sequence s\u209c:  </p> <p>\u2003\u2003Encoder/representation model:\u2003\u2003\u2003  \\(s_t \\sim p_\\theta\\left(s_t \\mid s_{t-1}, a_{t-1}, o_t\\right)\\)</p> <p>\u2003\u2003Decoder/observation model:\u2003\u2003\u2003\u2003 \\(o_t \\sim p_\\theta\\left(o_t \\mid s_t\\right)\\)</p> <p>\u2003\u2003Dynamics/Transition model:\u2003\u2003\u2003\u2003  \\(s_t \\sim p_\\theta\\left(s_t \\mid s_{t-1}, a_{t-1}\\right)\\)</p> <p>\u2003\u2003Reward model:\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003  \\(r_t \\sim p_\\theta\\left(r_t \\mid s_t\\right)\\)</p> <p>(1)</p> <p> </p> Fig. 3. A visualization of Dreamer architecture <sup>6</sup>, which encodes visual representations into latent states through recurrent estimation. <p>\u2003\u2003PlaNet <sup>20</sup> first demonstrates the effectiveness of learning dynamics in a latent space. The Dreamer family of models (a visualization is shown in Fig. 3) <sup>6</sup> <sup>7</sup> <sup>5</sup> <sup>8</sup> further verify this paradigm and establish a representative framework that reduces reliance on real-world data by performing imagination directly in latent space. Dreamer enables policy learning through imagined trajectories without continuous interaction with the environment, allowing agents to simulate multi-step consequences of actions and generalize to new states, objects, and environments.</p> <p> </p> Fig. 4. A visualization of Joint Embedding-Action-Prediction (JEPA) Archi- tecture <sup>21</sup>, where self-supervised learning is used to learn the future world state representations. <p>\u2003\u2003While sharing the objective of learning predictive worldstate representations, Joint-Embedding Predictive Architecture (JEPA) <sup>21</sup> <sup>1</sup> and RSSM diverge fundamentally in their learning mechanisms. RSSM relies on generative reconstruction of observations to model latent dynamics, whereas JEPA (a visualization is shown in Fig. 4) employs selfsupervised predictive coding in embedding spaces\u2014directly forecasting future state representations without decoding to raw sensory inputs. This paradigm eliminates the computational cost of pixel-level reconstruction but necessitates powerful hierarchical encoders to compress sufficient environmental information into abstract embeddings, creating an implicit information bottleneck that demands careful architectural balancing to preserve task-relevant features. Under the JEPA framework, Assran et al. <sup>22</sup> combine pre-trained video models with an action-conditioned predictor to autoregressively predict future states and actions. </p> <p> </p> Fig. 5. An illustration of video-geneation based world models <sup>23</sup>. World model serve as the core component, modelling the world dynamics and enabling action planning and generation. <p>\u2003\u2003The MuZero series <sup>24</sup> <sup>25</sup> <sup>26</sup> represent another form of latent-dynamics-based world modeling. Instead of modeling the complete environment dynamics, MuZero predicts only future quantities directly relevant to planning, such as rewards, values, and policies, given the complexity of real-world environments, and employs a tree-based search algorithm <sup>27</sup> to select optimal actions.</p> <p> 3) Video Generation.</p> <p>\u2003\u2003Video-based generative models are powerful tools for capturing environmental dynamics and predicting future scenes. These models operate directly on high-dimensional raw observations, such as RGB images, depth maps, or force fields <sup>9</sup> <sup>28</sup> <sup>29</sup> <sup>30</sup> <sup>31</sup> <sup>15</sup> <sup>32</sup> <sup>33</sup> <sup>34</sup>, treating the environment as a sequence of frames. By generating future scenes, they can support a wide range of applications, including visual planning, simulation, and action generation <sup>35</sup> <sup>36</sup> <sup>9</sup> <sup>11</sup> <sup>12</sup>. Moreover, they can leverage large-scale pre-training to enhance generalization and improve sample efficiency <sup>12</sup> <sup>28</sup> <sup>37</sup> <sup>38</sup>. Depending on the input modality, world models can be constructed using action-conditioned video prediction models <sup>9</sup>, text-to-video models <sup>10</sup> <sup>29</sup> <sup>38</sup> <sup>33</sup>, or trajectory-to-video models <sup>39</sup> <sup>40</sup>.</p> <p>\u2003\u2003There are several architectural families of video-based world models. Diffusion-based world models generate videos by progressively denoising random noise through multiple iterative steps. Representative examples include U-Net-based models <sup>41</sup> <sup>42</sup> and diffusion transformer (DiT)-based architectures <sup>43</sup> <sup>44</sup> <sup>39</sup> <sup>45</sup> <sup>46</sup>. Autoregressive world models, in contrast, predict the next token or frame conditioned on previously generated ones, effectively modeling temporal dependencies in the sequence <sup>23</sup> <sup>9</sup> <sup>15</sup> <sup>47</sup> <sup>48</sup> <sup>30</sup> <sup>40</sup>. Other architectures include variational autoencoder (VAE)-based models <sup>30</sup> and convolutional LSTMs <sup>35</sup> <sup>36</sup>. </p> <p>\u2003\u2003Autoregressive-based world models generate each step conditioned on previous outputs, allowing them to predict sequences of arbitrary length and making them well-suited for long-horizon predictions. However, they often suffer from error accumulation over extended sequences <sup>46</sup> and may struggle to represent highly multi-modal distributions. In contrast, diffusion-based models generate samples through an iterative denoising process, enabling them to model complex, multi-modal distributions and produce globally coherent sequences. This iterative refinement also makes diffusion models more robust to individual prediction errors, resulting in better performance on tasks requiring long-horizon consistency or high-quality generative outputs. On the downside, diffusion models are computationally intensive and slower during inference, and adapting them to sequential prediction requires careful conditioning. Overall, autoregressive world models tend to excel in scenarios demanding speed and accurate short-term predictions, whereas diffusion models are more suitable for tasks involving long-horizon, multi-modal, or high-dimensional outputs where maintaining global coherence is crucial. </p> <p>\u2003\u2003Compared with implicit world models and latent-space world models, video generation models provide more detailed visual predictions but at a higher computational cost, lower generation speed and sample efficiency. In addition, action predictions are only proved to be align with visual future generation <sup>49</sup>, as visual data contain relevant information to actions.</p>"},{"location":"03-overview/#b-architectural-design","title":"B. Architectural Design","text":"<p> 1) Flat architecture </p> <p>\u2003\u2003Most current methods adopt flat architectures <sup>50</sup> <sup>43</sup> <sup>48</sup> <sup>30</sup> <sup>31</sup> <sup>15</sup> <sup>32</sup> <sup>33</sup>, which face critical limitations. They lack structured representations of the environment, resulting in poor handling of multi-scale dynamics, limited longhorizon prediction, error accumulation, and reduced generalization. Specifically, in robotic manipulation, placing fragile objects requires the robot to react instantly to unexpected slips while simultaneously planning the sequence of pickand-place actions to achieve the overall goal. Many tasks further involve long-term objectives that must be completed through sequential subgoals and temporally extended actions. For example, assembling a piece of furniture requires picking up components, aligning and attaching them correctly, and tightening screws for each part. Moreover, operating at a single level of abstraction causes small prediction errors to compound over time, degrading performance in long-horizon tasks. Finally, flat architectures fail to extract high-level abstractions, limiting transferability across tasks and environments. </p> <p> 2) Hierarchical architecture </p> <p>\u2003\u2003Several studies have begun to explore and design hierarchical world models, in which lower-level modules handle intermediate reactions and short-term predictions, while higherlevel components are responsible for long-term planning and abstraction. Lecun et al. <sup>21</sup> hypothesize a hierarchical JEPA architecture, where low-level and high-level representations are learned for short- and long-term predictions, respectively.Gumbsch et al. <sup>51</sup> propose an RSSM-based hierarchical world model, where the low-level module captures immediate dynamics for reactive control, and the high-level module models abstract temporal patterns for strategic planning. Bjo \u0308rck et al. <sup>19</sup> introduce a dual-system architecture in which System 2 interprets the environment and task goals, while System 1 generates continuous motor commands in real time. Similarly, Wang et al. <sup>52</sup> design a dual-level world model consisting of an RSSM-based System 1 (RSSM-S1) and a logic-integrated neural network System 2 (LINN-S2). The inter-system feedback mechanism ensures that predicted sequences comply with domain-specific logical rules: LINN-S2 constrains RSSM-S1\u2019s predictions, while RSSM-S1 updates LINN-S2 based on new observations, enabling dynamic adaptation. Wang et al. <sup>53</sup> further employ System 2 for value-guided high-level planning by estimating state-action values and selecting optimal actions, while System 1 executes real-time motions via cascaded action denoising. </p> <p>\u2003\u2003Despite their advantages, hierarchical architectures introduce greater model complexity, higher computational cost, and increased training difficulty. Determining which goals or subtasks should be handled by high-level versus low-level modules remains challenging, as does designing appropriate architectures and preparing suitable training datasets. Moreover, maintaining effective information flow and coordination between layers is essential for stable and coherent performance. Consequently, developing hierarchical world models requires substantial effort in architecture design, goal decomposition, dataset construction, and inter-layer coordination.</p> <p></p>"},{"location":"03-overview/#c-world-observation-and-representation","title":"C. World Observation and Representation","text":"<p> 1) Dimensionality of the World </p> <p>\u2003\u2003In designing world models, the dimensionality of the environment plays a critical role, shaping how effectively a model captures spatial structures, temporal evolution, and causal dynamics.  </p> <p>\u2003\u2003Some works operate purely in 2D pixel space <sup>29</sup> <sup>30</sup> <sup>31</sup> <sup>15</sup> <sup>32</sup> <sup>33</sup>, capturing visual appearance and short-term dynamics but ignoring the real-world geometry. While 2D pixel-space models <sup>29</sup> can model appearance and short-term dynamics, they lack geometric awareness of real-world structure. This limitation motivates the development of 3D-aware architectures. To incorporate geometric understanding of the 3D world, Bu et al. <sup>54</sup> <sup>42</sup> <sup>11</sup> construct world models based on RGB-D data, while others extract richer 3D cues such as scene flow <sup>50</sup>, motion fields <sup>55</sup>, and 3D point clouds with associated language descriptions <sup>3</sup>, enabling more comprehensive modeling of 3D world dynamics. Additionally, Lu et al. <sup>56</sup> leverage 3D Gaussian Splatting, Diffusion Transformers, and a 3D Gaussian Variational Autoencoder to extract 3D representations from RGB observations. Zhang et al. <sup>16</sup> incorporate depth estimation to enhance understanding of 3D worlds.  </p> <p>\u2003\u2003In addition to geometric structure, temporal dynamics are incorporated to construct 4D world models that jointly capture spatial and temporal evolution. For example, Zhu et al. <sup>28</sup> synthesize 4D data from RGB-D videos by estimating depth and camera pose. Zhen et al. <sup>57</sup> leverage a pre-trained 3D VAE <sup>58</sup> to encode RGB, depth, and normal videos and combine them, while Huang et al. <sup>47</sup> employ 4D Gaussian splatting to model spatiotemporal dynamics in robotic environments.</p> <p> 2) Observation Viewpoint of the World </p> <p>\u2003\u2003Robots acquire skills by observing and imitating humans or other robots in their environment. Depending on the observation viewpoint, world models for robot learning can be categorized into third-person (exocentric) <sup>50</sup> <sup>43</sup> <sup>48</sup> and first-person (egocentric) <sup>1</sup> <sup>59</sup> perspectives. Many existing methods learn from exocentric perspectives, capturing skills from an external viewpoint <sup>50</sup> <sup>43</sup> <sup>48</sup>. However, exocentric observations do not fully align with how humans perceive the world, motivating the development of egocentric world models. For example, Chen et al. <sup>1</sup> observe a continuous loop of human interactions, in which humans perceive egocentric observations and take 3D actions repeatedly. They model these interactions as sequences of \u201cstate-action-state-action\u201d tokens processed using a causal attention mechanism. Zhang et al. <sup>11</sup> focus on multi-agent planning, inferring other agents\u2019 actions from world states estimated via partial egocentric observations.</p> <p>\u2003\u2003Grauman et al. <sup>59</sup> argue that egocentric and exocentric viewpoints are complementary: egocentric viewpoints provide fine-grained cues about hand-object interactions and the camera wearer\u2019s attention, while exocentric viewpoints supply broader context about the surrounding environment and whole-body poses.</p> <p> 3) Representation of the World</p> <p>\u2003\u2003A central aspect of world models lies in how the environment is represented, which directly influences their ability to reason about dynamics, predict future states, and generalize across tasks. World representations can be broadly categorized into scene-centric, object-centric, and flow-centric approaches. In scene-centric representations, the environment is encoded as a single holistic latent, typically learned directly from pixels or raw sensory inputs <sup>20</sup> <sup>6</sup> <sup>7</sup> <sup>8</sup> <sup>60</sup>. While video generation tasks aim to maximize the visual fidelity of predicted sequences, robotic manipulation often does not require the full visual detail. Irrelevant elements such as the background or parts of the robot arm can be ignored. This motivates the use of object-centric representations, which focus on task-relevant entities and their interactions <sup>43</sup> <sup>61</sup> <sup>48</sup> <sup>55</sup> <sup>62</sup>. Flow-centric representations, in contrast, are designed to capture the motion dynamics of the environment, emphasizing temporal change and spatial displacement <sup>63</sup>.</p> <p></p>"},{"location":"03-overview/#d-task-scope","title":"D. Task Scope","text":"<p>\u2003\u2003World models can also be categorized based on their task coverage. Some studies focus on single-task objectives, such as future-scene prediction <sup>64</sup> <sup>65</sup> <sup>61</sup> <sup>38</sup> <sup>66</sup>, or planning and action prediction <sup>67</sup>.</p> <p>\u2003\u2003In contrast, an increasing number of studies aim to support multiple tasks simultaneously, thereby enhancing the generality and applicability of world models.For instance, Cheang et al. <sup>40</sup> <sup>33</sup> <sup>10</sup> <sup>63</sup> generate videos for future-scene prediction and accordingly infer corresponding actions. Other works pursue simultaneous action prediction and world-scene forecasting <sup>68</sup> <sup>1</sup> <sup>3</sup> <sup>69</sup>. Beyond dual-task integration, several approaches extend world models to even broader capabilities. For instance, Bruce et al. <sup>30</sup> propose interactive video generation that supports environment prediction and imitation learning, and utilize a latent action model to infer policies from unseen, action-free videos. Liao et al. <sup>23</sup> introduce a unified framework for embodied video generation, policy learning, and simulation. Lu et al. <sup>56</sup> learn 3D world representations for future-state prediction, imitation learning, and simulator through video generation. Zhu et al. <sup>39</sup> develop an action-conditioned world model supporting trajectory-conditioned video generation, policy evaluation, and planning. Similarly, Huang et al. <sup>47</sup> achieve multi-view video generation, robotic action prediction, and a data flywheel mechanism for sim-to-real adaptation.</p> <p> Would foundation models. When discussing task scope, the notion of \u201cfoundation world models\u201d becomes essential. These approaches aim to generalize across diverse tasks through large-scale training, paving the way for world models that act as universal backbones for robotics. One line of research achieves this through large-scale pretraining followed by taskspecific fine-tuning <sup>56</sup> <sup>9</sup> <sup>44</sup> <sup>40</sup> <sup>70</sup>. In particular, Mazzaglia et al. <sup>70</sup> integrate a foundation VLM with a generative world model to enhance multimodal generalization. Other works directly pursue large-scale end-to-end training to build general-purpose world models <sup>30</sup> <sup>68</sup>.</p> <p></p>"},{"location":"03-overview/#references","title":"References","text":"<ol> <li> <p>L. Chen et al., \"EgoAgent: A joint predictive agent model in egocentric worlds,\" arXiv preprint arXiv:2502.05857, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>I. Team, \"Internlm: A multilingual language model with progressively enhanced capabilities.\" 2023.\u00a0\u21a9</p> </li> <li> <p>H. Zhen et al., \"3D-VLA: A 3D vision-language-action generative world model,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 61229--61245.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Y. Hong et al., \"3d-llm: Injecting the 3d world into large language models,\" Advances in Neural Information Processing Systems, vol. 36, pp. 20482--20494, 2023.\u00a0\u21a9\u21a9</p> </li> <li> <p>P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, \"Daydreamer: World models for physical robot learning,\" in Conference on robot learning, PMLR, 2023, pp. 2226--2240.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, \"Dream to control: Learning behaviors by latent imagination,\" in International conference on learning representations, 2019.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, \"Mastering atari with discrete world models,\" in International conference on learning representations, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse domains through world models,\" arXiv preprint arXiv:2301.04104, 2023.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Wu et al., \"Ivideogpt: Interactive videogpts are scalable world models,\" Advances in Neural Information Processing Systems, vol. 37, pp. 68082--68119, 2024.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Y. Du et al., \"Video language planning,\" arXiv preprint arXiv:2310.10625, 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>H. Zhang et al., \"COMBO: Compositional world models for embodied multi-agent cooperation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Rigter, T. Gupta, A. Hilmkil, and C. Ma, \"AVID: Adapting video diffusion models to world models,\" in Reinforcement learning conference, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Driess et al., \"PaLM-e: An embodied multimodal language model,\" in Proceedings of the 40th international conference on machine learning, 2023, pp. 8469--8488.\u00a0\u21a9\u21a9</p> </li> <li> <p>M. Ahn et al., \"Do as i can, not as i say: Grounding language in robotic affordances,\" arXiv preprint arXiv:2204.01691, 2022.\u00a0\u21a9</p> </li> <li> <p>J. Xiang et al., \"Pandora: Towards general world model with natural language actions and video states,\" arXiv preprint arXiv:2406.09455, 2024.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>W. Zhang et al., \"DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge,\" arXiv preprint arXiv:2507.04447, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>W. Zhao, J. Chen, Z. Meng, D. Mao, R. Song, and W. Zhang, \"Vlmpc: Vision-language model predictive control for robotic manipulation,\" arXiv preprint arXiv:2407.09829, 2024.\u00a0\u21a9</p> </li> <li> <p>B. Zitkovich et al., \"Rt-2: Vision-language-action models transfer web knowledge to robotic control,\" in Conference on robot learning, PMLR, 2023, pp. 2165--2183.\u00a0\u21a9</p> </li> <li> <p>J. Bjorck et al., \"Gr00t n1: An open foundation model for generalist humanoid robots,\" arXiv preprint arXiv:2503.14734, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner et al., \"Learning latent dynamics for planning from pixels,\" in International conference on machine learning, 2019, pp. 2555--2565.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Y. LeCun, \"A path towards autonomous machine intelligence,\" Open Review, vol. 62, no. 1, pp. 1--62, 2022.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Assran et al., \"V-jepa 2: Self-supervised video models enable understanding, prediction and planning,\" arXiv preprint arXiv:2506.09985, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Liao et al., \"Genie envisioner: A unified world foundation platform for robotic manipulation,\" arXiv preprint arXiv:2508.05635, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Schrittwieser et al., \"Mastering atari, go, chess and shogi by planning with a learned model,\" Nature, vol. 588, no. 7839, pp. 604--609, 2020.\u00a0\u21a9</p> </li> <li> <p>W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao, \"Mastering atari games with limited data,\" Advances in neural information processing systems, vol. 34, pp. 25476--25488, 2021.\u00a0\u21a9</p> </li> <li> <p>S. Wang, S. Liu, W. Ye, J. You, and Y. Gao, \"Efficientzero v2: Mastering discrete and continuous control with limited data,\" arXiv preprint arXiv:2403.00564, 2024.\u00a0\u21a9</p> </li> <li> <p>D. Silver et al., \"A general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\" Science, vol. 362, no. 6419, pp. 1140--1144, 2018.\u00a0\u21a9</p> </li> <li> <p>H. Zhu et al., \"Aether: Geometric-aware unified world modeling,\" in ICCV, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, \"Learning interactive real-world simulators,\" arXiv preprint arXiv:2310.06114, vol. 1, no. 2, p. 6, 2023.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Bruce et al., \"Genie: Generative interactive environments,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 4603--4623.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>T. Brooks et al., \"Video generation models as world simulators,\" OpenAI Blog, vol. 1, no. 8, p. 1, 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Z. Zheng et al., \"Open-sora: Democratizing efficient video production for all,\" arXiv preprint arXiv:2412.20404, 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, \"RoboDreamer: Learning compositional world models for robot imagination,\" in International conference on machine learning, PMLR, 2024, pp. 61885--61896.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Q. Ali, A. Sridhar, S. Matiana, A. Wong, and M. Al-Sharman, \"Humanoid world models: Open world foundation models for humanoid robotics,\" arXiv preprint arXiv:2506.01182, 2025.\u00a0\u21a9</p> </li> <li> <p>C. Finn and S. Levine, \"Deep visual foresight for planning robotic motion,\" in 2017 IEEE international conference on robotics and automation, IEEE, 2017, pp. 2786--2793.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine, \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control,\" arXiv preprint arXiv:1812.00568, 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>B. Wang et al., \"This\\ &amp;that: Language-gesture controlled video generation for robot planning,\" in 2025 IEEE international conference on robotics and automation (ICRA), IEEE, 2025, pp. 12842--12849.\u00a0\u21a9</p> </li> <li> <p>J. Jang et al., \"DreamGen: Unlocking generalization in robot learning through video world models,\" arXiv preprint arXiv:2505.12705, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>F. Zhu, H. Wu, S. Guo, Y. Liu, C. Cheang, and T. Kong, \"Irasim: Learning interactive real-robot action simulators,\" in ICCV, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>C.-L. Cheang et al., \"Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation,\" arXiv preprint arXiv:2410.06158, 2024.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, \"Video diffusion models,\" Advances in neural information processing systems, vol. 35, pp. 8633--8646, 2022.\u00a0\u21a9</p> </li> <li> <p>P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum, \"Learning to act from actionless videos through dense correspondences,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Ferraro, P. Mazzaglia, T. Verbelen, and B. Dhoedt, \"FOCUS: Object-centric world models for robotic manipulation,\" Frontiers in Neurorobotics, vol. 19, p. 1585386, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>N. Agarwal et al., \"Cosmos world foundation model platform for physical ai,\" arXiv preprint arXiv:2501.03575, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>T. Wan et al., \"Wan: Open and advanced large-scale video generative models,\" arXiv preprint arXiv:2503.20314, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Yang et al., \"RoboEnvision: A long-horizon video generation model for multi-task robot manipulation,\" arXiv preprint arXiv:2506.22007, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Huang et al., \"Enerverse: Envisioning embodied future space for robotics manipulation,\" arXiv preprint arXiv:2501.01895, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>A. Villar-Corrales and S. Behnke, \"PlaySlot: Learning inverse latent dynamics for controllable object-centric video prediction and planning,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>L. Wang, K. Zhao, C. Liu, and X. Chen, \"Learning real-world action-video dynamics with heterogeneous masked autoregression,\" arXiv preprint arXiv:2502.04296, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Guo, X. Ma, Y. Wang, M. Yang, H. Liu, and Q. Li, \"FlowDreamer: A RGB-d world model with flow-based motion representations for robot manipulation,\" arXiv preprint arXiv:2505.10075, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>C. Gumbsch, N. Sajid, G. Martius, and M. V. Butz, \"Learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>L. Wang, R. Shelim, W. Saad, and N. Ramakrishnan, \"DMWM: Dual-mind world model with long-term imagination,\" arXiv preprint arXiv:2502.07591, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Song et al., \"Hume: Introducing system-2 thinking in visual-language-action model,\" arXiv preprint arXiv:2505.21432, 2025.\u00a0\u21a9</p> </li> <li> <p>Q. Bu et al., \"Closed-loop visuomotor control with generative expectation for robotic manipulation,\" Advances in Neural Information Processing Systems, vol. 37, pp. 139002--139029, 2024.\u00a0\u21a9</p> </li> <li> <p>H. Zhi et al., \"3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model,\" arXiv preprint arXiv:2506.06199, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>G. Lu et al., \"GWM: Towards scalable gaussian world models for robotic manipulation,\" arXiv preprint arXiv:2508.17600, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>H. Zhen et al., \"TesserAct: Learning 4D embodied world models,\" arXiv preprint arXiv:2504.20995, 2025.\u00a0\u21a9</p> </li> <li> <p>D. P. Kingma and M. Welling, \"Auto-encoding variational bayes,\" arXiv preprint arXiv:1312.6114, 2013.\u00a0\u21a9</p> </li> <li> <p>K. Grauman et al., \"Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 19383--19400.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse control tasks through world models,\" Nature, pp. 1--7, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, \"Dream to manipulate: Compositional world models empowering robot imitation learning with imagination,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, \"Multiply: A multisensory object-centric embodied large language model in 3d world,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 26406--26416.\u00a0\u21a9</p> </li> <li> <p>C. Gao, H. Zhang, Z. Xu, C. Zhehao, and L. Shao, \"FLIP: Flow-centric generative planning as general-purpose manipulation world model,\" in The thirteenth international conference on learning representations, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Sudhakar, R. Liu, B. V. Hoorick, C. Vondrick, and R. Zemel, \"Controlling the world by sleight of hand,\" in European conference on computer vision, Springer, 2024, pp. 414--430.\u00a0\u21a9</p> </li> <li> <p>C. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" in Proceedings of the 30th international conference on neural information processing systems, 2016, pp. 64--72.\u00a0\u21a9</p> </li> <li> <p>F. Ebert, S. Dasari, A. X. Lee, S. Levine, and C. Finn, \"Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning,\" in Conference on robot learning, PMLR, 2018, pp. 983--993.\u00a0\u21a9</p> </li> <li> <p>R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, \"Planning to explore via self-supervised world models,\" in International conference on machine learning, 2020, pp. 8583--8592.\u00a0\u21a9</p> </li> <li> <p>J. Cen et al., \"WorldVLA: Towards autoregressive action world model,\" arXiv preprint arXiv:2506.21539, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Z. Song, S. Qin, T. Chen, L. Lin, and G. Wang, \"Physical autoregressive model for robotic manipulation without action pretraining,\" arXiv preprint arXiv:2508.09822, 2025.\u00a0\u21a9</p> </li> <li> <p>P. Mazzaglia, T. Verbelen, B. Dhoedt, A. Courville, and S. Rajeswar, \"GenRL: Multimodal-foundation world models for generalization in embodied agents,\" Advances in neural information processing systems, vol. 37, pp. 27529--27555, 2024.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"04-functions/","title":"IV Functions of the World Model","text":""},{"location":"04-functions/#iv-functions-of-the-world-model","title":"IV Functions of the World Model","text":"<p>\u2003\u2003World models play a central role in modern robotics by providing an internal predictive understanding of the environment. They enable robots to reason about future states, anticipate the consequences of actions, and perform counterfactual evaluations, which are particularly valuable in real-world settings where interactions are costly, risky, or time-consuming. By modeling environmental dynamics, world models form the foundation for autonomous, adaptable, and efficient robotic systems. In robotics, world models serve two complementary functions: decision support, by predicting future scenes, actions and planning, and training facilitation, by generating data or acting as learned simulators. These roles are often closely related. For example, a world model used as a simulator can simultaneously generate training data and assist decision making <sup>1</sup> <sup>2</sup>. By combining these functionalities, world models provide a comprehensive framework that enables robots to act intelligently, learn efficiently, and adapt to complex and dynamic environments. Additional details of the world models are provided in Table I, which complements the following discussion.</p>"},{"location":"04-functions/#a-decision-support","title":"A. Decision Support","text":"<p> 1) Implicit World Models for Action Prediction and Planning</p> <p>\u2003\u2003This line of work explores world models that enable action prediction and planning without explicitly modeling state transitions or world dynamics. These approaches typically leverage the strong reasoning and next-token prediction capabilities of Large Language Models (LLMs), Vision-Language Models (VLMs), and Vision-Language-Action (VLA) models. Since LLMs lack direct access to environmental or robotic states, auxiliary components are often incorporated to provide grounding. For example, Ahn et al. <sup>3</sup> introduce affordance functions to evaluate the feasibility of skills for completing a target task. Xiang et al. <sup>4</sup> <sup>5</sup> employ encoders to process environmental information, while Zhang et al. <sup>6</sup> integrate multimodal tokens including states, images, and text to enhance reasoning and generalization. Zhang et al. <sup>7</sup> further combine 2D and 3D encoders to process RGB images and 3D point clouds, capturing complementary spatial cues for richer world understanding. Hong et al. <sup>8</sup> extend this paradigm by incorporating additional sensory modalities such as vision, audio, tactile, and thermal inputs to achieve a more comprehensive understanding of the environment.</p> <p>\u2003\u2003Conventional LLMs are language-centric and typically treat visual and other sensory information as auxiliary inputs. VLMs extend this paradigm by jointly learning aligned visual and linguistic representations, enabling grounded perceptual understanding of the world <sup>9</sup>. Zhang et al. <sup>10</sup> further leverage VLMs to generate candidate action sequences, which are evaluated using a lightweight action-conditioned video prediction model to forecast future scenes. The predicted outcomes are then assessed by the VLM to select the final action. An increasing number of studies extend VLMs to VLA by equipping them with low-level action generation capabilities. For instance, Zitkovich et al. <sup>11</sup> represent robot actions as a form of language, effectively bridging perception and control through textual grounding. Zhen et al. <sup>12</sup> employ a 3D-based LLM <sup>13</sup> to represent and predict 3D world states and generate actions, incorporating a diffusion model to synthesize future scenes. Inspired by the dual-process theory of human cognition <sup>14</sup>, Bj\u00f6rck et al. <sup>15</sup> design a dual-system architecture in which a VLM serves as the reasoning module (System 2) and a Diffusion Transformer functions as the action module (System 1), with both components jointly optimized for coordinated reasoning and actuation. Zhou et al. <sup>16</sup> preserve the reasoning capability of VLMs while introducing a Mixture-of-Experts (MoE) to alleviate conflicts between multimodal understanding and robotic manipulation in the parameter space. Kim et al. <sup>17</sup> train their model on a large corpus of real-world robot demonstrations, enabling efficient adaptation to new robotic platforms through parameter-efficient fine-tuning.</p> <p>\u2003\u2003To further enhance long-horizon prediction, reasoning, and imagination, several methods integrate large language or multimodal models into other world model architectures, where they serve as core components. For instance, Chen et al. <sup>18</sup> employ the open-source LLM, i.e., InternLM <sup>19</sup>, to predict future states from egocentric observations as a fundamental element of the JEPA framework. Similarly, Vision-Language Models <sup>20</sup> and Video-Language Models <sup>21</sup> have been incorporated into Dreamer-style architectures for low-level dynamics modeling, where they extract high-level semantic knowledge of the world to guide prediction.</p> <p>\u2003\u2003Notably, LLMs, VLMs, and VLAs can also act as explicit world models that predict future scenes <sup>22</sup> <sup>6</sup> <sup>23</sup> or world knowledge <sup>6</sup>. We will elaborate them in Section IV-A3.</p> <p> 2) Latent Dynamics Modeling for Action Prediction and Planning</p> <p>\u2003\u2003This line of research focuses on modeling the temporal evolution of environment dynamics within a latent space, facilitating efficient action prediction, planning and future imagination. Operating in a compact latent space requires fewer environment interactions and reduces computational cost compared to pixel-based modeling. Hafner et al. <sup>24</sup> <sup>25</sup> <sup>26</sup> <sup>27</sup> <sup>28</sup> introduce online planning in latent space through the Recurrent State-Space Model (RSSM), which learns to reconstruct input observations <sup>24</sup>. The Dreamer series <sup>25</sup> <sup>26</sup> <sup>27</sup> <sup>28</sup> introduces latent imagination, allowing agents to predict and plan over latent trajectories instead of pixels for more efficient and stable policy learning. Specifically, DreamerV1 <sup>25</sup> learns long-horizon behaviors from images by jointly predicting actions and state values, greatly improving sample efficiency. DreamerV2 <sup>26</sup> extends this framework to discrete environments by introducing binary latent variables, achieving human-level performance on the Atari benchmark. DreamerV3 <sup>28</sup> <sup>29</sup> further improves scalability and generative capacity through techniques such as symlog normalization for reward stabilization, refined KL balancing, and enhanced replay buffers. Sekar et al. <sup>30</sup> enhance generalization to downstream tasks through self-supervised learning without task-specific rewards, while Wu et al. <sup>27</sup> deploy Dreamer in the real world without simulators. Gumbsch et al. <sup>31</sup> introduce context-sensitive dynamics via a context-specific RSSM and hierarchical architecture to improve scalability and long-horizon prediction. Ferraro et al. <sup>32</sup> develop object-centric world models for improved interaction reasoning.</p> <p>\u2003\u2003Under the JEPA framework, Chen et al. <sup>18</sup> capture causal and temporal dependencies by organizing states and actions into an interleaved sequence, integrating future state prediction and action generation within a unified transformer architecture. Building on this, Assran et al. <sup>33</sup> leverage pre-trained video encoders optimized with a masked denoising objective as the core of JEPA, enabling self-supervised learning through an action-conditioned predictor that autoregressively forecasts future states and actions. Incorporating other potential world models, such as LLMs and VLMs, have been introduced in Section IV-A1. </p> <p>\u2003\u2003There are also approaches that couple Model Predictive Control (MPC) with learned world models, where the predictive model is used to simulate future trajectories and select optimal actions in a receding-horizon manner. For example, Hansen et al. <sup>34</sup> learn task-specific latent dynamics models using temporal-difference objectives and apply them for efficient online Model Predictive Control. Hansen et al. <sup>35</sup> further improve generalization across diverse embodiments and action spaces by learning an implicit, control-centric dynamics model.</p> <p> 3) Vision-based Action Prediction and Planning</p> <p>\u2003\u2003Vision-based methods enable robots to predict future observations from sensory inputs, allowing them to plan actions in complex and unstructured environments. By simulating sequences of visual outcomes, robots can evaluate longhorizon behaviors, integrate multiple modalities (e.g., vision, language, and control), and generalize to novel tasks without task-specific retraining. This predictive capability makes visual imagination a key component of goal-directed and adaptive robotic decision-making. In particular, action-conditioned multi-frame prediction serves as a crucial element of prediction and planning, allowing robots to mentally simulate the outcomes of different actions before selecting the optimal one for a given task. According to the task formulation, existing approaches can be broadly classified into vision-conditioned and language-conditioned goal representations.  </p> <p> Vision-Conditioned Goals. Finn et al. <sup>36</sup> learn to predict motion dynamics that remain consistent across visual appearances, aiming to enable long-range, action-conditioned video prediction and generalization to unseen objects. Ebert et al. <sup>37</sup> <sup>38</sup> improve long-horizon prediction by using an image registration\u2013based cost function that continuously corrects errors during execution, achieving closed-loop visual planning. Bu et al. <sup>39</sup> further extend this idea with text-conditioned video generation to synthesize depth- and flow-consistent sub-goal images. A feedback mechanism then selects sub-goals and generates corresponding actions based on visual error evaluation, bridging visual planning and policy learning.</p> <p>\u2003\u2003Imagining the future does not inherently produce actions. To enable action predictions, Finn et al. <sup>40</sup> <sup>37</sup> <sup>38</sup> incorporate visual prediction models with model-predictive control (MPC) to select the best action (sequence). Bu et al. <sup>39</sup> use an error-measurement strategy to select the best sub-goal images and an MLP trained with an inverse-dynamics objective to decode the corresponding actions.</p> <p> Language-Conditioned Goals. In <sup>36</sup> <sup>40</sup> <sup>37</sup> <sup>38</sup>, task specifications are provided as goal images, which are often difficult to obtain and prone to over- or under-specification. To address this limitation, a growing line of research leverages language as a more flexible, compact, and general medium for specifying tasks. However, translating language instructions into precise, actionable representations grounded in the robot\u2019s observations remains challenging due to the misalignment between linguistic descriptions and visual perception. To bridge this gap, Nair et al. <sup>41</sup> use action-conditioned video prediction to simulate future scenes under different action sequences and learn a language-conditioned reward function from crowd-sourced descriptions to measure task completion\u2014the best sequence is selected to maximize the reward. Zhang et al. <sup>22</sup> take advantage of the semantic knowledge and reasoning abilities of VLA and incorporate a decoder into VLA to enable future scene predictions and action generation. Zhou et al. <sup>42</sup> parse language instructions into compositional primitives to capture spatial object relations and generalize to novel commands, while also supporting multimodal task inputs such as goal images and sketches. Zhang et al. <sup>6</sup> enhance reasoning and generalization by introducing additional dream queries that capture historical information and predict dynamic regions, depth, and semantic maps using foundation models such as DINOv2 <sup>43</sup> and SAM <sup>44</sup>.</p> <p> Diverse Goals. Some works leverage diverse goal conditions to improve task understanding and completion. For instance, Wang et al. <sup>45</sup> develop a language\u2013gesture-conditioned video generation model to disambiguate task specifications and integrate a behavior-cloning policy that unifies visual plan generation and manipulation. Du et al. <sup>46</sup> incorporate observed images as additional context in each frame-denoising step to synthesize video plans and employ an inverse-dynamics model (IDM) to infer the corresponding action sequences. Zhao et al. <sup>23</sup> introduce visual chain-of-thought (CoT) reasoning into VLA models by autoregressively generating sub-goal images alongside language instructions, enabling temporal planning and improving reasoning capability.</p> <p> Action inference. A key advantage of vision\u2013based action prediction is that it does not rely on large-scale action-labeled data. They can be pre-trained on large-scale video data and infer actions by training a simple action extractor using small amounts of action data. Techniques for action extraction include the inverse dynamics model <sup>47</sup> <sup>46</sup> <sup>42</sup>, a transformer encoder\u2013decoder architecture <sup>45</sup>. Zhang et al. <sup>48</sup> use vision\u2013language models to propose actions, and a tree search to find the best plan. However, video predictions can contain information irrelevant to the target tasks or actions to execute, such as background and robot arm. To handle this, Zhi et al. <sup>49</sup> extract 3D flow from video data and learn 3D optical flow as a representation of object motions to guide action planning. Zhang et al. <sup>6</sup> propose dynamic region\u2013based forecasting, which leverages an optical flow prediction model <sup>50</sup> <sup>51</sup> to identify dynamic regions within the scene, enabling the model to concentrate on areas of motion that are critical for task execution instead of redundant frame reconstruction. Agarwal et al. <sup>52</sup> leverage large-scale pre-training on images and post-training for robotic manipulation, including instruction-based video prediction and action-based next-frame prediction. 3D positional embeddings, including 3D factorized Rotary Position Embedding (RoPE) and absolute positional embedding (APE) for relative positions and absolute coordinates respectively, are adopted to capture spatial and temporal information. Actions are predicted through an action-embedder MLP. Tian et al. <sup>53</sup> propose an end-to-end Predictive Inverse Dynamics Model (PIDM), which learns actions and visual futures synergistically to enhance simulation and action-prediction ability. <sup>54</sup> predict both future frames and robot actions within a joint latent denoising process, which supports planning and acting in a closed-loop manner.</p> <p> Visual Fidelity vs. Action Prediction. Guo et al. <sup>55</sup> hypothesize that models trained solely with frame-prediction losses tend to emphasize visual appearance fidelity while underestimating accurate dynamics modeling. This highlights the need for approaches that explicitly separate dynamics learning from visual rendering. To address this, FlowDreamer adopts a two-stage framework that first predicts environment dynamics and then renders corresponding visual observations.</p>"},{"location":"04-functions/#b-training-facilitation","title":"B. Training Facilitation","text":"<p>\u2003\u2003World models can act both as data engines, generating synthetic trajectories that support imitation learning and reinforcement learning, and as evaluation modules that provide internal reward estimation or predictive feedback. Because many models combine these roles, it is difficult to assign them to a single category. Accordingly, when discussing each role, we introduce their complementary functions in parallel to highlight this overlap.</p> <p> 1) Data Engine</p> <p>\u2003\u2003Large-scale human teleoperation datasets have greatly advanced robot learning<sup>11</sup> <sup>56</sup> <sup>57</sup> <sup>58</sup> <sup>15</sup> <sup>59</sup>. However, collecting such data is labor-intensive and limits coverage across diverse environments and tasks. Vision-based world models, particularly video world models, offer an alternative by learning environment dynamics and generating synthetic data. These models can be broadly divided, according to whether they are conditioned on actions, into static video generation models <sup>60</sup>, which predict general future scenes, and action-conditioned video generation models, which simulate how actions change the environment. Beyond data generation, video-based world models increasingly support diverse tasks such as planning, policy learning, and action prediction, which will be reflected in the following content. </p> <p>\u2003\u2003Specifically, Du et al. <sup>46</sup> target to enable visual world imagination, action planning and generating video demonstration for training by learning a text-conditioned video generation model. Wu et al. <sup>61</sup> train a large-scale video world model to generate accurate and realistic simulated experiences, enabling video prediction, visual planning, and policy training. Jang et al. <sup>60</sup> propose to leverage video world models <sup>62</sup> to generate robot video data. They first fine-tune video world models on a target robot to capture the embodiment-specific dynamics and kinematics and prompt the model with initial frames and language instruction to generate corresponding data. Pseudo-action labels are generated by means of either a latent action model <sup>63</sup> or an inverse dynamics model (IDM) <sup>64</sup>. Lu et al. <sup>1</sup> leverage 3D-GS reconstruction with Diffusion Transformers (DiTs) to effectively model 3D dynamics, which can promote future scenes generation to support imitation learning and reinforcement learning. Ye et al. <sup>65</sup> synthesize data from diverse perspectives to introduce variations in texture, illumination, viewpoints, physical properties, task diversity, and interaction patterns. Their approach includes: (i) re-rendering real trajectories with diverse visual content, (ii) generating viewpoint-consistent multi-camera scenes with pose adjustments, and (iii) synthesizing embodied interaction sequences, such as converting first-person human videos into robot-centric demonstrations. To ensure realism and avoid hallucination artifacts, the authors further leverage a set of quality assessment metrics that evaluate geometric consistency <sup>66</sup>, multiview consistency <sup>66</sup>, text\u2013scene alignment <sup>67</sup>, and physical plausibility <sup>67</sup>. When constructing world models for training data generation, it is unrealistic to expect any training distribution to encompass all possible configurations of the world. To handle this, Barcellona et al. <sup>68</sup> construct a compositional world model to generate novel demonstration data for training by combining Gaussian Splatting <sup>69</sup> and physics simulators. Equivariant transformation is leveraged to augment data, which modify both observations and the corresponding action sequences to ensure semantical consistency.</p> <p> Support reinforcement learning (RL) based Robotics. Wang et al. <sup>70</sup> present a video-based world model capable of predicting future visual observations conditioned on VLA-generated actions. A VLM-guided instant reflector serves as a reward function that quantifies task completion through the semantic alignment between the predicted trajectory and the textual instruction. Despite recent progress, existing methods continue to face challenges in generating diverse and counterfactual data that remain physically plausible, thereby limiting the quality and diversity of synthetic datasets <sup>15</sup>.</p> <p> 2) Evaluation</p> <p>\u2003\u2003Traditionally, robot control policies have been developed and evaluated using handcrafted physics simulators <sup>71</sup> <sup>72</sup> <sup>73</sup>. However, such simulators rely on simplified or manually engineered dynamics, which struggle to capture complex real-world phenomena, particularly high-DoF interactions, deformable objects, and other non-rigid or contact-rich scenarios <sup>74</sup> <sup>75</sup> <sup>76</sup>. Consequently, the resulting discrepancies between simulated and real environments, commonly referred to as the sim-to-real gap, have significantly hindered the deployment and generalization of robotic policies in practice <sup>77</sup> <sup>78</sup>. To handle this, world models potentially emerge as a scalable, reproducible, and informative tool, which reduce reliance on trial-and-error in the real world. Compared to other filed such as autonomous driving <sup>79</sup> and navigation <sup>80</sup>, simulated evaluation of robotic manipulation remains difficult because of the highly varied and dynamic interactions that arise between the agent and its environment. Li et al. <sup>81</sup> leverage a video generative world model <sup>62</sup> to produce videos based on action representations from a policy network. A success detector <sup>82</sup> is then used to evaluate task completion from the generated videos and corresponding text prompts. Quevedo et al. <sup>83</sup> evaluate robot polices by means of Monte Carlo rollouts in the world model and take a vision-language model, i.e., GPT-4o <sup>84</sup>, as the reward model. He et al. <sup>85</sup> introduce a frame-level control and a motion-reinforced training to improve action-following ability and temporal, dynamic consistency, enhancing the dynamic prediction and action responsiveness of world simulator. More valuable transitions are discovered for policy learning. Zhu et al. <sup>86</sup> construct a frame-level, action-conditioned video world model based on a Diffusion Transformer, enabling scalable policy evaluation, planning, and future-scene generation. Liao et al. <sup>2</sup> take an action-conditioned video generator as the core to model the spatial, temporal, and semantic regularities of real-world interactions that are fundamental to robotic manipulation. The base world model can support future scene generation, action predictions, data engine and closed-loop policy evaluation. Wang et al. <sup>87</sup> promote the versatility of video world models for policy evaluation, visual simulation, synthetic data generation by perform training on heterogeneous actions data with a shared spatial-temporal transformer.</p> <p>\u2003\u2003Escontrela et al. <sup>88</sup> train an autoregressive transformerbased video prediction model and use the next-token likelihoods of the frozen model as a general reward function across diverse tasks.</p> <p></p>"},{"location":"04-functions/#references","title":"References","text":"<ol> <li> <p>G. Lu et al., \"GWM: Towards scalable gaussian world models for robotic manipulation,\" arXiv preprint arXiv:2508.17600, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. Liao et al., \"Genie envisioner: A unified world foundation platform for robotic manipulation,\" arXiv preprint arXiv:2508.05635, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>M. Ahn et al., \"Do as i can, not as i say: Grounding language in robotic affordances,\" arXiv preprint arXiv:2204.01691, 2022.\u00a0\u21a9</p> </li> <li> <p>J. Xiang et al., \"Pandora: Towards general world model with natural language actions and video states,\" arXiv preprint arXiv:2406.09455, 2024.\u00a0\u21a9</p> </li> <li> <p>D. Driess et al., \"PaLM-e: An embodied multimodal language model,\" in Proceedings of the 40th international conference on machine learning, 2023, pp. 8469--8488.\u00a0\u21a9</p> </li> <li> <p>W. Zhang et al., \"DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge,\" arXiv preprint arXiv:2507.04447, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Huang et al., \"An embodied generalist agent in 3D world,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 20413--20451.\u00a0\u21a9</p> </li> <li> <p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, \"Multiply: A multisensory object-centric embodied large language model in 3d world,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 26406--26416.\u00a0\u21a9</p> </li> <li> <p>Y. Hu, F. Lin, T. Zhang, L. Yi, and Y. Gao, \"Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning,\" arXiv preprint arXiv:2311.17842, 2023.\u00a0\u21a9</p> </li> <li> <p>W. Zhao, J. Chen, Z. Meng, D. Mao, R. Song, and W. Zhang, \"Vlmpc: Vision-language model predictive control for robotic manipulation,\" arXiv preprint arXiv:2407.09829, 2024.\u00a0\u21a9</p> </li> <li> <p>B. Zitkovich et al., \"Rt-2: Vision-language-action models transfer web knowledge to robotic control,\" in Conference on robot learning, PMLR, 2023, pp. 2165--2183.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Zhen et al., \"3D-VLA: A 3D vision-language-action generative world model,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 61229--61245.\u00a0\u21a9</p> </li> <li> <p>Y. Hong et al., \"3d-llm: Injecting the 3d world into large language models,\" Advances in Neural Information Processing Systems, vol. 36, pp. 20482--20494, 2023.\u00a0\u21a9</p> </li> <li> <p>D. Kahneman, Thinking, fast and slow. macmillan, 2011.\u00a0\u21a9</p> </li> <li> <p>J. Bjorck et al., \"Gr00t n1: An open foundation model for generalist humanoid robots,\" arXiv preprint arXiv:2503.14734, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Z. Zhou, Y. Zhu, J. Wen, C. Shen, and Y. Xu, \"Vision-language-action model with open-world embodied reasoning from pretrained knowledge,\" arXiv preprint arXiv:2505.21906, 2025.\u00a0\u21a9</p> </li> <li> <p>M. J. Kim et al., \"OpenVLA: An open-source vision-language-action model,\" in Conference on robot learning, PMLR, 2025, pp. 2679--2713.\u00a0\u21a9</p> </li> <li> <p>L. Chen et al., \"EgoAgent: A joint predictive agent model in egocentric worlds,\" arXiv preprint arXiv:2502.05857, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>I. Team, \"Internlm: A multilingual language model with progressively enhanced capabilities.\" 2023.\u00a0\u21a9</p> </li> <li> <p>P. Mazzaglia, T. Verbelen, B. Dhoedt, A. Courville, and S. Rajeswar, \"GenRL: Multimodal-foundation world models for generalization in embodied agents,\" Advances in neural information processing systems, vol. 37, pp. 27529--27555, 2024.\u00a0\u21a9</p> </li> <li> <p>Y. Wang, R. Yu, S. Wan, L. Gan, and D.-C. Zhan, \"Founder: Grounding foundation models in world models for open-ended embodied decision making,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Zhang, Y. Guo, Y. Hu, X. Chen, X. Zhu, and J. Chen, \"UP-VLA: A unified understanding and prediction model for embodied agent,\" ICML, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Q. Zhao et al., \"Cot-vla: Visual chain-of-thought reasoning for vision-language-action models,\" in Proceedings of the computer vision and pattern recognition conference, 2025, pp. 1702--1713.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner et al., \"Learning latent dynamics for planning from pixels,\" in International conference on machine learning, 2019, pp. 2555--2565.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, \"Dream to control: Learning behaviors by latent imagination,\" in International conference on learning representations, 2019.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, \"Mastering atari with discrete world models,\" in International conference on learning representations, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, \"Daydreamer: World models for physical robot learning,\" in Conference on robot learning, PMLR, 2023, pp. 2226--2240.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse domains through world models,\" arXiv preprint arXiv:2301.04104, 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse control tasks through world models,\" Nature, pp. 1--7, 2025.\u00a0\u21a9</p> </li> <li> <p>R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, \"Planning to explore via self-supervised world models,\" in International conference on machine learning, 2020, pp. 8583--8592.\u00a0\u21a9</p> </li> <li> <p>C. Gumbsch, N. Sajid, G. Martius, and M. V. Butz, \"Learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>S. Ferraro, P. Mazzaglia, T. Verbelen, and B. Dhoedt, \"FOCUS: Object-centric world models for robotic manipulation,\" Frontiers in Neurorobotics, vol. 19, p. 1585386, 2025.\u00a0\u21a9</p> </li> <li> <p>M. Assran et al., \"V-jepa 2: Self-supervised video models enable understanding, prediction and planning,\" arXiv preprint arXiv:2506.09985, 2025.\u00a0\u21a9</p> </li> <li> <p>N. A. Hansen, H. Su, and X. Wang, \"Temporal difference learning for model predictive control,\" in International conference on machine learning, 2022, pp. 8387--8406.\u00a0\u21a9</p> </li> <li> <p>N. Hansen, H. Su, and X. Wang, \"TD-MPC2: Scalable, robust world models for continuous control,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>C. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" in Proceedings of the 30th international conference on neural information processing systems, 2016, pp. 64--72.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Ebert, S. Dasari, A. X. Lee, S. Levine, and C. Finn, \"Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning,\" in Conference on robot learning, PMLR, 2018, pp. 983--993.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine, \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control,\" arXiv preprint arXiv:1812.00568, 2018.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Q. Bu et al., \"Closed-loop visuomotor control with generative expectation for robotic manipulation,\" Advances in Neural Information Processing Systems, vol. 37, pp. 139002--139029, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>C. Finn and S. Levine, \"Deep visual foresight for planning robotic motion,\" in 2017 IEEE international conference on robotics and automation, IEEE, 2017, pp. 2786--2793.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Nair, E. Mitchell, K. Chen, S. Savarese, and C. Finn, \"Learning language-conditioned robot behavior from offline data and crowd-sourced annotation,\" in Conference on robot learning, PMLR, 2022, pp. 1303--1315.\u00a0\u21a9</p> </li> <li> <p>S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, \"RoboDreamer: Learning compositional world models for robot imagination,\" in International conference on machine learning, PMLR, 2024, pp. 61885--61896.\u00a0\u21a9\u21a9</p> </li> <li> <p>M. Oquab et al., \"DINOv2: Learning robust visual features without supervision,\" Transactions on Machine Learning Research Journal, pp. 1--31, 2024.\u00a0\u21a9</p> </li> <li> <p>A. Kirillov et al., \"Segment anything,\" in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 4015--4026.\u00a0\u21a9</p> </li> <li> <p>B. Wang et al., \"This\\ &amp;that: Language-gesture controlled video generation for robot planning,\" in 2025 IEEE international conference on robotics and automation (ICRA), IEEE, 2025, pp. 12842--12849.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. Du et al., \"Learning universal policies via text-guided video generation,\" Advances in neural information processing systems, vol. 36, pp. 9156--9172, 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Liang et al., \"Video generators are robot policies,\" arXiv preprint arXiv:2508.00795, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Zhang et al., \"COMBO: Compositional world models for embodied multi-agent cooperation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Zhi et al., \"3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model,\" arXiv preprint arXiv:2506.06199, 2025.\u00a0\u21a9</p> </li> <li> <p>N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht, \"Cotracker: It is better to track together,\" in European conference on computer vision, Springer, 2024, pp. 18--35.\u00a0\u21a9</p> </li> <li> <p>N. Karaev, I. Makarov, J. Wang, N. Neverova, A. Vedaldi, and C. Rupprecht, \"Cotracker3: Simpler and better point tracking by pseudo-labelling real videos,\" arXiv preprint arXiv:2410.11831, 2024.\u00a0\u21a9</p> </li> <li> <p>N. Agarwal et al., \"Cosmos world foundation model platform for physical ai,\" arXiv preprint arXiv:2501.03575, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Tian et al., \"Predictive inverse dynamics models are scalable learners for robotic manipulation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Guo et al., \"Prediction with action: Visual policy learning via joint denoising process,\" Advances in Neural Information Processing Systems, vol. 37, pp. 112386--112410, 2024.\u00a0\u21a9</p> </li> <li> <p>J. Guo, X. Ma, Y. Wang, M. Yang, H. Liu, and Q. Li, \"FlowDreamer: A RGB-d world model with flow-based motion representations for robot manipulation,\" arXiv preprint arXiv:2505.10075, 2025.\u00a0\u21a9</p> </li> <li> <p>K. Black et al., \"\\(\\pi\\_0\\): A vision-language-action flow model for general robot control,\" arXiv preprint arXiv:2410.24164, 2024.\u00a0\u21a9</p> </li> <li> <p>G. R. Team et al., \"Gemini robotics: Bringing ai into the physical world,\" arXiv preprint arXiv:2503.20020, 2025.\u00a0\u21a9</p> </li> <li> <p>Q. Bu et al., \"Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems,\" arXiv preprint arXiv:2503.06669, 2025.\u00a0\u21a9</p> </li> <li> <p>S. Liu et al., \"RDT-1B: A diffusion foundation model for bimanual manipulation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Jang et al., \"DreamGen: Unlocking generalization in robot learning through video world models,\" arXiv preprint arXiv:2505.12705, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Wu et al., \"Ivideogpt: Interactive videogpts are scalable world models,\" Advances in Neural Information Processing Systems, vol. 37, pp. 68082--68119, 2024.\u00a0\u21a9</p> </li> <li> <p>T. Wan et al., \"Wan: Open and advanced large-scale video generative models,\" arXiv preprint arXiv:2503.20314, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Ye et al., \"Latent action pretraining from videos,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>B. Baker et al., \"Video pretraining (vpt): Learning to act by watching unlabeled online videos,\" Advances in Neural Information Processing Systems, vol. 35, pp. 24639--24654, 2022.\u00a0\u21a9</p> </li> <li> <p>Y. Angen, \"GigaBrain-0: A world model-powered vision-language-action model,\" arXiv:2510.19430, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Liu et al., \"RoboTransfer: Geometry-consistent video diffusion for robotic visual policy transfer,\" arXiv preprint arXiv:2505.23171, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. Azzolini et al., \"Cosmos-reason1: From physical common sense to embodied reasoning,\" arXiv preprint arXiv:2503.15558, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, \"Dream to manipulate: Compositional world models empowering robot imitation learning with imagination,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>B. Kerbl, G. Kopanas, T. Leimk\u00fchler, and G. Drettakis, \"3D gaussian splatting for real-time radiance field rendering.\" ACM Trans. Graph., vol. 42, no. 4, pp. 139--1, 2023.\u00a0\u21a9</p> </li> <li> <p>J. Xiao et al., \"World-env: Leveraging world model as a virtual environment for VLA post-training,\" arXiv preprint arXiv:2509.24948, 2025.\u00a0\u21a9</p> </li> <li> <p>E. Todorov, T. Erez, and Y. Tassa, \"Mujoco: A physics engine for model-based control,\" in 2012 IEEE/RSJ international conference on intelligent robots and systems, IEEE, 2012, pp. 5026--5033.\u00a0\u21a9</p> </li> <li> <p>T. Erez, Y. Tassa, and E. Todorov, \"Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx,\" in 2015 IEEE international conference on robotics and automation (ICRA), IEEE, 2015, pp. 4397--4404.\u00a0\u21a9</p> </li> <li> <p>R. Tedrake, \"Drake: Model-based design and verification for robotics.\" 2019.\u00a0\u21a9</p> </li> <li> <p>N. S\u00fcnderhauf et al., \"The limits and potentials of deep learning for robotics,\" The International journal of robotics research, vol. 37, no. 4--5, pp. 405--420, 2018.\u00a0\u21a9</p> </li> <li> <p>A. Afzal, D. S. Katz, C. L. Goues, and C. S. Timperley, \"A study on the challenges of using robotics simulators for testing,\" arXiv preprint arXiv:2004.07368, 2020.\u00a0\u21a9</p> </li> <li> <p>H. Choi et al., \"On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward,\" Proceedings of the National Academy of Sciences, vol. 118, no. 1, p. e1907856118, 2021.\u00a0\u21a9</p> </li> <li> <p>G. Dulac-Arnold, D. Mankowitz, and T. Hester, \"Challenges of real-world reinforcement learning,\" arXiv preprint arXiv:1904.12901, 2019.\u00a0\u21a9</p> </li> <li> <p>W. Zhao, J. P. textasciitilde na Queralta, and T. Westerlund, \"Sim-to-real transfer in deep reinforcement learning for robotics: A survey,\" in 2020 IEEE symposium series on computational intelligence (SSCI), IEEE, 2020, pp. 737--744.\u00a0\u21a9</p> </li> <li> <p>A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, \"CARLA: An open urban driving simulator,\" in Conference on robot learning, 2017, pp. 1--16.\u00a0\u21a9</p> </li> <li> <p>M. Deitke et al., \"Robothor: An open simulation-to-real embodied ai platform,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 3164--3174.\u00a0\u21a9</p> </li> <li> <p>Y. Li, Y. Zhu, J. Wen, C. Shen, and Y. Xu, \"WorldEval: World model as real-world robot policies evaluator,\" arXiv preprint arXiv:2505.19017, 2025.\u00a0\u21a9</p> </li> <li> <p>G. Team et al., \"Gemini: A family of highly capable multimodal models,\" arXiv preprint arXiv:2312.11805, 2023.\u00a0\u21a9</p> </li> <li> <p>J. Quevedo, P. Liang, and S. Yang, \"Evaluating robot policies in a world model,\" arXiv preprint arXiv:2506.00613, 2025.\u00a0\u21a9</p> </li> <li> <p>A. Hurst et al., \"Gpt-4o system card,\" arXiv preprint arXiv:2410.21276, 2024.\u00a0\u21a9</p> </li> <li> <p>H. He, Y. Zhang, L. Lin, Z. Xu, and L. Pan, \"Pre-trained video generative models as world simulators,\" arXiv preprint arXiv:2502.07825, 2025.\u00a0\u21a9</p> </li> <li> <p>F. Zhu, H. Wu, S. Guo, Y. Liu, C. Cheang, and T. Kong, \"Irasim: Learning interactive real-robot action simulators,\" in ICCV, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Wang, K. Zhao, C. Liu, and X. Chen, \"Learning real-world action-video dynamics with heterogeneous masked autoregression,\" arXiv preprint arXiv:2502.04296, 2025.\u00a0\u21a9</p> </li> <li> <p>A. Escontrela et al., \"Video prediction models as rewards for reinforcement learning,\" Advances in Neural Information Processing Systems, vol. 36, pp. 68760--68783, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"05-key-tech-challenges/","title":"V Key Techniques and Notable Challenges","text":""},{"location":"05-key-tech-challenges/#v-key-techniques-and-notable-challenges","title":"V Key Techniques and Notable Challenges","text":"<p>\u2003\u2003This section summarizes the key techniques that drive the development of world models and discusses the major challenges that remain in achieving general, scalable, and robust modeling. Some techniques and concepts are revisited across subsections to emphasize their central importance.</p>"},{"location":"05-key-tech-challenges/#a-data-limitations","title":"A. Data Limitations","text":"<p>\u2003\u2003World models require large amounts of data and supervision to learn generalizable representations of world dynamics and support diverse tasks. However, collecting real-world robotic data is labor-intensive and costly, and the available data are often heterogeneous in format and modality. To overcome these limitations, a variety of strategies have been proposed.</p> <p> 1) Training Data Scarcity </p> <p> 1.1) Leveraging Pre-trained Models</p> <p>\u2003\u2003Given the limited availability of training data, many approaches leverage existing pre-trained models. For example, Xiang et al.<sup>1</sup> bypass the need for training from scratch by integrating a pre-trained LLM and a pre-trained video model, requiring only lightweight fine-tuning. Zhu et al. <sup>2</sup> initialize IRASim with the pre-trained weights of OpenSora <sup>3</sup> to expedite training. Similarly, Sudhakar et al. <sup>4</sup> leverage a pre-trained diffusion model, while Wang et al. <sup>5</sup> utilize Stable Video Diffusion, fine-tuned with robotic videos to adapt to the robotics domain. Song et al. <sup>6</sup> further exploit the world knowledge embedded in pre-trained autoregressive video generation models such as NOVA <sup>7</sup>.</p> <p> 1.2) Incorporating Auxiliary Data Sources</p> <p>\u2003\u2003Some works tackle the shortage of robot data by using other available sources, such as human manipulation datasets. For instance, Zhi et al. <sup>8</sup> use both human and robot manipulation videos for training. However, these datasets often contain cluttered backgrounds and similar-looking objects. To address this, they apply optical flow constraints to make the learned representation embodiment-agnostic. Sudhakar et al. <sup>4</sup> leverage an automatic hand segmentation method to obtain agent-agnostic data for robot learning. Others resort to more diverse data. For example, Yang et al. <sup>9</sup> leverage diverse kinds of data, including objects, scenes, actions, motions, language, and motor control, and convert all actions into a common format.</p> <p> 1.3) Synthetic Data Generation</p> <p>\u2003\u2003Instead of relying on real-world data, Deng et al. <sup>10</sup> synthesize large-scale action data to train their model. To address the scarcity of 4D data, the Aether team <sup>11</sup> generate RGB-D synthetic videos and develop a robust camera-pose annotation pipeline to reconstruct full 4D dynamics. Similarly, Zhen et al. <sup>12</sup> build a 4D embodied video dataset that combines synthetic data with ground-truth depth, normal information and real-world data with estimated depth and normal maps obtained from off-the-shelf estimators.</p> <p> 2) Heterogeneous action data</p> <p>\u2003\u2003World models should be able to understand different forms of actions and embodiments to ensure their real-world applications. A basic strategy is to utilize diverse datasets for training. However, the inherent cross-domain and cross-embodiment nature of datasets lead to heterogeneous actions data, including action spaces, action frequencies, and action horizon. For example, diverse embodiment (e.g., different degrees of freedom across robotic arms) and control interface (end effector (EEF) position for arms) would lead to actions of different forms. To handle this, Zheng et al. <sup>13</sup> learn to capture their shared structural features to obtain the generic atomic behaviors by means of vision language models. Similarly, Zheng et al. <sup>14</sup> lean a share latent space for actions by decoupling observation and actions. More strategies can borrow from relevant fields <sup>15</sup> <sup>16</sup> <sup>17</sup>.</p> <p> 3) Action Label Missing </p> <p>\u2003\u2003Action-labeled data, which are essential for learning action-conditioned future predictions <sup>9</sup>, are particularly scarce in real-world settings.</p> <p> 3.1) Self-supervised Learning</p> <p>\u2003\u2003To address the lack of action-labeled data, self-supervised learning techniques have been explored <sup>18</sup> <sup>19</sup> <sup>20</sup>. For instance, Finn et al. <sup>18</sup> <sup>19</sup> propose to learn pixel-level motion in a self-supervised manner, while Ebert et al. <sup>21</sup> <sup>20</sup> leverage image-to-image registration between consecutive video frames to capture dynamics without explicit action labels. However, goal image-based learning presents several drawbacks: such goals are inconvenient for humans to specify, may over-constrain the desired behavior (leading to sparse rewards), or under-specify task-relevant information for non-goal-reaching tasks.</p> <p> 3.2) Action Label Extraction</p> <p>\u2003\u2003Another approach to handling missing action labels is to infer them directly from unlabeled videos. More specifically, Bruce et al. <sup>22</sup> <sup>23</sup> employ latent action autoencoders to extract latent actions in a self-supervised manner. In their studies, Bruce et al. <sup>22</sup> sample actions uniformly, while Gao et al. <sup>23</sup> introduce biased action sampling to encourage broader exploration and enable action reuse across contexts.Jiang et al. <sup>24</sup> extract pseudo-actions using either a latent action model <sup>25</sup> or an inverse dynamics model (IDM) <sup>26</sup>. Du et al. <sup>27</sup> <sup>28</sup> <sup>29</sup> <sup>30</sup> learn from unlabeled videos by training inverse dynamics models to infer actions or their embeddings. Ren et al. <sup>28</sup> further integrate an inverse dynamics module into a latent dynamics model to leverage rich temporal representations, improving the temporal consistency of predicted actions. Villar et al. <sup>29</sup> predict latent actions from object-centric representations.</p> <p> 3.3) Other strategies</p> <p>\u2003\u2003Some works aim to leverage pre-trained video models. For instance, Rigter et al. <sup>31</sup> adapt a pre-trained video diffusion model for action-conditioned world modeling by training a lightweight adapter, which is then fine-tuned on a small set of domain-specific, action-labeled videos. Black et al. <sup>32</sup> similarly employ a pre-trained image-editing diffusion model to support video-based world modeling. In addition, Zhu et al. <sup>33</sup> design a unified world model that integrates the action and video diffusion processes within a unified transformer architecture using separate diffusion timesteps. This can enable learning from action-free video data. Ko et al. <sup>30</sup> utilize optical flow extracted from videos, thereby circumventing the need for explicit action labels. </p>"},{"location":"05-key-tech-challenges/#b-perception-and-representation","title":"B. Perception and Representation","text":"<p>\u2003\u2003Perception lies at the heart of robotic world models, enabling systems to interpret task instructions and transform raw sensory inputs into meaningful representations. These representations allow robots to understand structured environments and, in turn, predict, react, and plan effectively.</p> <p> 1) Inputs</p> <p> Language. Task instructions are usually given in language. Many methods use pretrained models such as CLIP <sup>34</sup> <sup>30</sup> <sup>35</sup> <sup>36</sup>, Phi <sup>37</sup> <sup>6</sup>, or conditional VAEs <sup>6</sup> to extract semantic representations from the instructions.  </p> <p> Visual data. Similarly, visual inputs are often processed using pre-trained visual encoders. For example, Tian et al. <sup>36</sup> leverage pre-trained Vision Transformers (ViTs) <sup>38</sup> to process image observations. Wu et al. <sup>39</sup> employ a conditional VQGAN that encodes only task-relevant dynamic information, such as the position and pose of moving objects, to reduce temporal redundancy across frames. An autoregressive, GPT-like transformer is then used to generate the next tokens, which are decoded into future frames.  </p> <p> Action data. Actions are sometimes represented as integer values, which lack the contextual richness. This limitation can prevent world models from accurately capturing the intended meaning behind actions. To address this, He et al. <sup>40</sup> propose representing actions through language templates that explicitly encode their semantic meaning. In many cases, actions are instead expressed in natural language, as noted above. While this enables richer semantic representations, it also introduces challenges, such as instruction-following ambiguity, which are discussed in Section V-B2a.  </p> <p> Diverse data inputs. Robots need to gain a structured understanding of the world by jointly considering diverse sensory inputs. To achieve this, Song et al. <sup>6</sup> embed images and robot actions into a unified physical space, enabling the model to capture the sequential evolution of both the robot and its environment. Hong et al. <sup>41</sup> incorporate visual, auditory, tactile, and thermal modalities, projecting them into a shared feature space where a language model generates subsequent states and action tokens.</p> <p> 2) Challenges</p> <p> 2.1) Instruction Understanding and Following</p> <p>\u2003\u2003Instructions convey task goals and can take various forms, including linguistic directives (natural language or structured text), visual cues (sketches, images, or demonstration videos), and others. Compared to image-based goals, textual descriptions provide a more abstract, compositional, and flexible way of specifying objectives, enabling better generalization, clearer intent communication, and more efficient human\u2013robot interaction. Many recent works express target goals through text descriptions <sup>27</sup>. Ideally, language instructions should clearly describe the task and remain easily interpretable by the model. However, real-world scenarios often involve ambiguous or novel instructions, making effective interpretation and grounding critical for successful task execution.  </p> <p> Ambiguous instructions In real-world scenarios, language instructions are often ambiguous (e.g., ''put this near here'' <sup>5</sup>). To resolve such ambiguity, Wang et al. <sup>5</sup> use pointing gestures, interpreted through 2D gripper and object tracking, as an additional instruction modality.  </p> <p> New instructions World models are constrained to make predictions based on language instructions similar to those encountered during training, limiting their ability to generalize to novel commands. To solve this problem, Xiang et al. <sup>1</sup> curate a large and diverse set of action-state sequences from re-captioned videos and simulations, and fine-tune world models on this data to improve instruction interpretation and generalize to novel commands and tasks. Li et al. <sup>42</sup> employ a text parser to decompose language instructions into primitives, separating actions and spatial relationships. This decomposition allows the model to flexibly recombine these components and generalize to previously unseen combinations of instructions. However, decomposing instructions into primitives can ignore their interrelationships. To address this, Li et al. <sup>43</sup> represent each instruction as an action tree, capturing the hierarchical structure among primitives to better model task organization.</p> <p> 2.2) Raw Pixels Modeling vs. Concept Abstraction </p> <p>\u2003\u2003Some studies suggest that humans make predictions based on abstract concepts rather than raw pixels <sup>44</sup>. Instead of converting images into discrete tokens <sup>9</sup> <sup>39</sup>, Chen et al. <sup>44</sup> use learnable convolutional layers to project images into continuous semantic embeddings. Song et al. <sup>6</sup> adopt an open-source 3D variational autoencoder (Open-Sora <sup>3</sup>) to obtain video representations. In contrast, another line of work operates directly in pixel space. For instance, Ko et al. <sup>30</sup> adapt a U-Net-based image diffusion model with factorized spatial\u2013temporal convolutions <sup>45</sup> to jointly capture spatial and temporal information.</p> <p> 2.3) Task-irrelevant Issues </p> <p>\u2003\u2003Visual data often contain information irrelevant to the task, and models such as Vision Transformers (ViTs) may produce hundreds of features per image, affecting both efficiency and effectiveness. To address this, Tian et al. <sup>36</sup> extract task-relevant features using a perceiver resampler~\\cite{alayrac2022flamingo}. Ren et al. <sup>28</sup> learn compact visual representations that preserve fine-grained temporal dynamics through a causal encoder\u2013decoder structure and quantization with a discrete codebook <sup>46</sup>.</p> <p> 2.4) Spatiotemporal Awareness </p> <p>\u2003\u2003Understanding the world requires modeling how spatial structures evolve over time. To this end, several works design architectures that explicitly capture spatial and temporal dependencies. Tian et al. <sup>36</sup> enhance token representations with learnable positional embeddings at each timestep to capture temporal information. Bruce et al. <sup>22</sup> develop a spatiotemporal transformer composed of multiple spatiotemporal blocks to model spatial\u2013temporal relationships in dynamic scenes. Ko et al. <sup>30</sup> adopt factorized spatiotemporal convolutions following the design of <sup>47</sup>. Zhang et al. <sup>48</sup> extract spatiotemporal patch representations using a masked autoencoder <sup>38</sup>. Other studies incorporate additional cues to better understand the three-dimensional structure of the environment. For example, Zhang et al. <sup>48</sup> estimate depth information using depth estimation techniques <sup>49</sup> to enhance 3D spatial understanding. When encoding multi-view inputs, Liao et al. <sup>50</sup> augment each token with 2D rotary positional embeddings, view-specific learnable embeddings, and timestep encodings to promote spatiotemporal alignment while preserving viewpoint-specific distinctions.</p> <p></p>"},{"location":"05-key-tech-challenges/#c-long-horizon-reasoning","title":"C. Long-horizon Reasoning","text":"<p>\u2003\u2003Many robotic tasks require coherent long-horizon reasoning, where achieving the final objective depends on executing a temporally consistent sequence of actions over extended time scales. Existing methods are limited in long-horizon predictions  <sup>51</sup> <sup>52</sup> <sup>53</sup> <sup>54</sup> <sup>55</sup>. For example, Ha et al. <sup>52</sup> <sup>53</sup> <sup>54</sup> <sup>55</sup> predefine temporal horizons to guide planning in their world models. In terms of video generation, existing methods still suffer from limited length (short-horizon future video) <sup>56</sup>. For example, Ko et al. <sup>30</sup> predicts a fixed number (eight) of future frames with U-Net based diffusion model  <sup>45</sup>. Bruce et al. <sup>22</sup> can only memorize 16 frames and cannot produce consistent predictions. For autoregressive models, small prediction errors compound sequentially, leading to substantial inaccuracies in long-horizon forecasts.</p> <p> 1) Closed-loop learning scheme </p> <p>\u2003\u2003A line of work enabling long-term planning/predictions by learning through interaction with feedback and adjusting their behaviour accordingly <sup>57</sup> <sup>34</sup>. For example, Ebert et al. <sup>21</sup> <sup>20</sup> utilize image-to-image registration between predicted video frames and both the start and the goal images with the average length of the warping vectors as a cost function. The model would continue to retry until the task is completed. Du et al. <sup>57</sup> proposes a recursive planning framework comprising action proposal, video rollout generation, and evaluation. Vision\u2013language models (VLMs) are used to propose potential next actions, while video generation models simulate multiple possible future rollouts. The resulting trajectories are then evaluated by the VLMs to select the optimal action. Du et al. <sup>50</sup> design a neural simulator that predicts future visuals, enabling policy models to interact within a consistent environment. A sparse memory mechanism is leveraged to further enhance the consistency over the time. </p> <p> 2) Subgoals</p> <p>\u2003\u2003Pre-trained models possess a vast repository of commonsense and procedural knowledge that can be leveraged to decompose a high-level goal, often specified in natural language (e.g., \"make a cup of coffee\"), into a logical sequence of concrete sub-goals or skills. Bu et al. <sup>34</sup> propose to promote long-horizon manipulation tasks by decomposing the goal into sub-goals and handling error accumulations by designing a real-time feedback mechanism. Yang et al. <sup>58</sup> leverage VLM to produce sub-goals and utilize coarse and fine video diffusion models to generate long-horizon videos. Chen et al. <sup>59</sup> utilizes an LLM to generate a multi-stage plan and design a LLM-based dense reward generator for sub-tasks, providing crucial guidance for long-horizon planning. </p> <p> 3) Hierarchical structures</p> <p>\u2003\u2003Bu et al. <sup>60</sup> propose hierarchical world models with Adaptive Temporal Abstractions that separate the modeling of dynamics into high-level and low-level latent states. The low-level model captures fine-grained, short-term dynamics for immediate reactions, while the high-level model abstracts over longer temporal horizons to represent extended dependencies and long-term goals. By dynamically adapting the temporal granularity of the high-level latent states, the model can efficiently plan and predict over long horizons while maintaining accurate short-term predictions through the low-level module.</p> <p> 4) More strategies</p> <p>\u2003\u2003Driess et al. <sup>61</sup> provide a goal image in addition to language instructions. Du et al. <sup>57</sup> propose to take advantage of long-horizon inference of VLMs and the low-level visual dynamic modelling ability of text-to-video models to handle long-horizon visual planning. A tree search over the space of possible video sequences to find proper long-horizon plans. Ren et al. <sup>28</sup> lean compact representations for the visual world that preserve the detailed temporal dynamics by means of causal encoder-decoder and quantization with a discrete codebook <sup>46</sup>. </p>"},{"location":"05-key-tech-challenges/#d-spatiotemporal-consistency","title":"D. Spatiotemporal Consistency","text":"<p>\u2003\u2003Spatiotemporal consistency plays a vital role in ensuring coherent and physically plausible predictions of future states. It guarantees that the model preserves object continuity, motion smoothness, and causal relationships across time, enabling stable video simulation and reliable dynamics forecasting.</p> <p> 1) Data perspective</p> <p>\u2003\u2003In conditional video synthesis, Du et al. <sup>27</sup> incorporates the observed image as additional context when denoising each frame. Specifically, it adapts a temporal super-resolution diffusion architecture by tiling the conditioned visual observation across all timesteps. Each intermediate noisy frame is concatenated with the observed image throughout sampling, providing a strong spatial anchor that enforces consistent environmental states across time. Ko et al. <sup>30</sup> concatenates the initial condition frame with all subsequent frames, providing a stable reference that preserves both the spatial layout and temporal evolution of the environment throughout the sequence. Zhen et al. <sup>12</sup> refine depth maps using normal integration to enhance spatial consistency. Optical flow is then calculated to ensure depth coherence across frames, maintaining consistent scene geometry over time. </p> <p> 2) Model perspective</p> <p>\u2003\u2003Yang et al. <sup>58</sup> noted that in autoregressive predictions, standard spatiotemporal attention in video diffusion models degrades frame consistency due to limited long-range context. To address this, the temporal attention layers are replaced with 3D full attention layers, enabling computation of attention across all spatiotemporal tokens and better modeling of large motions. Additionally, the spatial attention layers are modified by reinjecting the VAE features of the first frame and computing cross-attention with the spatial tokens of the query features, further enhancing frame coherence.</p> <p> 3) Memory mechanism</p> <p>\u2003\u2003is often used to enhance the spatiotemporal consistency. For example, Liao et al. <sup>50</sup> design a sparse memory mechanism to provide long-term historical context, improving spatiotemporal consistency and task relevance. More information can refer to Section V-G.</p>"},{"location":"05-key-tech-challenges/#e-generalization","title":"E. Generalization","text":"<p>\u2003\u2003Robots are expected to operate robustly in complex and novel environments, interacting with unfamiliar objects and performing tasks beyond their training distribution.  </p> <p> 1) Data scaling</p> <p>\u2003\u2003An intuitive and effective strategy to enhance generalization is to scale the diversity and volume of training data. For example, Cheang et al. <sup>62</sup> increase the number of pre-training videos from 0.8 million in <sup>63</sup> to 38 million. Assran et al. <sup>64</sup> expand the dataset from 2 million used by <sup>65</sup> to 22 million videos. Wang et al. <sup>14</sup> expand each of the 40 datasets by increasing trajectories from 10 up to 10<sup>6</sup>. Cheang et al. <sup>66</sup> train the model with web-scale vision-language data,  human trajectory data and robot trajectory data. Kevin et al. <sup>67</sup> leverage diverse mobile manipulator data, diverse multi-environment non-mobile robot data, cross-embodiment laboratory data, high-level subtask prediction, and multi-modal web data. Cheang et al. <sup>68</sup> <sup>62</sup> investigate data augmentation strategies to enhance generalization. In <sup>68</sup>, object rotation and roto-translation are applied. Cheang et al. <sup>62</sup> generate novel scenes by injecting objects using a diffusion model <sup>69</sup> and/or altering backgrounds with the Segment Anything Model (SAM)   <sup>70</sup>. A video generation model <sup>70</sup> is subsequently employed to synthesize videos that preserve the original robot motions from the inpainted frames. Liao et al. <sup>50</sup> augment the dataset with a diverse set of failure cases, including erroneous executions, incomplete behaviors, and suboptimal control trajectories\u2014collected from both human teleoperation and real-world robotic deployments. One problem of data scaling is that it is unlikely to collect all data for each tasks. At the same time, how to balance different data tasks is also challenging. Moreover, performance gains by scaling data is also limited for consistent performance improvements. </p> <p> 2) Use of pretrained models</p> <p>\u2003\u2003Many methods aim to enhance generalization by leveraging the generative capabilities of video models. For example, Zhu et al. <sup>11</sup> combine video generation with geometric-aware learning to improve synthetic-to-real generalization across unseen viewpoints and support multiple downstream tasks. Zhen et al. <sup>12</sup> fine-tune a video generation model on RGB, depth, and normal videos to encode detailed shape, configuration, and temporal dynamics, enabling generalization to unseen scenes, objects, and cross-domain scenarios. The generalization capabilities of large language models, such as video-language models <sup>71</sup> and vision-language models <sup>72</sup>, can be leveraged to enhance world models. By extracting high-level knowledge about the environment, these models facilitate more effective low-level dynamics modeling.</p> <p> 3) Instructions decomposing </p> <p>\u2003\u2003Another generation issue comes from unseen instructions. To handle this, Zhou et al. <sup>42</sup> enhance the ability to unseen instructions by decomposing each spatial relation phrase into a set of compositional components with the pre-trained parser <sup>73</sup> and the rule-based approach. Detailed information can refer to Section V-B2a.</p> <p> 4) Invariant Representations </p> <p>\u2003\u2003Generalization can be significantly improved by learning representations that are invariant to superficial or task-irrelevant changes in the environment. For example, Pang et al. <sup>74</sup> model learns to explicitly decompose visual observations into a view-invariant representation, which is used for the control policy, and a view-dependent representation. This decoupling makes the resulting policy robust to changes in camera viewpoint, a common source of failure in visuomotor control. Similarly, the Martinez et al. <sup>75</sup> framework learns a transferable communicative context between two agents, which enables zero-shot adaptation to entirely unseen sparse-reward environments by decoupling the representation learning from the control problem. Wu et al. <sup>76</sup> disentangle the modeling of context and dynamics by introducing a context encoder, enabling the model to capture shared knowledge for predictions.</p> <p> 5) Task-relevant information focused </p> <p>\u2003\u2003Video data often contain irrelevant data to the actions such as background and robot arm, which would limited the generalization ability of the learned world models. To handle this, <sup>8</sup> propose to object-centric world models, which concentrated on object movements via the optical flow predictions that is independent of embodiment. Finn et al. <sup>18</sup> propose to explicitly model and predict motion that are relatively invariant to the object appearance, enabling long-range predictions and generalize to unseen objects.</p> <p> 6) Other strategies</p> <p>\u2003\u2003Black et al. <sup>32</sup> use a pretrained image-editing model to generate subgoals from language commands and current observations, enabling low-level controllers to act and generalize to novel objects and scenarios. Self-supervised learning without task-specific rewards that can enhancing generalization abilities into different tasks <sup>77</sup>. </p>"},{"location":"05-key-tech-challenges/#f-physics-informed-learning","title":"F. Physics-informed Learning","text":"<p>\u2003\u2003Existing world models struggle to generate physically consistent videos because they lack an inherent understanding of physics, often producing unrealistic dynamics and implausible event sequences. Simply scaling up training data or model size is insufficient to capture the underlying physical laws <sup>78</sup>. To address this challenge, several approaches have been proposed. For example, Yang et al. <sup>79</sup> introduce a two-stage image-to-video generation framework that explicitly incorporates physics through vision- and language-informed physical priors. Team et al. <sup>11</sup> estimate depth and camera pose directly from videos, facilitating physics-informed learning and enabling world models to infer and predict physically consistent dynamics. Peper et al. <sup>80</sup> argue that advancing from physics-informed to physics-interpretable world models requires rethinking model design, and propose four guiding principles: organizing latent spaces by physical intent, encoding invariant and equivariant environmental representations, integrating multiple supervision signals, and partitioning generative outputs to improve both scalability and verifiability.</p> <p></p>"},{"location":"05-key-tech-challenges/#g-memory","title":"G. Memory","text":"<p>\u2003\u2003Memory mechanisms enable world models to store and retrieve relevant past information, supporting hidden-state disambiguation and long-horizon reasoning. For example, LeCun et al. <sup>81</sup> incorporate a memory module that maintains past, current, and predicted world states along with intrinsic costs, allowing retrieval of contextual information for reasoning and training. Huang et al. <sup>82</sup> propose a sparse contextual memory mechanism that preserves essential prior information throughout the generation process in a non-redundant manner, theoretically enabling the generation of sequences of arbitrary length. Zhou et al. <sup>83</sup> employ a 3D feature-map memory to maintain temporal consistency during sequence generation.  </p> <p> Memory efficiency Standard transformer blocks apply Multi-Head Self-Attention (MHA) to all tokens in the input token sequence, resulting in quadratic computation cost. Zhu et al. <sup>2</sup> leverage the memory-efficient spatial-temporal attention mechanism to reduce the computation cost. Liao et al. <sup>50</sup> randomly sampled parse memory frames from prior video history to augment temporal diversity to improve representational invariance, and use low-frame-rate video sequence for fine-tuning frames.</p>"},{"location":"05-key-tech-challenges/#h-other-challenges","title":"H. Other Challenges","text":"<p> 1) Video fidelity </p> <p>\u2003\u2003To achieve high-fidelity video generation, several methods leverage powerful generative models. For instance, Ko et al. <sup>30</sup> employ an image diffusion model based on a U-Net with factorized spatiotemporal convolutions as the fundamental building block. Guo et al. <sup>84</sup> utilize the pre-trained variational autoencoder from Stable Diffusion <sup>85</sup>. Souvcek et al. <sup>86</sup> propose to make use of a variety of action and final state prompts. </p> <p> 2) Closed-loop Learning </p> <p>\u2003\u2003Closed-loop learning enables agents to actively refine their internal world models by observing and responding to real-time feedback from the environment. This continuous perception\u2013action cycle grounds learning in physical reality, enhances generalization, and allows adaptive correction\u2014key properties for robust embodied intelligence. Driess et al. <sup>61</sup> update observations based on the actions executed, which are then fed into VLMs to enable the robot to correct or reorganize its plan in response to environmental changes and task progress. Bu et al. <sup>34</sup> design a feedback mechanism that is based on the element-wise discrepancy measure between current and goal state embeddings. Zhi et al. <sup>8</sup>, estimate the location of the moving objects, depth prediction, 3D optical flow by input into GPT-4o to verify alignment with given instructions, enabling closed-loop planning. </p> <p> 3) Sim-to-real gap </p> <p>\u2003\u2003Huang et al. <sup>82</sup> propose combining the generative model with 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the sim-to-real gap.</p> <p> 4) 3D robotics world predicting </p> <p>\u2003\u2003General-purpose video generation models neglect the substantial gap between their representation space and the three-dimensional, temporally interconnected robotics environment, thereby hindering accurate action policy prediction. For example, Wen et al. <sup>87</sup> focuses on 2D image prediction before action generation. To handle this, Huang et al. <sup>82</sup> propose Free Anchor Views (FAVs), a multi-view video representation offering flexible, task-adaptive perspectives to address challenges like motion ambiguity and environmental constraints. </p> <p> 5) Fine-grained robot-object interaction </p> <p>\u2003\u2003Robots are expected to perform precise manipulation, which requires world models to support fine-grained robot-object interactions. To achieve this, Zhu et al. <sup>2</sup> design a novel frame-level action-conditioning module to achieve precise action-frame alignment. He et al. <sup>40</sup> adopt two different pre-trained video generative models as the base models, introduces a minimalist yet powerful add-on action-conditioned module that improves frame-level action awareness while maintaining architectural flexibility.</p> <p> 6) Multi-agent operation </p> <p>\u2003\u2003Certain tasks necessitate coordinated operation among multiple robots to achieve successful completion. To this end, Zhang et al. <sup>88</sup> factorize the joint actions of different agents as a set of text prompt and leverage composable video diffusion models to learn world dynamics and make predictions. An agent-dependent loss is imposed to let the model focus on the related pixel, where the loss coefficient matrix is based on each agent\u2019s reachable region reachable region.</p> <p> </p> Fig. 7. Potential Core Components and Capabilities of World Models. <p> 7) Reasoning </p> <p>\u2003\u2003Zhou et al. <sup>48</sup> enhance the reasoning and genrealization ability by incorporating context information and predicting dynamic regions, depth map, semantic knowledge by means of foundation models, e.g., DINOv2 <sup>89</sup> and SAM <sup>70</sup>. Ye et al. <sup>90</sup> introduce an Embodied Chain-of-Thought (CoT) as an intermediate reasoning representation, enabling more structured and interpretable decision-making in embodied tasks. Ye et al. <sup>90</sup> <sup>91</sup> generates a sub-goal image that represents the robot\u2019s planned state in pixel space, and then conditions its action on both the current observation and the generated subgoal image.</p> <p> 8) Error propagation</p> <p>\u2003\u2003Cen et al. <sup>92</sup> indicate that generating multiple actions in sequence leads to performance drop in autoregressive models. The primary reason for this is that pretrained multimodal language models have predominantly been exposed to images and text rather than actions, resulting in limited action generalization capabilities. In autoregressive models where subsequent actions are conditioned on preceding ones, error propagation becomes a critical issue, as the earlier incorrect predictions influence subsequent actions over time. To handle this, Cen et al. <sup>92</sup> propose an attention mask strategy that selectively masks prior actions during the generation of the current action. It enables both future imagination and action generation. </p>"},{"location":"05-key-tech-challenges/#references","title":"References","text":"<ol> <li> <p>J. Xiang et al., \"Pandora: Towards general world model with natural language actions and video states,\" arXiv preprint arXiv:2406.09455, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Zhu, H. Wu, S. Guo, Y. Liu, C. Cheang, and T. Kong, \"Irasim: Learning interactive real-robot action simulators,\" in ICCV, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Z. Zheng et al., \"Open-sora: Democratizing efficient video production for all,\" arXiv preprint arXiv:2412.20404, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>S. Sudhakar, R. Liu, B. V. Hoorick, C. Vondrick, and R. Zemel, \"Controlling the world by sleight of hand,\" in European conference on computer vision, Springer, 2024, pp. 414--430.\u00a0\u21a9\u21a9</p> </li> <li> <p>B. Wang et al., \"This\\ &amp;that: Language-gesture controlled video generation for robot planning,\" in 2025 IEEE international conference on robotics and automation (ICRA), IEEE, 2025, pp. 12842--12849.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Z. Song, S. Qin, T. Chen, L. Lin, and G. Wang, \"Physical autoregressive model for robotic manipulation without action pretraining,\" arXiv preprint arXiv:2508.09822, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>H. Deng et al., \"Autoregressive video generation without vector quantization,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Zhi et al., \"3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model,\" arXiv preprint arXiv:2506.06199, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, \"Learning interactive real-world simulators,\" arXiv preprint arXiv:2310.06114, vol. 1, no. 2, p. 6, 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>S. Deng et al., \"Graspvla: A grasping foundation model pre-trained on billion-scale synthetic action data,\" arXiv preprint arXiv:2505.03233, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Zhu et al., \"Aether: Geometric-aware unified world modeling,\" in ICCV, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>H. Zhen et al., \"TesserAct: Learning 4D embodied world models,\" arXiv preprint arXiv:2504.20995, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>J. Zheng et al., \"Universal actions for enhanced embodied foundation models,\" in Proceedings of the computer vision and pattern recognition conference, 2025, pp. 22508--22519.\u00a0\u21a9</p> </li> <li> <p>L. Wang, K. Zhao, C. Liu, and X. Chen, \"Learning real-world action-video dynamics with heterogeneous masked autoregression,\" arXiv preprint arXiv:2502.04296, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>R. Doshi, H. R. Walke, O. Mees, S. Dasari, and S. Levine, \"Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation,\" in Conference on robot learning, PMLR, 2025, pp. 496--512.\u00a0\u21a9</p> </li> <li> <p>O. M. Team et al., \"Octo: An open-source generalist robot policy,\" arXiv preprint arXiv:2405.12213, 2024.\u00a0\u21a9</p> </li> <li> <p>L. Wang, X. Chen, J. Zhao, and K. He, \"Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers,\" Advances in neural information processing systems, vol. 37, pp. 124420--124450, 2024.\u00a0\u21a9</p> </li> <li> <p>C. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" in Proceedings of the 30th international conference on neural information processing systems, 2016, pp. 64--72.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>C. Finn and S. Levine, \"Deep visual foresight for planning robotic motion,\" in 2017 IEEE international conference on robotics and automation, IEEE, 2017, pp. 2786--2793.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine, \"Visual foresight: Model-based deep reinforcement learning for vision-based robotic control,\" arXiv preprint arXiv:1812.00568, 2018.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>F. Ebert, S. Dasari, A. X. Lee, S. Levine, and C. Finn, \"Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning,\" in Conference on robot learning, PMLR, 2018, pp. 983--993.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Bruce et al., \"Genie: Generative interactive environments,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 4603--4623.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>S. Gao, S. Zhou, Y. Du, J. Zhang, and C. Gan, \"AdaWorld: Learning adaptable world models with latent actions,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Jang et al., \"DreamGen: Unlocking generalization in robot learning through video world models,\" arXiv preprint arXiv:2505.12705, 2025.\u00a0\u21a9</p> </li> <li> <p>S. Ye et al., \"Latent action pretraining from videos,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>B. Baker et al., \"Video pretraining (vpt): Learning to act by watching unlabeled online videos,\" Advances in Neural Information Processing Systems, vol. 35, pp. 24639--24654, 2022.\u00a0\u21a9</p> </li> <li> <p>Y. Du et al., \"Learning universal policies via text-guided video generation,\" Advances in neural information processing systems, vol. 36, pp. 9156--9172, 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Z. Ren et al., \"Videoworld: Exploring knowledge learning from unlabeled videos,\" in Proceedings of the computer vision and pattern recognition conference, 2025, pp. 29029--29039.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>A. Villar-Corrales and S. Behnke, \"PlaySlot: Learning inverse latent dynamics for controllable object-centric video prediction and planning,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum, \"Learning to act from actionless videos through dense correspondences,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Rigter, T. Gupta, A. Hilmkil, and C. Ma, \"AVID: Adapting video diffusion models to world models,\" in Reinforcement learning conference, 2025.\u00a0\u21a9</p> </li> <li> <p>K. Black et al., \"Zero-shot robotic manipulation with pre-trained image-editing diffusion models,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>C. Zhu, R. Yu, S. Feng, B. Burchfiel, P. Shah, and A. Gupta, \"Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets,\" arXiv preprint arXiv:2504.02792, 2025.\u00a0\u21a9</p> </li> <li> <p>Q. Bu et al., \"Closed-loop visuomotor control with generative expectation for robotic manipulation,\" Advances in Neural Information Processing Systems, vol. 37, pp. 139002--139029, 2024.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>A. Radford et al., \"Learning transferable visual models from natural language supervision,\" in International conference on machine learning, PmLR, 2021, pp. 8748--8763.\u00a0\u21a9</p> </li> <li> <p>Y. Tian et al., \"Predictive inverse dynamics models are scalable learners for robotic manipulation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>M. Javaheripi et al., \"Phi-2: The surprising power of small language models,\" Microsoft Research Blog, vol. 1, no. 3, p. 3, 2023.\u00a0\u21a9</p> </li> <li> <p>K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \"Masked autoencoders are scalable vision learners,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16000--16009.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Wu et al., \"Ivideogpt: Interactive videogpts are scalable world models,\" Advances in Neural Information Processing Systems, vol. 37, pp. 68082--68119, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. He, Y. Zhang, L. Lin, Z. Xu, and L. Pan, \"Pre-trained video generative models as world simulators,\" arXiv preprint arXiv:2502.07825, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, \"Multiply: A multisensory object-centric embodied large language model in 3d world,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 26406--26416.\u00a0\u21a9</p> </li> <li> <p>S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, \"RoboDreamer: Learning compositional world models for robot imagination,\" in International conference on machine learning, PMLR, 2024, pp. 61885--61896.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. Li et al., \"ManipDreamer: Boosting robotic manipulation world model with action tree and visual guidance,\" arXiv preprint arXiv:2504.16464, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Chen et al., \"EgoAgent: A joint predictive agent model in egocentric worlds,\" arXiv preprint arXiv:2502.05857, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>P. Dhariwal and A. Nichol, \"Diffusion models beat gans on image synthesis,\" Advances in neural information processing systems, vol. 34, pp. 8780--8794, 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>F. Mentzer, D. Minnen, E. Agustsson, and M. Tschannen, \"Finite scalar quantization: VQ-VAE made simple,\" in The twelfth international conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, \"Video diffusion models,\" Advances in neural information processing systems, vol. 35, pp. 8633--8646, 2022.\u00a0\u21a9</p> </li> <li> <p>W. Zhang et al., \"DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge,\" arXiv preprint arXiv:2507.04447, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, \"Depth anything: Unleashing the power of large-scale unlabeled data,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 10371--10381.\u00a0\u21a9</p> </li> <li> <p>Y. Liao et al., \"Genie envisioner: A unified world foundation platform for robotic manipulation,\" arXiv preprint arXiv:2508.05635, 2025.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>S. Nair, E. Mitchell, K. Chen, S. Savarese, and C. Finn, \"Learning language-conditioned robot behavior from offline data and crowd-sourced annotation,\" in Conference on robot learning, PMLR, 2022, pp. 1303--1315.\u00a0\u21a9</p> </li> <li> <p>D. Ha and J. Schmidhuber, \"World models,\" arXiv preprint arXiv:1803.10122, 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner et al., \"Learning latent dynamics for planning from pixels,\" in International conference on machine learning, 2019, pp. 2555--2565.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, \"Mastering atari with discrete world models,\" in International conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, \"Mastering diverse domains through world models,\" arXiv preprint arXiv:2301.04104, 2023.\u00a0\u21a9\u21a9</p> </li> <li> <p>C. Gao, H. Zhang, Z. Xu, C. Zhehao, and L. Shao, \"FLIP: Flow-centric generative planning as general-purpose manipulation world model,\" in The thirteenth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>Y. Du et al., \"Video language planning,\" arXiv preprint arXiv:2310.10625, 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>L. Yang et al., \"RoboEnvision: A long-horizon video generation model for multi-task robot manipulation,\" arXiv preprint arXiv:2506.22007, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Z. Chen, J. Huo, Y. Chen, and Y. Gao, \"Robohorizon: An llm-assisted multi-view world model for long-horizon robotic manipulation,\" arXiv preprint arXiv:2501.06605, 2025.\u00a0\u21a9</p> </li> <li> <p>C. Gumbsch, N. Sajid, G. Martius, and M. V. Butz, \"Learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>D. Driess et al., \"PaLM-e: An embodied multimodal language model,\" in Proceedings of the 40th international conference on machine learning, 2023, pp. 8469--8488.\u00a0\u21a9\u21a9</p> </li> <li> <p>C.-L. Cheang et al., \"Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation,\" arXiv preprint arXiv:2410.06158, 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>H. Wu et al., \"Unleashing large-scale video generative pre-training for visual robot manipulation,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>M. Assran et al., \"V-jepa 2: Self-supervised video models enable understanding, prediction and planning,\" arXiv preprint arXiv:2506.09985, 2025.\u00a0\u21a9</p> </li> <li> <p>A. Bardes et al., \"Revisiting feature prediction for learning visual representations from video,\" arXiv preprint arXiv:2404.08471, 2024.\u00a0\u21a9</p> </li> <li> <p>C. Cheang et al., \"Gr-3 technical report,\" arXiv preprint arXiv:2507.15493, 2025.\u00a0\u21a9</p> </li> <li> <p>K. Black et al., \"\\(\\pi\\_{0.5}\\): A vision-language-action model with open-world generalization,\" arXiv preprint arXiv:2504.16054, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, \"Dream to manipulate: Compositional world models empowering robot imitation learning with imagination,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Ho, A. Jain, and P. Abbeel, \"Denoising diffusion probabilistic models,\" Advances in neural information processing systems, vol. 33, pp. 6840--6851, 2020.\u00a0\u21a9</p> </li> <li> <p>A. Kirillov et al., \"Segment anything,\" in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 4015--4026.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Y. Wang, R. Yu, S. Wan, L. Gan, and D.-C. Zhan, \"Founder: Grounding foundation models in world models for open-ended embodied decision making,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>P. Mazzaglia, T. Verbelen, B. Dhoedt, A. Courville, and S. Rajeswar, \"GenRL: Multimodal-foundation world models for generalization in embodied agents,\" Advances in neural information processing systems, vol. 37, pp. 27529--27555, 2024.\u00a0\u21a9</p> </li> <li> <p>N. Kitaev, S. Cao, and D. Klein, \"Multilingual constituency parsing with self-attention and pre-training,\" in Proceedings of the 57th annual meeting of the association for computational linguistics, 2019, pp. 3499--3505.\u00a0\u21a9</p> </li> <li> <p>J.-C. Pang et al., \"Learning view-invariant world models for visual robotic manipulation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>F. Martinez-Lopez, T. Li, Y. Lu, and J. Chen, \"In-context reinforcement learning via communicative world models,\" arXiv preprint arXiv:2508.06659, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Wu, H. Ma, C. Deng, and M. Long, \"Pre-training contextualized world models with in-the-wild videos for reinforcement learning,\" Advances in Neural Information Processing Systems, vol. 36, pp. 39719--39743, 2023.\u00a0\u21a9</p> </li> <li> <p>R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, \"Planning to explore via self-supervised world models,\" in International conference on machine learning, 2020, pp. 8583--8592.\u00a0\u21a9</p> </li> <li> <p>B. Kang et al., \"How far is video generation from world model: A physical law perspective,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>X. Yang et al., \"VLIPP: Towards physically plausible video generation with vision and language informed physical prior,\" arXiv preprint arXiv:2503.23368, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Peper, Z. Mao, Y. Geng, S. Pan, and I. Ruchkin, \"Four principles for physically interpretable world models,\" arXiv preprint arXiv:2503.02143, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. LeCun, \"A path towards autonomous machine intelligence,\" Open Review, vol. 62, no. 1, pp. 1--62, 2022.\u00a0\u21a9</p> </li> <li> <p>S. Huang et al., \"Enerverse: Envisioning embodied future space for robotics manipulation,\" arXiv preprint arXiv:2501.01895, 2025.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>S. Zhou et al., \"Learning 3D persistent embodied world models,\" arXiv preprint arXiv:2505.05495, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Guo, X. Ma, Y. Wang, M. Yang, H. Liu, and Q. Li, \"FlowDreamer: A RGB-d world model with flow-based motion representations for robot manipulation,\" arXiv preprint arXiv:2505.10075, 2025.\u00a0\u21a9</p> </li> <li> <p>R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \"High-resolution image synthesis with latent diffusion models,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10684--10695.\u00a0\u21a9</p> </li> <li> <p>T. Sou\u010dek, D. Damen, M. Wray, I. Laptev, and J. Sivic, \"Genhowto: Learning to generate actions and state transformations from instructional videos,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 6561--6571.\u00a0\u21a9</p> </li> <li> <p>Y. Wen et al., \"Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation,\" Advances in Neural Information Processing Systems, vol. 37, pp. 41051--41075, 2024.\u00a0\u21a9</p> </li> <li> <p>H. Zhang et al., \"COMBO: Compositional world models for embodied multi-agent cooperation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>M. Oquab et al., \"DINOv2: Learning robust visual features without supervision,\" Transactions on Machine Learning Research Journal, pp. 1--31, 2024.\u00a0\u21a9</p> </li> <li> <p>Y. Angen, \"GigaBrain-0: A world model-powered vision-language-action model,\" arXiv:2510.19430, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>Q. Zhao et al., \"Cot-vla: Visual chain-of-thought reasoning for vision-language-action models,\" in Proceedings of the computer vision and pattern recognition conference, 2025, pp. 1702--1713.\u00a0\u21a9</p> </li> <li> <p>J. Cen et al., \"WorldVLA: Towards autoregressive action world model,\" arXiv preprint arXiv:2506.21539, 2025.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"06-core-components/","title":"VI Core Components & Capabilities","text":""},{"location":"06-core-components/#vi-towards-defining-core-components-and-capabilities-of-world-models","title":"VI Towards Defining Core Components and Capabilities of World Models","text":"<p>\u2003\u2003From our survey of current approaches, we summarize some potential key components and capabilities that a world model should possess. Future research may identify additional dimensions necessary for comprehensive world modeling.  </p> <p> 1) Comprehensive Multimodal Perception.World models should be capable of perceiving and integrating information across all available modalities, such as vision, language, action, touch, force, and proprioception, along with the spatial and temporal structures. By jointly modeling these modalities and dimensions, they can construct a unified and dynamic understanding of the environment that facilitate decision-making and support robot training. </p> <p> 2) Interactivity. World models should engage dynamically with their environments, not merely by passively observing or predicting changes, but by modeling how actions influence future states. Such action-conditioned dynamics enable agents to simulate interactions, evaluate potential outcomes, and plan behaviors grounded in causal understanding of the world. </p> <p> 3) Imagination.Imagination enables world models to simulate and evaluate possible futures, allowing agents to learn, plan, and reason without external interaction. </p> <p> 4) Long-horizon Reasoning. It enables world models to anticipate distant consequences of actions, plan multi-step behaviors, and optimize long-term outcomes rather than short-term rewards. </p> <p> 5) Spatiotemporal Reasoning.World models should reason about spatial and temporal relationships among entities to understand and predict dynamic changes in the environment.  </p> <p> 6) Counterfactual Reasoning.This enables world models to imagine alternative futures under different actions, allowing agents to evaluate possible outcomes and select the most effective course of action. </p> <p> 7) Abstract Reasoning.The world is immensely complex, and world models cannot capture every detail. Therefore, they must extract and represent the underlying principles and basic mechanisms that govern the world\u2019s dynamics. </p> <p> 8) High-fidelity Prediction. World models should generate accurate and detailed predictions of future states or observations, maintaining spatial, temporal, and physical consistency to ensure reliable simulation and planning.  </p> <p> 9) Physics Awareness. World models should maintain consistency with physical principles, enabling them to generate dynamically plausible predictions that support safe and reliable robotic interaction. </p> <p> 10) Generalization Ability. To operate effectively in complex real-world settings, world models must generalize beyond their training distributions, adapting to new tasks, objects, and domains. </p> <p> 11) Causality. World models should understand relationship between actions (causes) and their effects (outcomes) in the world. This causal understanding enables agents to predict how interventions will change future states, distinguish correlation from true influence, and generalize their behavior to unseen situations by reasoning about cause\u2013effect mechanisms rather than memorized patterns. </p> <p> 12) Memory.It enables world models to store and recall past experiences, ensuring temporal consistency and coherent predictions. In addition, world models should be able to access and integrate external information, thereby supporting richer reasoning, long-term planning, and adaptability\u2014analogous to the role of retrieval-augmented generation (RAG) in language models. </p> <p> 13) Collaboration Ability. World models should support both inter-agent and intra-agent coordination by reasoning about the behaviors, goals, and intentions of others and managing cooperation among multiple effectors (e.g., multi-arm systems).</p>"},{"location":"07-dataset/","title":"VII Dataset","text":""},{"location":"07-dataset/#vii-dataset","title":"VII Dataset","text":"<p>\u2003\u2003There are abundant datasets that facilitate robot learning, including general robotic manipulation datasets <sup>1</sup> <sup>2</sup> <sup>3</sup> <sup>4</sup> <sup>5</sup> <sup>6</sup>, Dual-arm robotic manipulation datasets <sup>7</sup>,  human manipulation datasets <sup>8</sup> <sup>9</sup> <sup>10</sup> <sup>11</sup> <sup>12</sup> <sup>13</sup>, combinations of robotic &amp; human manipulation <sup>13</sup>, egocentric datasets <sup>14</sup> <sup>15</sup> <sup>16</sup>, 3D &amp; 4D datasets <sup>17</sup> <sup>13</sup> <sup>4</sup> <sup>18</sup> <sup>14</sup> <sup>15</sup> <sup>19</sup> <sup>20</sup> <sup>21</sup> <sup>22</sup> <sup>23</sup>,  multi-view datasets <sup>21</sup> <sup>15</sup> <sup>6</sup> and panoramic-view datasets <sup>20</sup>. A detailed information of them can be found in Fig.\u2161.</p>"},{"location":"07-dataset/#table-ii","title":"Table \u2161","text":"<p>A SUMMARY OF REPRESENTATIVE DATASETS. <code>H</code>: HOUR, <code>Manip</code>: MANIPULATION, <code>Env.</code>: ENVIRONMENTS, <code>Traj.</code>: TRAJECTORIES. </p> <p></p> <p>\u2003\u2003Recent efforts in world models for robotic manipulation often have leveraged a large and diverse datasets, e.g., a combinations of different dataset, to be capable of generalizing across tasks and environments. For example, Yang et al. <sup>24</sup> constructed a large-scale natural dataset combining simulated executions and renderings <sup>19</sup>, real robot data <sup>3</sup>, human activity videos <sup>14</sup>, 3D panorama scans <sup>20</sup>, and internet text-image data LAION-400M <sup>25</sup>. Bruce et al. <sup>26</sup> combine the RT-1 dataset <sup>27</sup> with real robot grasping data <sup>28</sup>. Wu et al. <sup>29</sup> train the world model based on the combination of the Open X-Embodiment (OXE) dataset <sup>22</sup> and the Something-Something v2 (SSv2) trajectory dataset <sup>9</sup>. Bruce et al. <sup>12</sup> employ a pretraining and fine-tuning strategy. In the pretraining stage, a combination of human demonstration datasets such as Howto100M <sup>8</sup>, Ego4D <sup>14</sup>, Something-Something V2 <sup>9</sup>, EPIC-KITCHENS <sup>16</sup>, Kinetics-700 <sup>11</sup>, and robot datasets <sup>27</sup>. Fine-tuning data includes 105 table-top tasks via teleoperation covering eight skills (e.g., pick, place). Data augmentation are performed to add new objects or change backgrounds by means of a diffusion model <sup>30</sup> and the Segment Anything Model (SAM) <sup>31</sup>, as well as a video generation model <sup>32</sup> to sytheize new videos. Du et al. <sup>33</sup> curated an internet-scale pretraining dataset consisting of 14 million video-text pairs, 60 million image-text pairs <sup>34</sup>, LAION-400M <sup>25</sup>, and a smaller real-world robotic dataset <sup>3</sup>. Huang et al. <sup>35</sup> constructed multi-anchor view video datasets using public sources including RT-1 <sup>27</sup>, Taco-Play <sup>36</sup>, ManiSkill <sup>23</sup>, BridgeData V2 <sup>2</sup>, LanguageTable <sup>37</sup>, and RoboTurk <sup>4</sup>, augmented with Isaac Sim simulations <sup>38</sup>.  <sup>17</sup> construct a 4D embodied video dataset based on previous datasets <sup>39</sup> <sup>27</sup> <sup>2</sup> <sup>9</sup> by measuring depth and normal information.</p>"},{"location":"07-dataset/#references","title":"References","text":"<ol> <li> <p>A. Khazatsky et al., \"Droid: A large-scale in-the-wild robot manipulation dataset,\" arXiv preprint arXiv:2403.12945, 2024.\u00a0\u21a9</p> </li> <li> <p>H. R. Walke et al., \"Bridgedata v2: A dataset for robot learning at scale,\" in Conference on robot learning, 2023, pp. 1723--1736.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>F. Ebert et al., \"Bridge data: Boosting generalization of robotic skills with cross-domain datasets,\" arXiv preprint arXiv:2109.13396, 2021.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>A. Mandlekar et al., \"Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity,\" in 2019 IEEE/RSJ international conference on intelligent robots and systems (IROS), IEEE, 2019, pp. 1048--1055.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>S. Deng et al., \"Graspvla: A grasping foundation model pre-trained on billion-scale synthetic action data,\" arXiv preprint arXiv:2505.03233, 2025.\u00a0\u21a9</p> </li> <li> <p>Z. Chen et al., \"RH20T-p: A primitive-level robotic manipulation dataset towards composable generalization agents in real-world scenarios,\" in NeurIPS 2024 workshop on open-world agents, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>Q. Bu et al., \"Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems,\" arXiv preprint arXiv:2503.06669, 2025.\u00a0\u21a9</p> </li> <li> <p>A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, \"Howto100m: Learning a text-video embedding by watching hundred million narrated video clips,\" in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 2630--2640.\u00a0\u21a9\u21a9</p> </li> <li> <p>R. Goyal et al., \"The\\\" something something\\\" video database for learning and evaluating visual common sense,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 5842--5850.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>T. Sou\u010dek, D. Damen, M. Wray, I. Laptev, and J. Sivic, \"Genhowto: Learning to generate actions and state transformations from instructional videos,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 6561--6571.\u00a0\u21a9</p> </li> <li> <p>J. Carreira, E. Noland, C. Hillier, and A. Zisserman, \"A short note on the kinetics-700 human action dataset,\" arXiv preprint arXiv:1907.06987, 2019.\u00a0\u21a9\u21a9</p> </li> <li> <p>C.-L. Cheang et al., \"Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation,\" arXiv preprint arXiv:2410.06158, 2024.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Zhen et al., \"3D-VLA: A 3D vision-language-action generative world model,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 61229--61245.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>K. Grauman et al., \"Ego4d: Around the world in 3,000 hours of egocentric video,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 18995--19012.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>K. Grauman et al., \"Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 19383--19400.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Damen et al., \"Scaling egocentric vision: The epic-kitchens dataset,\" in Proceedings of the european conference on computer vision (ECCV), 2018, pp. 720--736.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Zhen et al., \"TesserAct: Learning 4D embodied world models,\" arXiv preprint arXiv:2504.20995, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Zhi et al., \"3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model,\" arXiv preprint arXiv:2506.06199, 2025.\u00a0\u21a9</p> </li> <li> <p>S. K. Ramakrishnan et al., \"Habitat-matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI,\" in Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2), 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. Chang et al., \"Matterport3D: Learning from RGB-d data in indoor environments,\" in 2017 international conference on 3D vision (3DV), IEEE Computer Society, 2017, pp. 667--676.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, \"Multiply: A multisensory object-centric embodied large language model in 3d world,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 26406--26416.\u00a0\u21a9\u21a9</p> </li> <li> <p>A. O'Neill et al., \"Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0,\" in 2024 IEEE international conference on robotics and automation (ICRA), IEEE, 2024, pp. 6892--6903.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Gu et al., \"ManiSkill2: A unified benchmark for generalizable manipulation skills,\" in The eleventh international conference on learning representations, 2023.\u00a0\u21a9\u21a9</p> </li> <li> <p>M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, \"Learning interactive real-world simulators,\" arXiv preprint arXiv:2310.06114, vol. 1, no. 2, p. 6, 2023.\u00a0\u21a9</p> </li> <li> <p>C. Schuhmann et al., \"Laion-400m: Open dataset of clip-filtered 400 million image-text pairs,\" arXiv preprint arXiv:2111.02114, 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Bruce et al., \"Genie: Generative interactive environments,\" in Proceedings of the 41st international conference on machine learning, 2024, pp. 4603--4623.\u00a0\u21a9</p> </li> <li> <p>A. Brohan et al., \"RT-1: Robotics transformer for real-world control at scale,\" Robotics: Science and Systems, 2023.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>D. Kalashnikov et al., \"Scalable deep reinforcement learning for vision-based robotic manipulation,\" in Conference on robot learning, 2018, pp. 651--673.\u00a0\u21a9</p> </li> <li> <p>J. Wu et al., \"Ivideogpt: Interactive videogpts are scalable world models,\" Advances in Neural Information Processing Systems, vol. 37, pp. 68082--68119, 2024.\u00a0\u21a9</p> </li> <li> <p>J. Ho, A. Jain, and P. Abbeel, \"Denoising diffusion probabilistic models,\" Advances in neural information processing systems, vol. 33, pp. 6840--6851, 2020.\u00a0\u21a9</p> </li> <li> <p>A. Kirillov et al., \"Segment anything,\" in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 4015--4026.\u00a0\u21a9</p> </li> <li> <p>X. Ma et al., \"Latte: Latent diffusion transformer for video generation,\" Transactions on Machine Learning Research, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Du et al., \"Learning universal policies via text-guided video generation,\" Advances in neural information processing systems, vol. 36, pp. 9156--9172, 2023.\u00a0\u21a9</p> </li> <li> <p>J. Ho et al., \"Imagen video: High definition video generation with diffusion models,\" arXiv preprint arXiv:2210.02303, 2022.\u00a0\u21a9</p> </li> <li> <p>S. Huang et al., \"Enerverse: Envisioning embodied future space for robotics manipulation,\" arXiv preprint arXiv:2501.01895, 2025.\u00a0\u21a9</p> </li> <li> <p>E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Burgard, \"Latent plans for task-agnostic offline reinforcement learning,\" in Conference on robot learning, 2023, pp. 1838--1849.\u00a0\u21a9</p> </li> <li> <p>C. Lynch et al., \"Interactive language: Talking to robots in real time,\" IEEE Robotics and Automation Letters, 2023.\u00a0\u21a9</p> </li> <li> <p>M. Mittal et al., \"Orbit: A unified simulation framework for interactive robot learning environments,\" IEEE Robotics and Automation Letters, vol. 8, no. 6, pp. 3740--3747, 2023.\u00a0\u21a9</p> </li> <li> <p>S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, \"Rlbench: The robot learning benchmark\\ &amp; learning environment,\" IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3019--3026, 2020.\u00a0\u21a9</p> </li> </ol>"},{"location":"08-conclusion/","title":"VIII Conclusion & Future directions","text":""},{"location":"08-conclusion/#viii-conclusion-and-future-directions","title":"VIII Conclusion and Future directions","text":""},{"location":"08-conclusion/#a-conclusion","title":"A. Conclusion","text":"<p>\u2003\u2003This survey provides a comprehensive evaluation of current approaches to world modeling, examining their relevance for robotic manipulation, underlying architectures, functionalities, key challenges, and proposed solutions. By synthesizing these findings, we offer insights into the nature of fully realized world models and outline the efforts required to advance the field. Our goal is to provide readers with a solid foundation and guide future research directions in world modeling.</p>"},{"location":"08-conclusion/#b-future-research-directions","title":"B. Future research directions","text":"<p>\u2003\u2003From our survey of current approaches and analysis of the core components and capabilities of world models, it is evident that present models fall short of accurately representing real-world phenomena. The limitations and the set of expected capabilities define promising directions for future research. To provide additional context, we also discuss several significant research directions.</p> <p> More Diverse Modalities. The real world contains diverse forms of information, and no single sensory modality can capture its full complexity. This motivates world models capable of perceiving and integrating multiple modalities, including vision, language, action, touch, force, and proprioception, along with their interactions. Early progress has been made in this direction. For example, Hong et al. <sup>1</sup> introduce the Multisensory-Universe dataset, which features interactive scenes enriched with tactile, audio, and temperature signals, generated with the assistance of ChatGPT <sup>2</sup>.</p> <p> Hierarchical World Models. Hierarchical systems play a critical role in building effective world models, as they allow agents to reason across multiple layers of abstraction. However, designing hierarchical models is inherently challenging: separating low-level and high-level dynamics is difficult, and coordinating interactions across layers adds further complexity. While existing studies primarily emphasize model design <sup>3</sup> <sup>4</sup> <sup>5</sup> <sup>6</sup>, their validation in complex real-world environments remains limited.</p> <p> Causality. Causality is a fundamental principle for understanding and modeling the world, describing how events or factors influence outcomes and enabling reasoning about future consequences. Causality is the key to world model as it allows agents to interact with the world, which is inline with the human cognition. Richens et al. <sup>7</sup> indicate that learning a causal model is the key to ensure the generalization ability to new domains. Wang et al. <sup>8</sup> <sup>9</sup> learn a causal dynamics model by removing unnecessary dependencies for tasks, which however are constrained to specific tasks. Gupta et al. <sup>10</sup> argue that conventional theory-driven approaches to causal modeling, such as those in <sup>11</sup>, are insufficient for world models that aim for generalizable understanding. These methods rely on predefined variables and case-specific theoretical properties. In the real world, sensory inputs are complex, often unstructured, and key theoretical properties,such as identifiability, may not hold.</p> <p> Resource-Constrained Deployment. Current world models, particularly those based on video generation, are computationally intensive and contain hundreds of millions of parameters, which limits their feasibility for real-world robotic deployment and on-device inference. To enable practical applications, designing lightweight and efficient world models has become increasingly important. Quantization and model compression techniques offer promising directions for reducing memory and computational costs, and have been extensively explored in related domains <sup>12</sup> <sup>13</sup> <sup>14</sup> <sup>15</sup>, providing both direct solutions and inspiration for future lightweight world model architectures.</p> <p> Fairness and Security. As world models become integral to embodied agents and decision-making systems, ensuring their ethical alignment and fairness is critical. Unlike conventional vision or language models, world models directly influence how autonomous agents perceive, reason, and act within real environments, which amplifies the consequences of biased or unsafe representations. To handle this, emerging research explores bias auditing, fairness-aware training, and safety-constrained learning objectives to prevent harmful behaviors and unintended policy generalization.</p> <p>\u2003\u2003Furthermore, deep models are known to be vulnerable to adversarial attacks, which can compromise performance by introducing imperceptible perturbations to inputs <sup>16</sup> <sup>17</sup>, modifying model parameters <sup>18</sup> <sup>19</sup>, or even exploiting hardware-level weaknesses <sup>20</sup> <sup>21</sup>. These vulnerabilities raise serious concerns regarding the security and reliability of world models, especially when deployed in safety-critical domains. To date, systematic studies on the robustness and security of world models remain limited, underscoring an urgent need for dedicated research into adversarial resilience, trustworthy deployment, and secure model adaptation.</p> <p> Evaluation Protocols. Current evaluation practices for world models are fragmented and only loosely aligned with their intended capabilities, often relying on task-specific or proxy metrics and partial human validation <sup>22</sup>. There is a pressing need for standardized benchmarks and unified evaluation frameworks that can comprehensively assess world model competence across multiple dimensions, including visual fidelity, policy success, causal consistency, physical plausibility, generalization, and long-horizon reasoning.</p> <p> Beyond Human Intelligence. Insights from human cognition have profoundly influenced the design of robotic and world modeling systems. However, the completeness of the world extends beyond human cognition, which is bounded by partial observation, finite memory, limited attention, and inherent heuristic biases. World models are therefore expected to transcend human cognitive bounds, providing a deeper and more systematic understanding of complex environments.</p> <p>Structured and Abstract World Understanding.</p> <p> \u201cTruth is ever to be found in simplicity, and not in the multiplicity and confusion of things.\u201d \u2003\u2003\u2003\u2014\u2014 Isaac Newton</p> <p>\u2003\u2003The ultimate goal of world models is to capture underlying regularities and structured abstractions, rather than memorizing every detail of a complex environment. This focus on essential structure is key for enabling world models to generalize across diverse environments.</p>"},{"location":"08-conclusion/#references","title":"References","text":"<ol> <li> <p>Y. Hong, Z. Zheng, P. Chen, Y. Wang, J. Li, and C. Gan, \"Multiply: A multisensory object-centric embodied large language model in 3d world,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 26406--26416.\u00a0\u21a9</p> </li> <li> <p>J. Achiam et al., \"Gpt-4 technical report,\" arXiv preprint arXiv:2303.08774, 2023.\u00a0\u21a9</p> </li> <li> <p>C. Gumbsch, N. Sajid, G. Martius, and M. V. Butz, \"Learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>Y. LeCun, \"A path towards autonomous machine intelligence,\" Open Review, vol. 62, no. 1, pp. 1--62, 2022.\u00a0\u21a9</p> </li> <li> <p>L. Wang, R. Shelim, W. Saad, and N. Ramakrishnan, \"DMWM: Dual-mind world model with long-term imagination,\" arXiv preprint arXiv:2502.07591, 2025.\u00a0\u21a9</p> </li> <li> <p>E. Xing, M. Deng, J. Hou, and Z. Hu, \"Critiques of world models,\" arXiv preprint arXiv:2507.05169, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Richens and T. Everitt, \"Robust agents learn causal world models,\" arXiv preprint arXiv:2402.10877, 2024.\u00a0\u21a9</p> </li> <li> <p>Z. Wang, X. Xiao, Z. Xu, Y. Zhu, and P. Stone, \"Causal dynamics learning for task-independent state abstraction,\" in International conference on machine learning, 2022, pp. 23151--23180.\u00a0\u21a9</p> </li> <li> <p>M. Tomar, A. Zhang, R. Calandra, M. E. Taylor, and J. Pineau, \"Model-invariant state abstractions for model-based reinforcement learning,\" arXiv preprint arXiv:2102.09850, 2021.\u00a0\u21a9</p> </li> <li> <p>T. Gupta et al., \"The essential role of causality in foundation world models for embodied AI,\" arXiv preprint arXiv:2402.06665, 2024.\u00a0\u21a9</p> </li> <li> <p>E. A. Stuart, \"Matching methods for causal inference: A review and a look forward,\" Statistical science: a review journal of the Institute of Mathematical Statistics, vol. 25, no. 1, p. 1, 2010.\u00a0\u21a9</p> </li> <li> <p>A. Polino, R. Pascanu, and D. Alistarh, \"Model compression via distillation and quantization,\" in International conference on learning representations, 2018.\u00a0\u21a9</p> </li> <li> <p>A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, \"A survey of quantization methods for efficient neural network inference,\" in Low-power computer vision, Chapman; Hall/CRC, 2022, pp. 291--326.\u00a0\u21a9</p> </li> <li> <p>Y. Shang, Z. Yuan, B. Xie, B. Wu, and Y. Yan, \"Post-training quantization on diffusion models,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 1972--1981.\u00a0\u21a9</p> </li> <li> <p>Y. Li, T. Chen, P.-F. Zhang, and H. Yin, \"Lightweight self-attentive sequential recommendation,\" in Proceedings of the 30th ACM international conference on information\\ &amp; knowledge management, 2021, pp. 967--977.\u00a0\u21a9</p> </li> <li> <p>C. Szegedy et al., \"Intriguing properties of neural networks,\" arXiv preprint arXiv:1312.6199, 2013.\u00a0\u21a9</p> </li> <li> <p>P.-F. Zhang, Z. Huang, and G. Bai, \"Universal adversarial perturbations for vision-language pre-trained models,\" in Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval, 2024, pp. 862--871.\u00a0\u21a9</p> </li> <li> <p>J. Ren, Y. Zhou, J. Jin, L. Lyu, and D. Yan, \"Dimension-independent certified neural network watermarks via mollifier smoothing,\" in ICML, PMLR, 2023, pp. 28976--29008.\u00a0\u21a9</p> </li> <li> <p>N. Park and S. Kim, \"Blurs behave like ensembles: Spatial smoothings to improve accuracy, uncertainty, and robustness,\" in ICML, 2022, pp. 17390--17419.\u00a0\u21a9</p> </li> <li> <p>L. Cojocar et al., \"Are we susceptible to rowhammer? An end-to-end methodology for cloud providers,\" in SP, 2020, pp. 712--728.\u00a0\u21a9</p> </li> <li> <p>P. Jattke, M. Wipfli, F. Solt, M. Marazzi, M. B\u00f6lcskei, and K. Razavi, \"\\(\\{\\)ZenHammer\\(\\}\\): Rowhammer attacks on \\(\\{\\)AMD\\(\\}\\) zen-based platforms,\" in USENIX security, 2024, pp. 1615--1633.\u00a0\u21a9</p> </li> <li> <p>Y. Liao et al., \"Genie envisioner: A unified world foundation platform for robotic manipulation,\" arXiv preprint arXiv:2508.05635, 2025.\u00a0\u21a9</p> </li> </ol>"},{"location":"99-references/","title":"References","text":""},{"location":"99-references/#references","title":"References","text":""},{"location":"99-references/#decision-support","title":"Decision Support","text":"<p> 1) Implicit World Models for Action Prediction and Planning</p> <p>\u2003\u2003Do as i can, not as i say: Grounding language in robotic affordances [paper] [code]</p> <p>\u2003\u2003Pandora: Towards general world model with natural language actions and video states [paper] [code]</p> <p>\u2003\u2003PaLM-e: An embodied multimodal language model [paper]</p> <p>\u2003\u2003DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge [paper] [code]</p> <p>\u2003\u2003An embodied generalist agent in 3D world [paper] [code]</p> <p>\u2003\u2003Multiply: A multisensory object-centric embodied large language model in 3d world [paper] [code]</p> <p>\u2003\u2003Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning [paper]</p> <p>\u2003\u2003Vlmpc: Vision-language model predictive control for robotic manipulation [paper] [code]</p> <p>\u2003\u2003Rt-2: Vision-language-action models transfer web knowledge to robotic control [paper] [code]</p> <p>\u2003\u20033D-VLA: A 3D vision-language-action generative world model [paper] [code]</p> <p>\u2003\u20033d-llm: Injecting the 3d world into large language models [paper] [code]</p> <p>\u2003\u2003Thinking, fast and slow [paper]</p> <p>\u2003\u2003Gr00t n1: An open foundation model for generalist humanoid robots [paper] [code]</p> <p>\u2003\u2003Vision-language-action model with open-world embodied reasoning from pretrained knowledge [paper]</p> <p>\u2003\u2003OpenVLA: An open-source vision-language-action model [paper] [code]</p> <p>\u2003\u2003EgoAgent: A joint predictive agent model in egocentric worlds [paper] [code]</p> <p>\u2003\u2003Internlm: A multilingual language model with progressively enhanced capabilities [paper]</p> <p>\u2003\u2003GenRL: Multimodal-foundation world models for generalization in embodied agents [paper] [code]</p> <p>\u2003\u2003Founder: Grounding foundation models in world models for open-ended embodied decision making [paper]</p> <p>\u2003\u2003UP-VLA: A unified understanding and prediction model for embodied agent [paper] [code]</p> <p>\u2003\u2003Cot-vla: Visual chain-of-thought reasoning for vision-language-action models [paper]</p> <p> 2) Latent Dynamics Modeling for Action Prediction and Planning</p> <p>\u2003\u2003Learning latent dynamics for planning from pixels [paper] [code]</p> <p>\u2003\u2003Dream to control: Learning behaviors by latent imagination [paper] [code]</p> <p>\u2003\u2003Mastering atari with discrete world models [paper] [code]</p> <p>\u2003\u2003Daydreamer: World models for physical robot learning [paper] [code]</p> <p>\u2003\u2003Mastering diverse domains through world models [paper] [code]</p> <p>\u2003\u2003Mastering diverse control tasks through world models [paper] [code]</p> <p>\u2003\u2003Planning to explore via self-supervised world models [paper] [code]</p> <p>\u2003\u2003Learning hierarchical world models with adaptive temporal abstractions from discrete latent dynamics [paper] [code]</p> <p>\u2003\u2003FOCUS: Object-centric world models for robotic manipulation [paper] [code]</p> <p>\u2003\u2003EgoAgent: A joint predictive agent model in egocentric worlds [paper] [code]</p> <p>\u2003\u2003V-jepa 2: Self-supervised video models enable understanding, prediction and planning [paper] [code]</p> <p>\u2003\u2003Temporal difference learning for model predictive control [paper]</p> <p>\u2003\u2003TD-MPC2: Scalable, robust world models for continuous control [paper] [code]</p> <p> 3) Implicit World Models for Action Prediction and Planning</p> <p>\u2003\u2003Unsupervised learning for physical interaction through video prediction [paper]</p> <p>\u2003\u2003Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning [paper] [code]</p> <p>\u2003\u2003Visual foresight: Model-based deep reinforcement learning for vision-based robotic control [paper] [code]</p> <p>\u2003\u2003Closed-loop visuomotor control with generative expectation for robotic manipulation [paper] [code]</p> <p>\u2003\u2003Deep visual foresight for planning robotic motion [paper]</p> <p>\u2003\u2003Learning language-conditioned robot behavior from offline data and crowd-sourced annotation [paper] [code]</p> <p>\u2003\u2003UP-VLA: A unified understanding and prediction model for embodied agent [paper] [code]</p> <p>\u2003\u2003RoboDreamer: Learning compositional world models for robot imagination [paper] [code]</p> <p>\u2003\u2003DreamVLA: A vision-language-action model dreamed with comprehensive world knowledge [paper] [code]</p> <p>\u2003\u2003DINOv2: Learning robust visual features without supervision [paper] [code]</p> <p>\u2003\u2003Segment anything [paper] [code]</p> <p>\u2003\u2003This&amp;that: Language-gesture controlled video generation for robot planning [paper] [code]</p> <p>\u2003\u2003Learning universal policies via text-guided video generation [paper]</p> <p>\u2003\u2003Cot-vla: Visual chain-of-thought reasoning for vision-language-action models [paper]</p> <p>\u2003\u2003Video generators are robot policies [paper] [code]</p> <p>\u2003\u2003COMBO: Compositional world models for embodied multi-agent cooperation [paper] [code]</p> <p>\u2003\u20033DFlowAction: Learning cross-embodiment manipulation from 3D flow world model [paper] [code]</p> <p>\u2003\u2003Cotracker: It is better to track together [paper] [code]</p> <p>\u2003\u2003Cotracker3: Simpler and better point tracking by pseudo-labelling real videos [paper] [code]</p> <p>\u2003\u2003Cosmos world foundation model platform for physical ai [paper] [code]</p> <p>\u2003\u2003Predictive inverse dynamics models are scalable learners for robotic manipulation [paper] [code]</p> <p>\u2003\u2003Prediction with action: Visual policy learning via joint denoising process [paper] [code]</p> <p>\u2003\u2003FlowDreamer: A RGB-d world model with flow-based motion representations for robot manipulation [paper]</p>"},{"location":"99-references/#training-facilitation","title":"Training Facilitation","text":"<p> 1) Data Engine</p> <p>\u2003\u2003Rt-2: Vision-language-action models transfer web knowledge to robotic control [paper] [code]</p> <p>\u2003\u2003\u03c0_0: A vision-language-action flow model for general robot control [paper] [code]</p> <p>\u2003\u2003Gemini robotics: Bringing ai into the physical world [paper]</p> <p>\u2003\u2003Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems [paper] [code]</p> <p>\u2003\u2003Gr00t n1: An open foundation model for generalist humanoid robots [paper] [code]</p> <p>\u2003\u2003RDT-1B: A diffusion foundation model for bimanual manipulation [paper] [code]</p> <p>\u2003\u2003DreamGen: Unlocking generalization in robot learning through video world models [paper] [code]</p> <p>\u2003\u2003Learning universal policies via text-guided video generation [paper]</p> <p>\u2003\u2003Ivideogpt: Interactive videogpts are scalable world models [paper] [code]</p> <p>\u2003\u2003Wan: Open and advanced large-scale video generative models [paper] [code]</p> <p>\u2003\u2003Latent action pretraining from videos [paper] [code]</p> <p>\u2003\u2003Video pretraining (vpt): Learning to act by watching unlabeled online videos [paper] [code]</p> <p>\u2003\u2003GWM: Towards scalable gaussian world models for robotic manipulation [paper] [code]</p> <p>\u2003\u2003GigaBrain-0: A world model-powered vision-language-action model [paper]</p> <p>\u2003\u2003RoboTransfer: Geometry-consistent video diffusion for robotic visual policy transfer [paper] [code]</p> <p>\u2003\u2003Cosmos-reason1: From physical common sense to embodied reasoning [paper] [code]</p> <p>\u2003\u2003Dream to manipulate: Compositional world models empowering robot imitation learning with imagination [paper] [code]</p> <p>\u2003\u20033D gaussian splatting for real-time radiance field rendering [paper] [code]</p> <p>\u2003\u2003World-env: Leveraging world model as a virtual environment for VLA post-training [paper] [code]</p> <p> 2) Evaluation</p> <p>\u2003\u2003Mujoco: A physics engine for model-based control [paper] [code]</p> <p>\u2003\u2003Simulation tools for model-based robotics: Comparison of bullet, havok, mujoco, ode and physx [paper]</p> <p>\u2003\u2003Drake: Model-based design and verification for robotics [paper] [code]</p> <p>\u2003\u2003The limits and potentials of deep learning for robotics [paper]</p> <p>\u2003\u2003A study on the challenges of using robotics simulators for testing [paper]</p> <p>\u2003\u2003On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward [paper]</p> <p>\u2003\u2003Challenges of real-world reinforcement learning [paper]</p> <p>\u2003\u2003Sim-to-real transfer in deep reinforcement learning for robotics: A survey [paper]</p> <p>\u2003\u2003CARLA: An open urban driving simulator [paper] [code]</p> <p>\u2003\u2003Robothor: An open simulation-to-real embodied ai platform [paper] [code]</p> <p>\u2003\u2003WorldEval: World model as real-world robot policies evaluator [paper] [code]</p> <p>\u2003\u2003Wan: Open and advanced large-scale video generative models [paper] [code]</p> <p>\u2003\u2003Gemini: A family of highly capable multimodal models [paper]</p> <p>\u2003\u2003Evaluating robot policies in a world model [paper] [code]</p> <p>\u2003\u2003Gpt-4o system card [paper]</p> <p>\u2003\u2003Pre-trained video generative models as world simulators [paper]</p> <p>\u2003\u2003Irasim: Learning interactive real-robot action simulators [paper] [code]</p> <p>\u2003\u2003Genie envisioner: A unified world foundation platform for robotic manipulation [paper] [code]</p> <p>\u2003\u2003Learning real-world action-video dynamics with heterogeneous masked autoregression [paper] [code]</p> <p>\u2003\u2003Video prediction models as rewards for reinforcement learning [paper] [code]</p>"},{"location":"abstract/","title":"Abstract","text":""},{"location":"abstract/#a-step-toward-world-models-a-survey-on-robotic-manipulation","title":"A Step Toward World Models: A Survey on  Robotic Manipulation","text":"<p>Peng-Fei Zhang, Ying Cheng, Xiaofan Sun, Shijie Wang, Fengling Li, Lei Zhu, Heng Tao Shen School of Computer Science and Technology, Tongji University</p>"},{"location":"abstract/#abstract","title":"Abstract","text":"<p>\u2003\u2003Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and support prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, we go beyond prescribing a fixed definition and limiting our scope to methods explicitly labeled as world models. Instead, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a fully realized world model should possess. Building on this analysis, we aim to motivate further development toward generalizable and practical world models for robotics. This is an initial version of the survey. The content will be expanded and refined in future updates. </p> <p>Index Terms\u2014\u2014\u2014World Model; Robotic Manipulation.</p>"},{"location":"abstract/#citation","title":"Citation","text":"<p>If you find this project useful, please consider citing our paper.</p> <pre><code>@article{\n    title={A Step Toward World Models: A Survey on Robotic Manipulation},\n    author={Peng-Fei Zhang, Ying Cheng, Xiaofan Sun, Shijie Wang, Fengling Li, Lei Zhu, Heng Tao Shen},\n    journal={},\n    year={2025}\n}\n</code></pre>"},{"location":"contact/","title":"Contact","text":""},{"location":"out/","title":"Out","text":""},{"location":"out/#a-generalization","title":"A. Generalization","text":"<p>\u2003\u2003Robots are expected to operate robustly in complex and novel environments, interacting with unfamiliar objects and performing tasks beyond their training distribution.  </p> <p> 1) Data Scaling </p> <p>\u2003\u2003An intuitive and effective strategy to enhance generalization is to scale the diversity and volume of training data. For example, Cheang et al. <sup>1</sup> increase the number of pre-training videos from 0.8 million in <sup>2</sup> to 38 million. Assran et al. <sup>3</sup> expand the dataset from 2 million used by <sup>4</sup> to 22 million videos. Wang et al. <sup>5</sup> expand each of the 40 datasets by increasing trajectories from 10 up to \\(10^{6}\\). Cheang et al. <sup>6</sup> train the model with web-scale vision-language data,  human trajectory data and robot trajectory data. Kevin et al. <sup>7</sup> leverage diverse mobile manipulator data, diverse multi-environment non-mobile robot data, cross-embodiment laboratory data, high-level subtask prediction, and multi-modal web data. Cheang et al. <sup>8</sup> <sup>1</sup> investigate data augmentation strategies to enhance generalization. In <sup>8</sup>, object rotation and roto-translation are applied. Cheang et al. <sup>1</sup> generate novel scenes by injecting objects using a diffusion model <sup>9</sup> and/or altering backgrounds with the Segment Anything Model (SAM)  <sup>10</sup>. A video generation model <sup>10</sup> is subsequently employed to synthesize videos that preserve the original robot motions from the inpainted frames. Liao et al. <sup>11</sup> augment the dataset with a diverse set of failure cases, including erroneous executions, incomplete behaviors, and suboptimal control trajectories\u2014collected from both human teleoperation and real-world robotic deployments. One problem of data scaling is that it is unlikely to collect all data for each tasks. At the same time, how to balance different data tasks is also challenging. Moreover, performance gains by scaling data is also limited for consistent performance improvements. </p> <p> 2) Use of Pretrained Models </p> <p>\u2003\u2003Many methods aim to enhance generalization by leveraging the generative capabilities of video models. For example, Zhu et al. <sup>12</sup> combine video generation with geometric-aware learning to improve synthetic-to-real generalization across unseen viewpoints and support multiple downstream tasks. Zhen et al. <sup>13</sup> fine-tune a video generation model on RGB, depth, and normal videos to encode detailed shape, configuration, and temporal dynamics, enabling generalization to unseen scenes, objects, and cross-domain scenarios. The generalization capabilities of large language models, such as video-language models <sup>14</sup> and vision-language models <sup>15</sup>, can be leveraged to enhance world models. By extracting high-level knowledge about the environment, these models facilitate more effective low-level dynamics modeling.</p> <p> 3) Instructions Decomposing </p> <p>\u2003\u2003Another generation issue comes from unseen instructions. To handle this, Zhou et al. <sup>16</sup> enhance the ability to unseen instructions by decomposing each spatial relation phrase into a set of compositional components with the pre-trained parser <sup>17</sup> and the rule-based approach. Detailed information can refer to Section \\ref{IUF}.</p> <p> 4) Invariant Representations </p> <p>\u2003\u2003Generalization can be significantly improved by learning invariant representations to superficial or task-irrelevant changes in the environment. For example, Pang et al. <sup>18</sup> model learns to explicitly decompose visual observations into a view-invariant representation, which is used for the control policy, and a view-dependent representation. This decoupling makes the resulting policy robust to changes in camera viewpoint, a common source of failure in visuomotor control. Similarly, the Martinez et al. <sup>19</sup> framework learns a transferable communicative context between two agents, which enables zero-shot adaptation to entirely unseen sparse-reward environments by decoupling the representation learning from the control problem. Wu et al. <sup>20</sup> disentangle the modeling of context and dynamics by introducing a context encoder, enabling the model to capture shared knowledge for predictions.</p> <p> 5) Task-relevant Information Focused </p> <p>\u2003\u2003Video data often contain irrelevant data to the actions such as background and robot arm, which would limited the generalization ability of the learned world models. To handle this, <sup>21</sup> propose to object-centric world models, which concentrated on object movements via the optical flow predictions that is independent of embodiment. Finn et al. <sup>22</sup> propose to explicitly model and predict motion that are relatively invariant to the object appearance, enabling long-range predictions and generalize to unseen objects.</p> <p> 6) Other Strategies </p> <p>\u2003\u2003Black et al. <sup>23</sup> use a pretrained image-editing model to generate subgoals from language commands and current observations, enabling low-level controllers to act and generalize to novel objects and scenarios. Self-supervised learning without task-specific rewards that can enhancing generalization abilities into different tasks <sup>24</sup>. </p>"},{"location":"out/#b-physics-informed-learning","title":"B. Physics-informed Learning","text":"<p>\u2003\u2003Existing world models struggle to generate physically consistent videos because they lack an inherent understanding of physics, often producing unrealistic dynamics and implausible event sequences. Simply scaling up training data or model size is insufficient to capture the underlying physical laws <sup>25</sup>. To address this challenge, several approaches have been proposed. \u2003\u2003For example, Yang et al. <sup>26</sup> introduce a two-stage image-to-video generation framework that explicitly incorporates physics through vision- and language-informed physical priors. Team et al. <sup>12</sup> estimate depth and camera pose directly from videos, facilitating physics-informed learning and enabling world models to infer and predict physically consistent dynamics. Peper et al. <sup>27</sup> argue that advancing from physics-informed to physics-interpretable world models requires rethinking model design, and propose four guiding principles: organizing latent spaces by physical intent, encoding invariant and equivariant environmental representations, integrating multiple supervision signals, and partitioning generative outputs to improve both scalability and verifiability.</p> <ol> <li> <p>C.-L. Cheang et al., \"Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation,\" arXiv preprint arXiv:2410.06158, 2024.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>H. Wu et al., \"Unleashing large-scale video generative pre-training for visual robot manipulation,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>M. Assran et al., \"V-jepa 2: Self-supervised video models enable understanding, prediction and planning,\" arXiv preprint arXiv:2506.09985, 2025.\u00a0\u21a9</p> </li> <li> <p>A. Bardes et al., \"Revisiting feature prediction for learning visual representations from video,\" arXiv preprint arXiv:2404.08471, 2024.\u00a0\u21a9</p> </li> <li> <p>L. Wang, K. Zhao, C. Liu, and X. Chen, \"Learning real-world action-video dynamics with heterogeneous masked autoregression,\" arXiv preprint arXiv:2502.04296, 2025.\u00a0\u21a9</p> </li> <li> <p>C. Cheang et al., \"Gr-3 technical report,\" arXiv preprint arXiv:2507.15493, 2025.\u00a0\u21a9</p> </li> <li> <p>K. Black et al., \"\\(\\pi\\_{0.5}\\): A vision-language-action model with open-world generalization,\" arXiv preprint arXiv:2504.16054, 2025.\u00a0\u21a9</p> </li> <li> <p>L. Barcellona, A. Zadaianchuk, D. Allegro, S. Papa, S. Ghidoni, and E. Gavves, \"Dream to manipulate: Compositional world models empowering robot imitation learning with imagination,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>J. Ho, A. Jain, and P. Abbeel, \"Denoising diffusion probabilistic models,\" Advances in neural information processing systems, vol. 33, pp. 6840--6851, 2020.\u00a0\u21a9</p> </li> <li> <p>A. Kirillov et al., \"Segment anything,\" in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 4015--4026.\u00a0\u21a9\u21a9</p> </li> <li> <p>Y. Liao et al., \"Genie envisioner: A unified world foundation platform for robotic manipulation,\" arXiv preprint arXiv:2508.05635, 2025.\u00a0\u21a9</p> </li> <li> <p>H. Zhu et al., \"Aether: Geometric-aware unified world modeling,\" in ICCV, 2025.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. Zhen et al., \"TesserAct: Learning 4D embodied world models,\" arXiv preprint arXiv:2504.20995, 2025.\u00a0\u21a9</p> </li> <li> <p>Y. Wang, R. Yu, S. Wan, L. Gan, and D.-C. Zhan, \"Founder: Grounding foundation models in world models for open-ended embodied decision making,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>P. Mazzaglia, T. Verbelen, B. Dhoedt, A. Courville, and S. Rajeswar, \"GenRL: Multimodal-foundation world models for generalization in embodied agents,\" Advances in neural information processing systems, vol. 37, pp. 27529--27555, 2024.\u00a0\u21a9</p> </li> <li> <p>S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, \"RoboDreamer: Learning compositional world models for robot imagination,\" in International conference on machine learning, PMLR, 2024, pp. 61885--61896.\u00a0\u21a9</p> </li> <li> <p>N. Kitaev, S. Cao, and D. Klein, \"Multilingual constituency parsing with self-attention and pre-training,\" in Proceedings of the 57th annual meeting of the association for computational linguistics, 2019, pp. 3499--3505.\u00a0\u21a9</p> </li> <li> <p>J.-C. Pang et al., \"Learning view-invariant world models for visual robotic manipulation,\" in The thirteenth international conference on learning representations, 2025.\u00a0\u21a9</p> </li> <li> <p>F. Martinez-Lopez, T. Li, Y. Lu, and J. Chen, \"In-context reinforcement learning via communicative world models,\" arXiv preprint arXiv:2508.06659, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Wu, H. Ma, C. Deng, and M. Long, \"Pre-training contextualized world models with in-the-wild videos for reinforcement learning,\" Advances in Neural Information Processing Systems, vol. 36, pp. 39719--39743, 2023.\u00a0\u21a9</p> </li> <li> <p>H. Zhi et al., \"3DFlowAction: Learning cross-embodiment manipulation from 3D flow world model,\" arXiv preprint arXiv:2506.06199, 2025.\u00a0\u21a9</p> </li> <li> <p>C. Finn, I. Goodfellow, and S. Levine, \"Unsupervised learning for physical interaction through video prediction,\" in Proceedings of the 30th international conference on neural information processing systems, 2016, pp. 64--72.\u00a0\u21a9</p> </li> <li> <p>K. Black et al., \"Zero-shot robotic manipulation with pre-trained image-editing diffusion models,\" in The twelfth international conference on learning representations, 2024.\u00a0\u21a9</p> </li> <li> <p>R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak, \"Planning to explore via self-supervised world models,\" in International conference on machine learning, 2020, pp. 8583--8592.\u00a0\u21a9</p> </li> <li> <p>B. Kang et al., \"How far is video generation from world model: A physical law perspective,\" in Forty-second international conference on machine learning, 2025.\u00a0\u21a9</p> </li> <li> <p>X. Yang et al., \"VLIPP: Towards physically plausible video generation with vision and language informed physical prior,\" arXiv preprint arXiv:2503.23368, 2025.\u00a0\u21a9</p> </li> <li> <p>J. Peper, Z. Mao, Y. Geng, S. Pan, and I. Ruchkin, \"Four principles for physically interpretable world models,\" arXiv preprint arXiv:2503.02143, 2025.\u00a0\u21a9</p> </li> </ol>"}]}